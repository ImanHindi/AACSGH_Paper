{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#greenhouse_path = Path(\"C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\greenhouse_fill_missing_values.csv\")\n",
    "#weather_path = Path(\"C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\weather_fill_missing_values.csv\")\n",
    "#greenhouse_climate_path = Path(\"C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\greenhouse_climate_fill_missing_values.csv\")\n",
    "#root_zone_path = Path(\"C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\root_zone_fill_missing_values.csv\")\n",
    "#Crop_parameters_path = Path(\"C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\root_zone_fill_missing_values.csv\")\n",
    "#\n",
    "#greenhouse = pd.read_csv(greenhouse_path,parse_dates=[\"%time\"]).set_index('%time')\n",
    "#weather=pd.read_csv(weather_path,parse_dates=[\"%time\"]).set_index('%time')\n",
    "#greenhouse_climate=pd.read_csv(greenhouse_climate_path,parse_dates=[\"%time\"]).set_index('%time')\n",
    "#root_zone=pd.read_csv(root_zone_path,parse_dates=[\"%time\"]).set_index('%time')\n",
    "#Crop_parameters=pd.read_csv(Crop_parameters_path,parse_dates=[\"%time\"]).set_index('%time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"Weekly Crop Parameters\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "orgin_path = \"D:\\\\Iman\\\\AGHC\\\\CherryTomato\\\\Data\\\\\"\n",
    "filenames = ['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators'\n",
    "             ]\n",
    "\n",
    "daily_Avg=dict()\n",
    "Weekly_Crop_Parameters=dict()\n",
    "greenhouse_climate=dict()\n",
    "for filename in filenames:\n",
    "    path=Path(orgin_path+filename+'\\\\CropParameters.csv')\n",
    "    Weekly_Crop_Parameters[filename] = pd.read_csv(path,parse_dates=[\"%Time\"],low_memory=False).set_index('%Time').astype(float)\n",
    "    #print(Weekly_Crop_Parameters[filename].index.shape)\n",
    "    Weekly_Crop_Parameters[filename]['%Time_t']=Weekly_Crop_Parameters[filename].index.astype(str)+\"_\"+filename\n",
    "    Weekly_Crop_Parameters[filename]=Weekly_Crop_Parameters[filename].set_index('%Time_t')\n",
    "    path=Path(orgin_path+filename+'\\\\GreenhouseClimate.csv')\n",
    "    greenhouse_climate[filename]=pd.read_csv(path,parse_dates=[\"%time\"],low_memory=False).set_index('%time').astype(float)\n",
    "    \n",
    "    #greenhouse_climate[filename].drop(columns=[  'AssimLight', 'BlackScr', 'CO2air', 'EC_drain_PC', 'EnScr', 'HumDef',\n",
    "    #   'PipeGrow', 'PipeLow', 'Rhair', 'Tair', 'Tot_PAR', 'Tot_PAR_Lamps',\n",
    "    #   'VentLee', 'Ventwind', 'co2_dos', 'Cum_irr','water_sup'],inplace=True)\n",
    "    sp_vip_paired_attributtes=[ [('assim_sp','assim_vip'),('assim_vip','assim_sp')],\n",
    "                            [('co2_sp','co2_vip'),('co2_vip','co2_sp')],\n",
    "                            [('dx_sp','dx_vip'),('dx_vip','dx_sp')],\n",
    "                            [('int_blue_sp','int_blue_vip'),('int_blue_vip','int_blue_sp')],\n",
    "                            [('int_farred_sp','int_farred_vip'),('int_farred_vip','int_farred_sp')],\n",
    "                            [('int_red_sp','int_red_vip'),('int_red_vip','int_red_sp')], \n",
    "                            [('int_white_sp','int_white_vip'),('int_white_vip','int_white_sp')],\n",
    "                            [('scr_blck_sp','scr_blck_vip'),('scr_blck_vip','scr_blck_sp')],\n",
    "                            [('scr_enrg_sp','scr_enrg_vip'),('scr_enrg_vip','scr_enrg_sp')],\n",
    "                            [('t_grow_min_sp','t_grow_min_vip'),('t_grow_min_vip','t_grow_min_sp')],\n",
    "                            [('t_heat_sp','t_heat_vip'),('t_heat_vip','t_heat_sp')],\n",
    "                            [('t_rail_min_sp','t_rail_min_vip'),('t_rail_min_vip','t_rail_min_sp')],\n",
    "                            [('t_vent_sp','t_ventlee_vip'),('t_ventlee_vip','t_vent_sp')],\n",
    "                            [('water_sup_intervals_sp_min','water_sup_intervals_vip_min'),('water_sup_intervals_vip_min','water_sup_intervals_sp_min')],\n",
    "                            [('window_pos_lee_sp','window_pos_lee_vip'),('window_pos_lee_vip','window_pos_lee_sp')],\n",
    "                            ]\n",
    "    norm_mean_difference=[]\n",
    "    for sp_vip_paired_attributte in sp_vip_paired_attributtes: \n",
    "        for (sp, vip) in sp_vip_paired_attributte:\n",
    "            x=greenhouse_climate[filename].iloc[(np.where(greenhouse_climate[filename][sp].isnull()))][vip]\n",
    "            #print(teams_ghc[sp])\n",
    "            difference=(greenhouse_climate[filename][sp]-greenhouse_climate[filename][vip]).mean()\n",
    "            mean_difference=difference/(greenhouse_climate[filename][sp].mean())\n",
    "            norm_mean_difference.append([sp,vip,mean_difference])\n",
    "\n",
    "            #print(f'{sp},{vip} difference={norm_mean_difference}')\n",
    "            #print(x.index)\n",
    "            for i in x.index:\n",
    "                #print(i)\n",
    "                greenhouse_climate[filename].loc[i,sp]= greenhouse_climate[filename].loc[i,vip]#.iloc[i][vip]\n",
    "                #print(i,teams_ghc.loc[i,sp]) \n",
    "            #plt.figure()\n",
    "            #correlation = greenhouse_climate[filename].corr()[vip][sp]\n",
    "            #corr1=greenhouse_climate[filename].corr()[vip].sort_values(ascending=False)\n",
    "            #corr1.plot(kind='bar',title=f'{vip} SP-VIP Correlation',figsize=(18, 6),fontsize=12)\n",
    "            ##plt.show\n",
    "            #save_fig(f'{vip}SP-VIP Correlation')\n",
    "            #print(f\"correlation between {vip} and {sp}=\",correlation)\n",
    "            #plt.show\n",
    "    norm_mean_difference     \n",
    "\n",
    "\n",
    "\n",
    "    #fill missing values\n",
    "    for gh_attribute in greenhouse_climate[filename].columns:\n",
    "        x=greenhouse_climate[filename].iloc[(np.where(greenhouse_climate[filename][gh_attribute].isnull()))][gh_attribute]\n",
    "        for i in reversed(x.index):\n",
    "            if i.day+(1)< greenhouse_climate[filename].index[-1].day:\n",
    "                #print(i+(12*24))\n",
    "                d=i\n",
    "                #print(d.day+1)\n",
    "\n",
    "                d = d.replace(day=d.day+1)\n",
    "\n",
    "                #print(d.day+1)\n",
    "                greenhouse_climate[filename].loc[i,gh_attribute]= greenhouse_climate[filename].loc[d,gh_attribute]\n",
    "                #print(i+(12*24),teams_ghc.loc[i,gh_attribute])\n",
    "    for gh_attribute in greenhouse_climate[filename].columns:\n",
    "        x=greenhouse_climate[filename].iloc[(np.where(greenhouse_climate[filename][gh_attribute].isnull()))][gh_attribute]\n",
    "        for i in x.index:\n",
    "            if i.day-(1)>= 0:\n",
    "                #print(i+(12*24))\n",
    "                d=i\n",
    "                #print(d)\n",
    "                d = d.replace(day=d.day-1)\n",
    "                #print(d)\n",
    "                greenhouse_climate[filename].loc[i,gh_attribute]= greenhouse_climate[filename].loc[d,gh_attribute]\n",
    "                #print(i+(12*24),teams_ghc.loc[i,gh_attribute])\n",
    "\n",
    "    #print(greenhouse_climate[filename].groupby(greenhouse_climate[filename].index.date).mean())\n",
    "    \n",
    "    daily_Avg[filename]=greenhouse_climate[filename].groupby(greenhouse_climate[filename].index.date).mean()\n",
    "    daily_Avg[filename]['%time_t']=daily_Avg[filename].index.astype(str)+\"_\"+filename\n",
    "    daily_Avg[filename]=daily_Avg[filename].set_index('%time_t')\n",
    "    #print(daily_Avg[filename].index[-1])\n",
    "    daily_Avg[filename].drop(daily_Avg[filename].index[-1],axis=0,inplace=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for feature in daily_Avg[filename].columns:\n",
    "    for i in range(len(filenames)):\n",
    "        daily_Avg[filenames[i]][\"2019-12-16 00:00:00\" : \"2019-12-26 00:00:00\"][feature].plot(ylabel=feature,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{filenames[i]}')\n",
    "    plt.title(f'green_house_{feature}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    save_fig(f'green_house_{feature}_time_series_plot ')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    #greenhouse_climate[filename]['hours']=greenhouse_climate[filename].index.time\n",
    "    #greenhouse_climate[filename]['hours']=greenhouse_climate[filename]['hours'].astype('string').str.split(':',expand=True)[0]\n",
    "    #greenhouse_climate[filename]['days']=greenhouse_climate[filename].index.date\n",
    "    #greenhouse_climate[filename]['days']=greenhouse_climate[filename]['days'].astype('string').str.split('-',expand=True)[2]\n",
    "    #print('days',greenhouse_climate[filename]['days'])\n",
    "    #print('hours',greenhouse_climate[filename]['hours'])\n",
    "    #print(greenhouse_climate[filename]['%time'])\n",
    "    #print(greenhouse_climate[filename].head())\n",
    "    \n",
    "    greenhouse_climate[filename]['%time']=greenhouse_climate[filename].index\n",
    "    out=greenhouse_climate[filename]['%time'].astype('string').str.split(' ',expand=True).reset_index()\n",
    "    #print(out.head())\n",
    "    out['days']=np.zeros(shape=out[0].shape[0])\n",
    "    out['hours']=np.zeros(shape=out[0].shape[0])\n",
    "    out['minutes']=np.zeros(shape=out[0].shape[0])\n",
    "#\n",
    "    out.shape\n",
    "    \n",
    "    \n",
    "    day_count=1\n",
    "    min_counts=1\n",
    "    for i in (out.groupby(by=[0])[0].sum().index):\n",
    "        #print(i)\n",
    "        min_counts=1\n",
    "        hours_count=1\n",
    "        for idx in out[0].index:     \n",
    "            if out[0].iloc[idx]==i:\n",
    "                out['days'].iloc[idx]=day_count\n",
    "                #print('done',idx)\n",
    "                out['hours'].iloc[idx]=hours_count\n",
    "                min_counts+=1\n",
    "                if min_counts>12:\n",
    "                    hours_count+=1\n",
    "                    min_counts=1\n",
    "        day_count+=1\n",
    "    out\n",
    "    #\n",
    "    greenhouse_climate[filename]['days']=out['days'].values\n",
    "    greenhouse_climate[filename]['hours']=out['hours'].values\n",
    "    greenhouse_climate[filename]['minutes']=out['minutes'].values\n",
    "#\n",
    "    #greenhouse_climate[filename].drop('%time',inplace=True)\n",
    "    #print(greenhouse_climate[filename]['days'].value_counts())\n",
    "    \n",
    "    \n",
    "    #print(out['hours'].value_counts())\n",
    "    #print(out['days'].value_counts())\n",
    "    \n",
    "    greenhouse_climate[filename]['%time_t']=greenhouse_climate[filename].index.astype(str)+\"_\"+filename\n",
    "    greenhouse_climate[filename]=greenhouse_climate[filename].set_index('%time_t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators'\n",
    "             ]\n",
    "for i in range(len(filenames)):\n",
    "    #print(greenhouse_climate[filenames[i]].index)\n",
    "    greenhouse_climate[filenames[i]]['%time'] = pd.to_datetime(greenhouse_climate[filenames[i]].index.str.split('_').str[0])\n",
    "    greenhouse_climate[filenames[i]].set_index('%time', inplace=True)  # Set datetime as the index\n",
    "\n",
    "    #print(greenhouse_climate[filenames[i]].columns)\n",
    "    daily_Avg[filenames[i]]['%time'] = pd.to_datetime(daily_Avg[filenames[i]].index.str.split('_').str[0])\n",
    "    daily_Avg[filenames[i]].set_index('%time', inplace=True)  # Set datetime as the index\n",
    "\n",
    "    Weekly_Crop_Parameters[filenames[i]]['%Time'] = pd.to_datetime(Weekly_Crop_Parameters[filenames[i]].index.str.split('_').str[0])\n",
    "    Weekly_Crop_Parameters[filenames[i]].set_index('%Time', inplace=True)  # Set datetime as the index\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filenames)):\n",
    "\n",
    "    greenhouse_climate[filenames[i]].drop(['assim_sp', 'co2_sp',  'dx_sp',\n",
    "           'int_blue_sp', 'int_farred_sp','scr_blck_sp',\n",
    "           'int_red_sp', 'int_white_sp','t_grow_min_sp',\n",
    "           't_heat_sp', 't_rail_min_sp', 'hours',\n",
    "           't_vent_sp','water_sup_intervals_sp_min',\n",
    "           'window_pos_lee_sp','scr_enrg_sp','minutes'],axis=1,inplace=True)\n",
    "    daily_Avg[filenames[i]].drop(['assim_sp', 'co2_sp',  'dx_sp',\n",
    "           'int_blue_sp', 'int_farred_sp','scr_blck_sp',\n",
    "           'int_red_sp', 'int_white_sp','t_grow_min_sp',\n",
    "           't_heat_sp', 't_rail_min_sp',\n",
    "           't_vent_sp','water_sup_intervals_sp_min',\n",
    "           'window_pos_lee_sp','scr_enrg_sp'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "teams_data=dict()\n",
    "corr=dict()\n",
    "team_corr=dict()\n",
    "filenames = ['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators'\n",
    "             ]\n",
    "for i in range(len(filenames)):\n",
    "\n",
    "    teams_ghc_weekly = greenhouse_climate[filenames[i]].resample('W').mean()\n",
    "    \n",
    "    Weekly_Crop_Parameters[filenames[i]].set_index(teams_ghc_weekly.index[0:-1], inplace=True)\n",
    "\n",
    "    merged_data = teams_ghc_weekly.merge(Weekly_Crop_Parameters[filenames[i]], left_index=True, right_index=True, how='inner', suffixes=('_ghc', '_wcp'))\n",
    "    #print(merged_data.head())\n",
    "    correlation_matrix = merged_data.corr()\n",
    "    \n",
    "    outputs=merged_data.columns[-5:-2]\n",
    "    \n",
    "    for output in outputs:\n",
    "        corr[output]=merged_data[merged_data.columns].corr()[output].sort_values(ascending=False)\n",
    "        corr[output].drop(outputs,axis=0,inplace=True)\n",
    "        corr[output].plot(kind='bar', title=f'{filenames[i]} {output} Correlation with SP',figsize=(18, 6),fontsize=12)\n",
    "        plt.show()\n",
    "        save_fig(f'{filenames[i]} {output} Correlation with SP')\n",
    "        \n",
    "    team_corr[filenames[i]]=corr\n",
    "    # Plotting the correlation matrix\n",
    "    #plt.figure(figsize=(15, 10))\n",
    "    #sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "    #plt.title('Correlation Matrix Between SP Features and Crop Parameters')\n",
    "    #plt.show()\n",
    "    \n",
    "    #print('control_features',merged_data.columns[:-3])\n",
    "    control_features = merged_data.columns[:-5]\n",
    "    \n",
    "    #print('output',merged_data.columns[-5:-2])\n",
    "    \n",
    "    for control_feature in control_features:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        sns.lineplot(data=merged_data[control_feature], palette='tab11',legend=True)           \n",
    "        sns.lineplot(data=merged_data[outputs], palette='tab10')\n",
    "\n",
    "        plt.title(f'Time Series of {control_feature} and CropParameters for {filenames[i]}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.legend(loc='right')\n",
    "        plt.show()\n",
    "    #for control_feature in control_features:\n",
    "    #    plt.figure(figsize=(15, 6))\n",
    "    #    for output in outputs:\n",
    "    #        sns.lineplot(x=merged_data[control_feature],y=merged_data[output], palette='tab10')\n",
    "    #        #sns.lineplot(data=teams_ghc_weekly[output], palette='tab10')\n",
    "    #plt.title(f'{control_feature} with CropParameters {filenames[i]}')\n",
    "    #plt.xlabel(control_feature)\n",
    "    #plt.ylabel('CropParameters')\n",
    "    #plt.legend(loc='best')\n",
    "    #plt.show()\n",
    "    \n",
    "    # Step 8: Effect of Specific Control Setpoints on Crop Parameters\n",
    "    for control_feature in ['CO2air', 'Tair', 'HumDef', 'Rhair']:\n",
    "        for output in outputs:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.scatterplot(data=merged_data, x=control_feature, y=output)\n",
    "            sns.regplot(data=merged_data, x=control_feature, y=output, scatter=False, color='red')\n",
    "            plt.title(f'Effect of {control_feature} on {output} for {filenames[i]}')\n",
    "            plt.show()\n",
    "\n",
    "    teams_data[filenames[i]]=merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filenames)):\n",
    "    correlation_values=pd.concat([team_corr[filenames[i]][outputs[0]],\n",
    "                                  team_corr[filenames[i]][outputs[1]],\n",
    "                                  team_corr[filenames[i]][outputs[2]]],axis=1)\n",
    "    correlation_values\n",
    "    headers = {\n",
    "        \"selector\": \"th:not(.index_name)\",\n",
    "        \"props\": \"background-color: #800000; color: white; text-align: center\"\n",
    "    }\n",
    "    properties = {\"border\": \"1px solid black\", \"width\": \"65px\", \"text-align\": \"center\"}\n",
    "    cell_hover = {\n",
    "        \"selector\": \"td:hover\",\n",
    "        \"props\": [(\"background-color\", \"#FFFFE0\")]\n",
    "    }\n",
    "    index_names = {\n",
    "        \"selector\": \".index_name\",\n",
    "        \"props\": \"font-style: italic; color: darkgrey; font-weight:normal;\"\n",
    "    }\n",
    "    correlation_values.style.format(precision=2).set_table_styles([cell_hover, index_names, headers]).set_properties(**properties)\n",
    "    correlation_values.style.highlight_max(color=\"red\",axis=0)\n",
    "    correlation_values.style.highlight_min(color=\"red\",axis=0)\n",
    "\n",
    "    def mean_highlighter(x):\n",
    "        style_lt = \"background-color: whight\"\n",
    "        style_gt = \"background-color: red\"\n",
    "        gt_mean = abs(x) > .5\n",
    "        return [style_gt if i else style_lt for i in gt_mean]\n",
    "\n",
    "\n",
    "    correlation_values.style.apply(mean_highlighter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_Avg[filenames[i]].shape)\n",
    "greenhouse_climate[filenames[i]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames = ['AICU', 'Automatoes',\n",
    "#             'Digilog', 'IUACAAS', \n",
    "#             'Reference', 'TheAutomators'\n",
    "#             ]\n",
    "#for i in range(len(filenames)):\n",
    "#    print(filenames)\n",
    "#    print(filenames[i])\n",
    "#    greenhouse_climate[filenames[i]]['team']=filenames[i]\n",
    "#    daily_Avg[filenames[i]]['team']=filenames[i]\n",
    "#    Weekly_Crop_Parameters[filenames[i]]['team']=filenames[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(greenhouse_climate['AICU']['team'])\n",
    "#print(daily_Avg['AICU']['team'])\n",
    "#\n",
    "#print(Weekly_Crop_Parameters['AICU']['team'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp=pd.concat([Weekly_Crop_Parameters[filenames[0]],Weekly_Crop_Parameters[filenames[1]],\n",
    "                    Weekly_Crop_Parameters[filenames[2]],Weekly_Crop_Parameters[filenames[3]],\n",
    "                    Weekly_Crop_Parameters[filenames[4]],Weekly_Crop_Parameters[filenames[5]]], axis=0)\n",
    "\n",
    "teams_ghc=pd.concat([greenhouse_climate[filenames[0]],greenhouse_climate[filenames[1]],\n",
    "                     greenhouse_climate[filenames[2]],greenhouse_climate[filenames[3]],\n",
    "                     greenhouse_climate[filenames[4]],greenhouse_climate[filenames[5]]], axis=0)\n",
    "\n",
    "teams_da=pd.concat([ daily_Avg[filenames[0]],daily_Avg[filenames[1]],\n",
    "                     daily_Avg[filenames[2]],daily_Avg[filenames[3]],\n",
    "                     daily_Avg[filenames[4]],daily_Avg[filenames[5]]], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feature=[]\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "all_teams_data=pd.concat([teams_data[filenames[0]],teams_data[filenames[1]],\n",
    "                     teams_data[filenames[2]],teams_data[filenames[3]],\n",
    "                     teams_data[filenames[4]],teams_data[filenames[5]]], axis=0)\n",
    "# Step 9: Feature Importance Analysis Using RandomForest Regressor\n",
    "all_teams_data.dropna(axis=0,inplace=True)\n",
    "X = all_teams_data.drop(['Stem_elong', 'Stem_thick', 'Cum_trusses','stem_dens ', 'plant_dens'], axis=1)\n",
    "for crop_par in ['Stem_elong', 'Stem_thick', 'Cum_trusses']:\n",
    "    y = all_teams_data[crop_par]\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    for fi in feature_importances.index[:5]:\n",
    "        important_feature.append(fi)\n",
    "    # Plot Feature Importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    feature_importances.drop('days').plot(kind='bar')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Feature Importance')\n",
    "    plt.title(f'Feature Importance for Predicting {crop_par}')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in ['Stem_elong', 'Stem_thick', 'Cum_trusses']:\n",
    "    important_feature.append(x)\n",
    "important_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for i in range(len(filenames)):\n",
    "    #plt.figure(figsize=(10, 8))\n",
    "    #sns.boxplot(teams_data[filenames[i]],orient = 'h')\n",
    "    #plt.title(f'da_sp Distribution Across {filenames[i]} Team')\n",
    "    #plt.xlabel('Feature')\n",
    "    #plt.ylabel('Values')\n",
    "    #plt.show()\n",
    "    \n",
    "    # Step 8: Feature Distribution Comparison Between Teams\n",
    "#for i in range(len(filenames)):\n",
    "#    plt.figure(figsize=(10, 8))\n",
    "#    sns.boxplot(daily_Avg[filenames[i]],orient = 'h')\n",
    "#    plt.title(f'da_sp Distribution Across {filenames[i]}')\n",
    "#    plt.xlabel('Feature')\n",
    "#    plt.ylabel('Values')\n",
    "#    plt.show()\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.boxplot(greenhouse_climate[filenames[i]],orient = 'h')\n",
    "    plt.title(f'sp Distribution Across {filenames[i]}')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()\n",
    "\n",
    "for i in range(len(filenames)):  \n",
    "    plt.figure(figsize=(10, 8)) \n",
    "    print(Weekly_Crop_Parameters[filenames[i]].columns[:-2]) \n",
    "    bb=Weekly_Crop_Parameters[filenames[i]][Weekly_Crop_Parameters[filenames[i]].columns[:-2]]\n",
    "    sns.boxplot(bb,orient = 'h')\n",
    "    plt.title(f'crop_par Distribution Across {filenames[i]}')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "features=['AssimLight', 'CO2air', 'Cum_irr',\n",
    "       'HumDef', 'Rhair', 'Tair', 'Tot_PAR',\n",
    "       'Tot_PAR_Lamps', 'co2_dos',\n",
    "       'co2_vip', 'dx_vip', 'int_blue_vip', 'int_farred_vip', 'int_red_vip',\n",
    "       'int_white_vip', 'pH_drain_PC', 'scr_blck_vip','t_heat_vip',\n",
    "       'water_sup','water_sup_intervals_vip_min']\n",
    "# Step 10: Advanced Visualizations\n",
    "for feature in features:\n",
    "    for output in outputs:\n",
    "        # Interactive Scatter Plot with Plotly\n",
    "        fig = px.scatter(all_teams_data, x=feature, y=output, color=[filenames[i] for x in range(18) for i in range(len(filenames))])       \n",
    "        fig.update_layout(title=f'Interactive Scatter Plot: {feature} vs. {output}')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Violin Plot to Compare Distributions\n",
    "for output in outputs:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(data=all_teams_data, x=[filenames[i] for x in range(18)  for i in range(len(filenames))], y=output)\n",
    "    plt.title('Violin Plot for Stem Elongation Across Teams')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for feature in greenhouse_climate[filename].columns:\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.gca()\n",
    "    for i in range(len(filenames)):\n",
    "\n",
    "        daily_avg = greenhouse_climate[filenames[i]][feature].resample('h').mean()  # compute the mean for each hour\n",
    "        #weekly_avg = Weekly_Crop_Parameters[filenames[i]][output].resample('W').mean()\n",
    "        #print(dayly_avg)\n",
    "        period=slice(\"2019-12-16 00:00:00\", \"2020-05-30 00:00:00\")\n",
    "        rolling_average_daily = daily_avg[period].rolling(window=24*30).mean()\n",
    "        #rolling_average_weekly = weekly_avg[period].rolling(window=4).mean()\n",
    "\n",
    "        rolling_average_daily.plot(grid=True,legend=True,label=f'{filenames[i]}')\n",
    "    ax.set_xlabel(xlabel='%time')\n",
    "    ax.set_ylabel(ylabel=f'{feature}')\n",
    "    ax.set_title(f'{feature} time series of Teams')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#add the legend for each line of plot which represent the file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_actions = greenhouse_climate[filenames[i]].resample('W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#period=slice(\"2019-12-16 00:00:00\", \"2020-05-30 00:00:00\")\n",
    "weekly_actions#[period]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weekly_Crop_Parameters[filenames[0]].to_csv(f'{filenames[0]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[1]].to_csv(f'{filenames[1]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[2]].to_csv(f'{filenames[2]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[3]].to_csv(f'{filenames[3]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[4]].to_csv(f'{filenames[4]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[5]].to_csv(f'{filenames[5]}Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Weekly_Crop_Parameters[filenames[0]].to_csv(f'{filenames[0]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[1]].to_csv(f'{filenames[1]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[2]].to_csv(f'{filenames[2]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[3]].to_csv(f'{filenames[3]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[4]].to_csv(f'{filenames[4]}Results.csv')\n",
    "Weekly_Crop_Parameters[filenames[5]].to_csv(f'{filenames[5]}Results.csv')\n",
    "\n",
    "\n",
    "greenhouse_climate[filenames[0]].to_csv(f'{filenames[0]}Actions.csv')\n",
    "greenhouse_climate[filenames[1]].to_csv(f'{filenames[1]}Actions.csv')\n",
    "greenhouse_climate[filenames[2]].to_csv(f'{filenames[2]}Actions.csv')\n",
    "greenhouse_climate[filenames[3]].to_csv(f'{filenames[3]}Actions.csv')\n",
    "greenhouse_climate[filenames[4]].to_csv(f'{filenames[4]}Actions.csv')\n",
    "greenhouse_climate[filenames[5]].to_csv(f'{filenames[5]}Actions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for output in outputs:\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.gca()\n",
    "    for i in range(len(filenames)):\n",
    "        #daily_avg = greenhouse_climate[filenames[i]][feature].resample('h').mean()  # compute the mean for each hour\n",
    "        weekly_avg = Weekly_Crop_Parameters[filenames[i]][output].resample('W').mean()\n",
    "        #print(dayly_avg)\n",
    "        period=slice(\"2019-12-16 00:00:00\", \"2020-05-30 00:00:00\")\n",
    "        #rolling_average_daily = daily_avg[period].rolling(window=24*30).mean()\n",
    "        rolling_average_weekly = weekly_avg[period].rolling(window=4).mean()\n",
    "        #dayly_avg=weather.groupby(by=['time'])[weather_attribute].mean()\n",
    "        #print(weather_attribute,weather.groupby(by=['time'])[weather_attribute].mean())\n",
    "        #greenhouse_climate[filenames[i]][feature].plot(ax=ax,ylabel=feature,xlabel='Day Index',grid='True', marker=\".\", figsize=(18, 6))\n",
    "        #rolling_average_dayly = dayly_avg.rolling(window=10).mean()\n",
    "        #print(weather_attribute,rolling_average_dayly)\n",
    "        #rolling_average_daily.plot(grid=True,legend=True,label=f'{filenames[i]}')\n",
    "        rolling_average_weekly.plot(grid=True,legend=True,label=f'{filenames[i]}')\n",
    "    ax.set_xlabel(xlabel='%time')\n",
    "    ax.set_ylabel(ylabel=f'{output} crop parameter')\n",
    "    ax.set_title(f'{output} time series of Teams')\n",
    "    plt.show()\n",
    "#add the legend for each line of plot which represent the file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('teams_wcp.npy',teams_wcp)\n",
    "np.save('teams_ghc.npy',teams_ghc)\n",
    "np.save('teams_da.npy',teams_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teams_ghc.astype(float).isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_da.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_ghc.hist(figsize=(20,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams_da.shape)\n",
    "print(teams_wcp.shape)\n",
    "print(teams_ghc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_da.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_da.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp.head(138)\n",
    "teams_wcp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp.drop([\"stem_dens \", \"plant_dens\"],inplace=True,axis=1)\n",
    "teams_wcp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_ghc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_ghc.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['assim_sp', 'assim_vip', 'co2_dos', 'co2_sp',\n",
    "       'co2_vip', 'dx_sp', 'dx_vip', 'int_blue_sp', 'int_blue_vip',\n",
    "       'int_farred_sp', 'int_farred_vip', 'int_red_sp', 'int_red_vip',\n",
    "       'int_white_sp', 'int_white_vip', 'scr_blck_sp',\n",
    "       'scr_blck_vip', 'scr_enrg_sp', 'scr_enrg_vip', 't_grow_min_sp',\n",
    "       't_grow_min_vip', 't_heat_sp', 't_heat_vip', 't_rail_min_sp',\n",
    "       't_rail_min_vip', 't_vent_sp', 't_ventlee_vip', 't_ventwind_vip',\n",
    "       'water_sup_intervals_sp_min', 'window_pos_lee_sp', 'window_pos_lee_vip',\n",
    "       'water_sup_intervals_vip_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_ghc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_ghc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_ghc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teams_ghc.drop('%time',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(teams_ghc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp.to_csv('teams_wcp.csv')\n",
    "teams_ghc.to_csv('teams_ghc.csv')\n",
    "teams_da.to_csv('teams_da.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_ghc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('teams_wcp.npy',teams_wcp)\n",
    "np.save('teams_ghc.npy',teams_ghc)\n",
    "np.save('teams_da.npy',teams_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "teams_wcp_scaler =      MinMaxScaler()\n",
    "teams_ghc_scaler =      MinMaxScaler()\n",
    "teams_da_scaler  =      MinMaxScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_wcp_scaled = pd.DataFrame(teams_wcp_scaler.fit_transform(teams_wcp),columns=teams_wcp.columns)\n",
    "teams_ghc_scaled = pd.DataFrame(teams_ghc_scaler.fit_transform(teams_ghc),columns=teams_ghc.columns)\n",
    "teams_da_scaled = pd.DataFrame(teams_da_scaler.fit_transform(teams_da),columns=teams_da.columns)\n",
    "teams_da_scaled.isnull().sum(axis=1)\n",
    "teams_wcp_scaled.isnull().sum(axis=0)\n",
    "\n",
    "print(teams_wcp_scaled.min(axis=0),teams_wcp_scaled.max(axis=0))\n",
    "print(teams_ghc_scaled.min(axis=0),teams_ghc_scaled.max(axis=0))\n",
    "print(teams_da_scaled.min(axis=0),teams_da_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=Weekly_Crop_Parameters[filenames[0]].drop(['stem_dens ', 'plant_dens'],axis=1).columns\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(teams_wcp_scaler.transform(Weekly_Crop_Parameters[filenames[0]].drop(['stem_dens ', 'plant_dens'],axis=1)),columns=col,index=Weekly_Crop_Parameters[filenames[0]].index).to_csv(f'{filenames[0]}Results.csv')\n",
    "pd.DataFrame(teams_wcp_scaler.transform(Weekly_Crop_Parameters[filenames[1]].drop(['stem_dens ', 'plant_dens'],axis=1)),columns=col,index=Weekly_Crop_Parameters[filenames[1]].index).to_csv(f'{filenames[1]}Results.csv')\n",
    "pd.DataFrame(teams_wcp_scaler.transform(Weekly_Crop_Parameters[filenames[2]].drop(['stem_dens ', 'plant_dens'],axis=1)),columns=col,index=Weekly_Crop_Parameters[filenames[2]].index).to_csv(f'{filenames[2]}Results.csv')\n",
    "pd.DataFrame(teams_wcp_scaler.transform(Weekly_Crop_Parameters[filenames[3]].drop(['stem_dens ', 'plant_dens'],axis=1)),columns=col,index=Weekly_Crop_Parameters[filenames[3]].index).to_csv(f'{filenames[3]}Results.csv')\n",
    "pd.DataFrame(teams_wcp_scaler.transform(Weekly_Crop_Parameters[filenames[4]].drop(['stem_dens ', 'plant_dens'],axis=1)),columns=col,index=Weekly_Crop_Parameters[filenames[4]].index).to_csv(f'{filenames[4]}Results.csv')\n",
    "pd.DataFrame(teams_wcp_scaler.transform(Weekly_Crop_Parameters[filenames[5]].drop(['stem_dens ', 'plant_dens'],axis=1)),columns=col,index=Weekly_Crop_Parameters[filenames[5]].index).to_csv(f'{filenames[5]}Results.csv')\n",
    "\n",
    "\n",
    "pd.DataFrame(teams_ghc_scaler.transform(greenhouse_climate[filenames[0]]),columns=greenhouse_climate[filenames[0]].columns,index=greenhouse_climate[filenames[0]].index).to_csv(f'{filenames[0]}Actions.csv')\n",
    "pd.DataFrame(teams_ghc_scaler.transform(greenhouse_climate[filenames[1]]),columns=greenhouse_climate[filenames[1]].columns,index=greenhouse_climate[filenames[1]].index).to_csv(f'{filenames[1]}Actions.csv')\n",
    "pd.DataFrame(teams_ghc_scaler.transform(greenhouse_climate[filenames[2]]),columns=greenhouse_climate[filenames[2]].columns,index=greenhouse_climate[filenames[2]].index).to_csv(f'{filenames[2]}Actions.csv')\n",
    "pd.DataFrame(teams_ghc_scaler.transform(greenhouse_climate[filenames[3]]),columns=greenhouse_climate[filenames[3]].columns,index=greenhouse_climate[filenames[3]].index).to_csv(f'{filenames[3]}Actions.csv')\n",
    "pd.DataFrame(teams_ghc_scaler.transform(greenhouse_climate[filenames[4]]),columns=greenhouse_climate[filenames[4]].columns,index=greenhouse_climate[filenames[4]].index).to_csv(f'{filenames[4]}Actions.csv')\n",
    "pd.DataFrame(teams_ghc_scaler.transform(greenhouse_climate[filenames[5]]),columns=greenhouse_climate[filenames[5]].columns,index=greenhouse_climate[filenames[5]].index).to_csv(f'{filenames[5]}Actions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "wcp_data=[]\n",
    "da_data=[]\n",
    "\n",
    "test_data=[]\n",
    "test_wcp_data=[]\n",
    "test_da_data=[]\n",
    "for i, wcp in enumerate(teams_wcp_scaled.index):\n",
    "    #print(str(teams_wcp_scaled.iloc[i,:].values[0]),str(teams_wcp_scaled.iloc[i,:].values[0])!= 'nan')\n",
    "    if str(teams_wcp_scaled.iloc[i,:].values[0]) != 'nan':\n",
    "        \n",
    "        #print(i*7,\":\",i*7+7)\n",
    "\n",
    "        wcp_sample=np.array(teams_da_scaled[i*7:i*7+7]).T\n",
    "        da_sample=np.array(teams_wcp_scaled.iloc[i,:])\n",
    "\n",
    "\n",
    "        #print(wcp_sample.shape, \"...\")\n",
    "        #print(da_sample.shape)\n",
    "\n",
    "        da_data.append(wcp_sample)\n",
    "        wcp_data.append(da_sample)\n",
    "        data.append((wcp_sample,da_sample))\n",
    "        #print(\"Label:\", teams_wcp_scaled.iloc[i,:])\n",
    "    else:\n",
    "        #print(\"test\",i*7,\":\",i*7+7)\n",
    "\n",
    "        wcp_test_sample=np.array(teams_da_scaled[i*7:i*7+7])\n",
    "        da_test_sample=np.array(teams_wcp_scaled.iloc[i,:])\n",
    "\n",
    "\n",
    "        #print(wcp_test_sample.shape, \"...\")\n",
    "        #print(da_test_sample.shape)\n",
    "\n",
    "        test_da_data.append(wcp_test_sample)\n",
    "        test_wcp_data.append(da_test_sample)\n",
    "        test_data.append((wcp_test_sample,da_test_sample))\n",
    "#print(data)\n",
    "#data=np.array(data)\n",
    "#test_data=np.array(test_data)\n",
    "\n",
    "#print(\"data shape\", data.shape)\n",
    "#print(\"data sample shape\",data[1,0].shape )\n",
    "\n",
    "#print(\"test data shape\", test_data.shape)\n",
    "#print(\"test data sample shape\",test_data[1,0].shape )\n",
    "\n",
    "wcp_data=np.array(wcp_data)\n",
    "test_wcp_data=np.array(test_wcp_data)\n",
    "\n",
    "print(\"wcp data shape\", wcp_data.shape)\n",
    "print(\"wcp data sample shape\",wcp_data[1,:].shape )\n",
    "\n",
    "print(\"test wcp data shape\", test_wcp_data.shape)\n",
    "print(\"test wcp data sample shape\",test_wcp_data[1,:].shape )\n",
    "\n",
    "da_data=np.array(da_data)\n",
    "test_da_data=np.array(test_da_data)\n",
    "\n",
    "\n",
    "print(\"da data shape\", da_data.shape)\n",
    "print(\"da data sample shape\",da_data[1,:].shape )\n",
    "\n",
    "print(\"test da data shape\", test_da_data.shape)\n",
    "print(\"test da data sample shape\",test_da_data[:].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=[]\n",
    "wcp_data=[]\n",
    "ghc_data=[]\n",
    "\n",
    "full_test_data=[]\n",
    "test_wcp_data=[]\n",
    "test_ghc_data=[]\n",
    "for i, wcp in enumerate(teams_wcp_scaled.index):\n",
    "    print(str(teams_wcp_scaled.iloc[i,:].values[0]),str(teams_wcp_scaled.iloc[i,:].values[0])!= 'nan')\n",
    "    if str(teams_wcp_scaled.iloc[i,:].values[0]) != 'nan':\n",
    "        \n",
    "        print(i*2016,\":\",i*2016+2016)\n",
    "\n",
    "        wcp_sample=np.array(teams_ghc_scaled[i*2016:i*2016+2016])\n",
    "        ghc_sample=np.array(teams_wcp_scaled.iloc[i,:])\n",
    "\n",
    "\n",
    "        print(wcp_sample.shape, \"...\")\n",
    "        print(ghc_sample.shape)\n",
    "\n",
    "        ghc_data.append(wcp_sample)\n",
    "        wcp_data.append(ghc_sample)\n",
    "        full_data.append((wcp_sample,ghc_sample))\n",
    "        #print(\"Label:\", teams_wcp_scaled.iloc[i,:])\n",
    "    else:\n",
    "        print(\"test\",i*2016,\":\",i*2016+2016)\n",
    "\n",
    "        wcp_test_sample=np.array(teams_ghc_scaled[i*2016:i*2016+2016])\n",
    "        ghc_test_sample=np.array(teams_wcp_scaled.iloc[i,:])\n",
    "\n",
    "\n",
    "        print(wcp_test_sample.shape, \"...\")\n",
    "        print(ghc_test_sample.shape)\n",
    "\n",
    "        test_ghc_data.append(wcp_test_sample)\n",
    "        test_wcp_data.append(ghc_test_sample)\n",
    "        full_test_data.append((wcp_test_sample,ghc_test_sample))\n",
    "#print(data)\n",
    "#full_data=np.array(full_data)\n",
    "#full_test_data=np.array(full_test_data)\n",
    "\n",
    "#print(\"data shape\", full_data.shape)\n",
    "#print(\"data sample shape\",full_data[1,0].shape )\n",
    "\n",
    "#print(\"test data shape\", full_test_data.shape)\n",
    "#print(\"test data sample shape\",full_test_data[1,0].shape )\n",
    "\n",
    "wcp_data=np.array(wcp_data)\n",
    "test_wcp_data=np.array(test_wcp_data)\n",
    "\n",
    "print(\"wcp data shape\", wcp_data.shape)\n",
    "print(\"wcp data sample shape\",wcp_data[1,:].shape )\n",
    "\n",
    "print(\"test wcp data shape\", test_wcp_data.shape)\n",
    "print(\"test wcp data sample shape\",test_wcp_data[1,:].shape )\n",
    "\n",
    "ghc_data=np.array(ghc_data)\n",
    "test_ghc_data=np.array(test_ghc_data)\n",
    "\n",
    "\n",
    "print(\"da data shape\", ghc_data.shape)\n",
    "print(\"da data sample shape\",ghc_data[1,:].shape )\n",
    "\n",
    "print(\"test da data shape\", test_ghc_data.shape)\n",
    "print(\"test da data sample shape\",test_ghc_data[:].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"data shape\", data.shape)\n",
    "#print(\"data sample shape\",data[1,0].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"data shape\", full_data.shape)\n",
    "#print(\"data sample shape\",full_data[1,0].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Conv1D, Bidirectional, Layer, Add, Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Constants\n",
    "TIME_STEPS = 2016  # 24 hours * 12 samples per hour (5 min intervals)\n",
    "CROP_PARAM_COUNT = 3  # fresh weight, dry weight, height, diameter\n",
    "CROP_TIME_STEPS = 1  # for hourly crop parameter predictions\n",
    "BATCH_SIZE = 8 #16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE=0.001\n",
    "ckpt=\"CherryTomato_wcp_model\"\n",
    "wcp_data_reshaped = wcp_data.reshape(108, 1, 3)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data = train_test_split(ghc_data, wcp_data_reshaped, test_size=0.2, random_state=42)\n",
    "print(train_data[0].shape)\n",
    "(X_train, X_test, y_train, y_test) = train_data[0],train_data[1],train_data[2],train_data[3]\n",
    "\n",
    "# Combine train and test datasets from all teams\n",
    "X_train_combined = X_train\n",
    "y_train_combined = y_train\n",
    "X_test_combined = X_test\n",
    "y_test_combined = y_test\n",
    "\n",
    "print(X_train_combined.shape)\n",
    "print(y_train_combined.shape)\n",
    "print(X_test_combined.shape)\n",
    "print(y_test_combined.shape)\n",
    "\n",
    "# Build the Encoder-Decoder Model\n",
    "def build_model(learning_rate,epochs):\n",
    "        # Encoder\n",
    "    encoder_inputs = Input(shape=(TIME_STEPS, X_train_combined.shape[-1]))\n",
    "    conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(encoder_inputs)\n",
    "    encoder_lstm = Bidirectional(LSTM(128, return_sequences=True, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(conv)\n",
    "    state_h = Add()([forward_h, backward_h])\n",
    "    state_c = Add()([forward_c, backward_c])\n",
    "\n",
    "    # Attention mechanism\n",
    "    attention = Attention()([encoder_outputs, encoder_outputs])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(CROP_TIME_STEPS,CROP_PARAM_COUNT))\n",
    "    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
    "    decoder_dense = Dense(CROP_PARAM_COUNT)\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Model\n",
    "    #model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    ## Encoder\n",
    "    #print('[INFO]  Building Model...')\n",
    "    #encoder_inputs = Input(shape=(TIME_STEPS, X_train_combined.shape[-1]))\n",
    "    #conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(encoder_inputs)\n",
    "    #encoder_lstm = Bidirectional(LSTM(128, return_sequences=True))(conv)\n",
    "    ## Attention mechanism\n",
    "    ##attention = Attention()([encoder_outputs, encoder_outputs])\n",
    "    ##attention = Attention()([encoder_lstm,encoder_lstm])\n",
    "    #encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "    #    LSTM(128, return_sequences=False, return_state=True))(encoder_lstm)\n",
    "    #\n",
    "    ##encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(conv)\n",
    "    #state_h = Add()([forward_h, backward_h])\n",
    "    #state_c = Add()([forward_c, backward_c])\n",
    "    ## Decoder\n",
    "    #decoder_inputs = Input(shape=[(CROP_TIME_STEPS, CROP_PARAM_COUNT)])\n",
    "    #decoder_lstm = LSTM(128, return_sequences=True, return_state=True) #128\n",
    "    #decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
    "    #decoder_dense = Dense((CROP_TIME_STEPS, CROP_PARAM_COUNT))\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    opt = Adam(learning_rate=learning_rate, decay=learning_rate / epochs)\n",
    "    ## Model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and train the model\n",
    "model = build_model(LEARNING_RATE,EPOCHS)\n",
    "print('[INFO]  Training Model...')\n",
    "model.summary()\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=3, restore_best_weights=True)\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(ckpt, monitor=\"val_mae\", save_best_only=True)\n",
    "\n",
    "history = model.fit([X_train_combined, y_train_combined],\n",
    "                    y_train_combined,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping_cb,model_ckpt])\n",
    "\n",
    "# Plot training history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluate the model\n",
    "print('[INFO]  Evaluating Model...')\n",
    "test_loss, test_mae, test_mse = model.evaluate([X_test_combined, y_test_combined], y_test_combined)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}, Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['mae'], label='Train Loss')\n",
    "    plt.plot(history.history['val_mae'], label='Validation Loss')\n",
    "    plt.title('Model mae Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_history(history)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using TensorFlow's save method\n",
    "model.save('CherryTomato_wcp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('CherryTomato_wcp_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set=data[:100]\n",
    "#valid_set=data[100:]\n",
    "#test_set=teams_da_scaled\n",
    "#print(train_set[:,0][1].shape)\n",
    "#print(valid_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = tf.convert_to_tensor(da_data[:70])\n",
    "train_output = tf.convert_to_tensor(wcp_data[:70])\n",
    "valid_input = tf.convert_to_tensor(da_data[70:])\n",
    "valid_output = tf.convert_to_tensor(wcp_data[70:])\n",
    "\n",
    "ghc_train_input = tf.convert_to_tensor(ghc_data[:70])\n",
    "ghc_valid_input = tf.convert_to_tensor(ghc_data[70:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ghc_train_input[1,2000,:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ghc_train_input[1,100,:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ghc_train_input.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_input.shape)\n",
    "print(valid_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "MLP_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=[ghc_train_input.shape[1],ghc_train_input.shape[-1]]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(128,activation=\"relu\"),\n",
    "\n",
    "    tf.keras.layers.Dense(3, activation=\"linear\")\n",
    "])\n",
    "\n",
    "MLP_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "avg_LSTM_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=[train_input.shape[1],train_input.shape[-1]]),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=4, strides=1, padding=\"same\",\n",
    "                           activation=\"relu\"),\n",
    "    tf.keras.layers.LSTM(64,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(64,return_sequences=False),\n",
    "    tf.keras.layers.Dense(3, activation=\"linear\")\n",
    "])\n",
    "avg_LSTM_model.input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def fit_and_evaluate(model, train_input,train_output, valid_input,valid_output, learning_rate,ckpt, epochs=10,batch_size=32,patience=10):\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_mae\", patience=patience, restore_best_weights=True)\n",
    "    model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckpt, monitor=\"val_mae\", save_best_only=True)\n",
    "    opt = Adam(learning_rate=learning_rate, decay=learning_rate / epochs)\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=[\"mae\"])\n",
    "    history = model.fit(train_input,train_output, validation_split=0.2, epochs=epochs,batch_size=batch_size,\n",
    "                        callbacks=[early_stopping_cb,model_ckpt])\n",
    "    valid_loss, valid_mae = model.evaluate(valid_input,valid_output)\n",
    "    \n",
    "    #import joblib\n",
    "    #joblib.dump(model,f'{ckpt}_model.pkl')\n",
    "\n",
    "    # Save the model using TensorFlow's save method\n",
    "    model.save(f'{ckpt}_model.h5')\n",
    "    #loaded_model=joblib.load(f'{ckpt}_model.pkl')\n",
    "\n",
    "    #new_data=testX\n",
    "    #predictions=loaded_model.predict(new_data)\n",
    "    #print(predictions,testY)\n",
    "    # Plot training history\n",
    "    plt.plot(history.history['mae'], label='train_mae')\n",
    "    plt.plot(history.history['val_mae'], label='val_mae')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return valid_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()\n",
    "\n",
    "fit_and_evaluate(MLP_model, ghc_train_input,train_output, ghc_valid_input,valid_output, \n",
    "                learning_rate=0.001,ckpt=\"wcp_mlp_model\",epochs=100,patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()\n",
    "fit_and_evaluate(avg_LSTM_model, train_input,train_output, valid_input,valid_output, \n",
    "                learning_rate=0.001,ckpt=\"wcp_avg_LSTM_model\",epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "LSTM_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=[ghc_train_input.shape[1],ghc_train_input.shape[-1]]),\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=4, strides=1, padding=\"same\",\n",
    "                           activation=\"relu\"),\n",
    "    tf.keras.layers.LSTM(32,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32,return_sequences=False),\n",
    "    tf.keras.layers.Dense(3, activation=\"linear\")\n",
    "])\n",
    "LSTM_model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()\n",
    "\n",
    "fit_and_evaluate(LSTM_model, ghc_train_input,train_output, ghc_valid_input,valid_output, \n",
    "                learning_rate=0.001,ckpt=\"wcp_LSTM_model\",epochs=100,patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Conv1D, Bidirectional, Layer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()\n",
    "\n",
    "# Constants\n",
    "TIME_STEPS = 2016  # 7 days * 24 hours * 12 samples per hour (5 min intervals)\n",
    "CROP_PARAM_COUNT = 3  # fresh weight, dry weight, height, diameter\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "# Custom Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[-1]), initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        score = tf.nn.softmax(q, axis=1)\n",
    "        return score * inputs\n",
    "\n",
    "# Custom layer to handle the TensorFlow operations\n",
    "class RepeatAndExpandLayer(Layer):\n",
    "    def __init__(self, crop_param_count, **kwargs):\n",
    "        super(RepeatAndExpandLayer, self).__init__(**kwargs)\n",
    "        self.crop_param_count = crop_param_count\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.expand_dims(inputs, axis=1)\n",
    "        return tf.repeat(x, repeats=self.crop_param_count, axis=1)\n",
    "\n",
    "def build_model():\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(TIME_STEPS, ghc_train_input.shape[2]))\n",
    "    x = Conv1D(64, kernel_size=3, padding='same', activation='relu')(encoder_inputs)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    \n",
    "    # Use 'return_state=True' to capture LSTM states\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "        LSTM(128, return_sequences=False, return_state=True))(x)\n",
    "\n",
    "    # Combine forward and backward states\n",
    "    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
    "    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
    "\n",
    "    # Decoder (Single output instead of sequence)\n",
    "    decoder_inputs = encoder_outputs  # No need to expand/repeat for single output\n",
    "    x = Dense(256, activation='relu')(decoder_inputs)\n",
    "    decoder_outputs = Dense(CROP_PARAM_COUNT, activation='linear')(x)\n",
    "\n",
    "    # Define model\n",
    "    model = Model(encoder_inputs, decoder_outputs)\n",
    "    #model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build and train model\n",
    "EncDec_model = build_model()\n",
    "learning_rate=0.001\n",
    "ckpt=\"wcp_EncDec_model_\"\n",
    "epochs=50\n",
    "batch_size=32,\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_mae\", patience=5, restore_best_weights=True)\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "ckpt, monitor=\"val_mae\", save_best_only=True)\n",
    "opt = Adam(learning_rate=learning_rate, decay=learning_rate / epochs)\n",
    "EncDec_model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=[\"mae\"])\n",
    "\n",
    "EncDec_model.summary()\n",
    "#history = EncDec_model.fit(ghc_train_input, train_output, validation_split=0.2, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "history = EncDec_model.fit(ghc_train_input,train_output,  validation_split=0.2, epochs=EPOCHS,batch_size=BATCH_SIZE,\n",
    "                        callbacks=[early_stopping_cb,model_ckpt])\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "loss = EncDec_model.evaluate(ghc_valid_input, valid_output)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['mae'], label='train_mae')\n",
    "plt.plot(history.history['val_mae'], label='val_mae')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Example of making predictions\n",
    "def predict(model, time_series_data):\n",
    "    predictions = model.predict(time_series_data)\n",
    "    return predictions\n",
    "\n",
    "# Sample prediction\n",
    "#sample_predictions = predict(EncDec_model, ghc_valid_input[:5])\n",
    "#print(\"Sample Predictions:\\n\", sample_predictions)\n",
    "#print(\"Target:\\n\", valid_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging Step: Print shapes before training\n",
    "print(\"Input shape:\", ghc_train_input.shape)\n",
    "print(\"Output shape:\", train_output.shape)\n",
    "\n",
    "# Example of making predictions\n",
    "sample_predictions = EncDec_model.predict(ghc_valid_input[:5])\n",
    "print(\"Prediction shape:\", sample_predictions.shape)\n",
    "print(\"Target shape:\", valid_output[:5].shape)\n",
    "\n",
    "# Ensure that shapes of predictions and target outputs match\n",
    "assert sample_predictions.shape == valid_output[:5].shape, \"Shape mismatch between predictions and target outputs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()\n",
    "\n",
    "fit_and_evaluate(EncDec_model, ghc_train_input,train_output, ghc_valid_input,valid_output, \n",
    "                learning_rate=0.001,ckpt=\"wcp_EncDec_model\",epochs=50,patience=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
