{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Conv1D,Input,MaxPooling1D,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import  random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import dataframe_image as dfi\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#define the default font sizes:\n",
    "plt.rc('font', size=12)\n",
    "plt.rc('axes', labelsize=14, titlesize=12)\n",
    "plt.rc('legend', fontsize=12)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# save the figures\n",
    "IMAGES_PATH = Path() / \"images\" / \"final\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=1200):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather=pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\Master\\\\RoboticsAndControlSystems\\\\Project\\\\Data\\\\Weather\\\\Weather.csv\",parse_dates=[\"%time\"],low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weather_attribute in weather.columns:\n",
    "    x=weather.iloc[(np.where(weather[weather_attribute].isnull()))][weather_attribute]\n",
    "    for i in reversed(x.index):\n",
    "        if i+(12*24)< int(weather.index[-1]):\n",
    "            #print(i+(12*24))\n",
    "            weather.loc[i,weather_attribute]= weather.iloc[i+12*24][weather_attribute]\n",
    "            #print(i+(12*24),weather.loc[i,weather_attribute])\n",
    "for weather_attribute in weather.columns:  \n",
    "    x=weather.iloc[(np.where(weather[weather_attribute].isnull()))][weather_attribute]\n",
    "    for i in x.index:\n",
    "        if i-(12*24)>= 0:\n",
    "            #print(i+(12*24))\n",
    "            weather.loc[i,weather_attribute]= weather.iloc[i-12*24][weather_attribute]\n",
    "            #print(i+(12*24),weather.loc[i,weather_attribute])\n",
    "\n",
    "print(weather.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather=weather.set_index('%time').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def convert_to_preferred_format(sec):\n",
    "#   sec = sec % (24 * 3600)\n",
    "#   hour = sec // 3600\n",
    "#   sec %= 3600\n",
    "#   min = sec // 60\n",
    "#   sec %= 60\n",
    "#   print(\"seconds value in hours:\",hour)\n",
    "#   print(\"seconds value in minutes:\",min)\n",
    "#   return \"%02d:%02d:%02d\" % (hour, min, sec) \n",
    "#\n",
    "#timestamp = weather['%time']\n",
    "#print(\"Time in preferred format :-\",convert_to_preferred_format(timestamp[0]))\n",
    "#\n",
    "#from datetime import datetime\n",
    "# \n",
    "# \n",
    "#timestamp = weather['%time']\n",
    "#dt_obj = datetime.fromtimestamp(timestamp[0])\n",
    "# \n",
    "#print(\"date_time:\",dt_obj)\n",
    "#print(\"type of dt:\",type(dt_obj))\n",
    "#import time \n",
    "#time.strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.gmtime(timestamp[0]))\n",
    "#import datetime; \n",
    "#datetime.datetime.utcfromtimestamp(timestamp[0]).replace(tzinfo=datetime.timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather=weather.set_index('%time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.hist(bins=50,figsize=(20,15),column=weather.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['rain', 'no rain']\n",
    "values=[(weather['Rain']==1).sum(),(weather['Rain']==0).sum()]\n",
    "weather['Rain'][weather['Rain']<0.2]=0\n",
    "weather['Rain'][weather['Rain']>0.2]=1\n",
    "\n",
    "values=[(weather['Rain']==1).sum(),(weather['Rain']==0).sum()]\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.bar(labels,values)\n",
    "ax.set_ylabel('rain')\n",
    "ax.set_title('Rain')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = weather.corr()['Rain'].sort_values(ascending=False)\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas.plotting import scatter_matrix\n",
    "#attributes = weather.columns.drop('Rain')\n",
    "#scatter_matrix(weather[attributes], figsize=(15, 10))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from matplotlib import pyplot as plt\n",
    "#T=.1\n",
    "##weather['Rain'].iloc[weather['Rain']<T]=0\n",
    "##weather['Rain'].iloc[weather['Rain']>T]=1\n",
    "#for col in attributes:\n",
    "#    weather.boxplot(column=col, by='Rain', figsize=(6,6))\n",
    "#    plt.title(col)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate=pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\Master\\\\RoboticsAndControlSystems\\\\Project\\\\Data\\\\Automatoes\\\\GreenhouseClimate.csv\",parse_dates=[\"%time\"],low_memory=False).set_index('%time').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate.head(5)\n",
    "green_house_climate['Water_Quantity']=green_house_climate['Cum_irr']\n",
    "green_house_climate['Duration_of_Irrigation']=green_house_climate['water_sup']\n",
    "green_house_climate['Irrigation_Time_Intervals']=green_house_climate['water_sup_intervals_vip_min']\n",
    "\n",
    "green_house_climate.drop(columns=['water_sup','Cum_irr','water_sup_intervals_vip_min'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#green_house_climate.drop(columns=[  'AssimLight', 'BlackScr', \n",
    "#       'EnScr',  'PipeGrow', 'PipeLow', \n",
    "#       'Tot_PAR_Lamps', 'VentLee', 'Ventwind', 'assim_sp', 'assim_vip',\n",
    "#        'co2_sp', 'co2_vip', 'dx_sp', 'dx_vip', 'int_blue_sp',\n",
    "#       'int_blue_vip', 'int_farred_sp', 'int_farred_vip', 'int_red_sp',\n",
    "#       'int_red_vip', 'int_white_sp', 'int_white_vip', \n",
    "#       'scr_blck_sp', 'scr_blck_vip', 'scr_enrg_sp', 'scr_enrg_vip',\n",
    "#       't_grow_min_sp', 't_grow_min_vip', 't_heat_sp', 't_heat_vip',\n",
    "#       't_rail_min_sp', 't_rail_min_vip', 't_vent_sp', 't_ventlee_vip',\n",
    "#       't_ventwind_vip',  'window_pos_lee_sp','water_sup_intervals_sp_min',\n",
    "#       'window_pos_lee_vip'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate.info()\n",
    "green_house_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#green_house_climate[np.where(green_house_climate.isnull()==True)]\n",
    "#print(green_house_climate['Water_Quantity'].value_counts())\n",
    "\n",
    "#print(green_house_climate['Irrigation_Time_Intervals'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate.hist(bins=10,figsize=(100,85),grid=[5,5],column=green_house_climate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram for each numeric feature\n",
    "numeric_feature=green_house_climate.columns\n",
    "for col in numeric_feature:\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.gca()\n",
    "    feature = green_house_climate[col]\n",
    "    feature.hist(bins=30, ax = ax)\n",
    "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n",
    "    ax.set_title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Outliers in numarical features by using boxplot :\n",
    "#from matplotlib import pyplot as plt\n",
    "#\n",
    "#for col in green_house_climate.columns:\n",
    "#    green_house_climate.boxplot(column=col, figsize=(3,3))\n",
    "#    plt.title(col)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather.set_index('%time')\n",
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for num_attribute in green_house_climate.columns:\n",
    "#        for weather_attribute in weather.columns:\n",
    "#            plt.scatter(x=weather[weather_attribute],y=green_house_climate[num_attribute])\n",
    "#            plt.show()\n",
    "\n",
    "#find a trend \n",
    "#weather['time']=weather.index\n",
    "##print(weather['%time'].astype('string').str.split('.',expand=True).astype('int')[0])\n",
    "#x=weather['time'].astype('string').str.split('/',expand=True).astype('int')[1]\n",
    "#weather['time']=x\n",
    "\n",
    "\n",
    "\n",
    "#print('weather[time]',weather.index)\n",
    "#print('************')\n",
    "#weather_attributes=weather.columns.drop(['time'])\n",
    "for weather_attribute in weather.columns:\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.gca()\n",
    "            dayly_avg = weather[weather_attribute].resample('H').mean()  # compute the mean for each hour\n",
    "            #print(dayly_avg)\n",
    "            period=slice(\"2019-12-16 00:00:00\", \"2020-05-30 00:00:00\")\n",
    "            rolling_average_dayly = dayly_avg[period].rolling(window=24*10).mean()\n",
    "            #dayly_avg=weather.groupby(by=['time'])[weather_attribute].mean()\n",
    "            #print(weather_attribute,weather.groupby(by=['time'])[weather_attribute].mean())\n",
    "            weather[weather_attribute].plot(ax=ax,ylabel=weather_attribute,xlabel='Day Index',grid='True', marker=\".\", figsize=(18, 6))\n",
    "            \n",
    "            #rolling_average_dayly = dayly_avg.rolling(window=10).mean()\n",
    "            #print(weather_attribute,rolling_average_dayly)\n",
    "            rolling_average_dayly.plot(grid=True)\n",
    "            ax.set_xlabel(xlabel='%time')\n",
    "            ax.set_ylabel(ylabel=weather_attribute)\n",
    "            ax.set_title(weather_attribute)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weather_attribute in weather.columns:\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.gca()\n",
    "            \n",
    "            weather[\"2019-12-16 00:00:00\" : \"2019-12-30 00:00:00\"][weather_attribute].plot(ylabel=weather_attribute,xlabel='Day Index',grid='True', marker=\".\", figsize=(18, 6))\n",
    "            #ax.set_xlabel(xlabel='%time')\n",
    "            #ax.set_ylabel(ylabel=weather_attribute)\n",
    "            ax.set_title(weather_attribute)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from matplotlib import pyplot as plt\n",
    "## 'Water_Quantity', 'Duration_of_Irrigation','Irrigation_Time_Intervals\n",
    "#for col in green_house_climate.columns:\n",
    "#    green_house_climate.boxplot(column=col, by='Duration_of_Irrigation', figsize=(6,6))\n",
    "#    plt.title(col)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the normal difference between the set point of a feature and the measured value from sensors\n",
    "x=green_house_climate['assim_sp']-green_house_climate['assim_vip']\n",
    "x.mean()/green_house_climate['assim_sp'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=green_house_climate['co2_sp']-green_house_climate['co2_vip']\n",
    "x.mean()/green_house_climate['co2_sp'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_vip_paired_attributtes=[ [('assim_sp','assim_vip'),('assim_vip','assim_sp')],\n",
    "                            [('co2_sp','co2_vip'),('co2_vip','co2_sp')],\n",
    "                            [('dx_sp','dx_vip'),('dx_vip','dx_sp')],\n",
    "                            [('int_blue_sp','int_blue_vip'),('int_blue_vip','int_blue_sp')],\n",
    "                            [('int_farred_sp','int_farred_vip'),('int_farred_vip','int_farred_sp')],\n",
    "                            [('int_red_sp','int_red_vip'),('int_red_vip','int_red_sp')], \n",
    "                            [('int_white_sp','int_white_vip'),('int_white_vip','int_white_sp')],\n",
    "                            [('scr_blck_sp','scr_blck_vip'),('scr_blck_vip','scr_blck_sp')],\n",
    "                            [('scr_enrg_sp','scr_enrg_vip'),('scr_enrg_vip','scr_enrg_sp')],\n",
    "                            [('t_grow_min_sp','t_grow_min_vip'),('t_grow_min_vip','t_grow_min_sp')],\n",
    "                            [('t_heat_sp','t_heat_vip'),('t_heat_vip','t_heat_sp')],\n",
    "                            [('t_rail_min_sp','t_rail_min_vip'),('t_rail_min_vip','t_rail_min_sp')],\n",
    "                            [('t_vent_sp','t_ventlee_vip'),('t_ventlee_vip','t_vent_sp')],\n",
    "                            [('water_sup_intervals_sp_min','Irrigation_Time_Intervals'),('Irrigation_Time_Intervals','water_sup_intervals_sp_min')],\n",
    "                            [('window_pos_lee_sp','window_pos_lee_vip'),('window_pos_lee_vip','window_pos_lee_sp')],\n",
    "                            ]\n",
    "norm_mean_difference=[]\n",
    "similarity=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sp_vip_paired_attributte in sp_vip_paired_attributtes: \n",
    "    for (sp, vip) in sp_vip_paired_attributte:\n",
    "        x=green_house_climate.iloc[(np.where(green_house_climate[sp].isnull()))][vip]\n",
    "        #print(green_house_climate[sp])\n",
    "        difference=(green_house_climate[sp]-green_house_climate[vip]).mean()\n",
    "        mean_difference=difference/(green_house_climate[sp].mean())\n",
    "        norm_mean_difference.append([sp,vip,mean_difference])\n",
    "        \n",
    "        #print(f'{sp},{vip} difference={norm_mean_difference}')\n",
    "        #print(x.index)\n",
    "        for i in x.index:\n",
    "            #print(i)\n",
    "            green_house_climate.loc[i,sp]= green_house_climate.loc[i,vip]#.iloc[i][vip]\n",
    "            #print(i,green_house_climate.loc[i,sp]) \n",
    "        plt.figure()\n",
    "        correlation = green_house_climate.corr()[vip][sp]\n",
    "        corr1=green_house_climate.corr()[vip].sort_values(ascending=False)\n",
    "        corr1.plot(kind='bar',title=f'{vip} SP-VIP Correlation',figsize=(18, 6),fontsize=12)\n",
    "        plt.show\n",
    "        save_fig(f'{vip}SP-VIP Correlation')\n",
    "        print(f\"correlation between {vip} and {sp}=\",correlation)\n",
    "        plt.show\n",
    "norm_mean_difference     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_diff=pd.DataFrame(np.array(norm_mean_difference),columns=['sp-vip','vip-sp','Norm difference'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "green_house_climate.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=green_house_climate.iloc[(np.where(green_house_climate['assim_sp'].isnull()))]['assim_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'assim_sp']= green_house_climate.iloc[i]['assim_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'assim_sp'])  \n",
    "#        \n",
    "##green_house_climate.iloc[np.where(green_house_climate['assim_sp'].isnull())]['assim_sp']  =    green_house_climate.iloc[np.where(green_house_climate['assim_sp'].isnull())]['assim_vip']                 #785 \n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['assim_vip'].isnull()))]['assim_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'assim_vip']= green_house_climate.iloc[i]['assim_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'assim_vip'])  \n",
    "#\n",
    "#\n",
    "#\n",
    "##green_house_climate['assim_vip']                        #71   \n",
    "#        \n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['co2_sp'].isnull()))]['co2_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'co2_sp']= green_house_climate.iloc[i]['co2_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'co2_sp']) \n",
    "##green_house_climate['co2_dos']                        #2    \n",
    "##green_house_climate.iloc[np.where(green_house_climate['co2_sp'].isnull())]['co2_sp']  =    green_house_climate.iloc[np.where(green_house_climate['co2_sp'].isnull())]['co2_vip']                        #754\n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['co2_vip'].isnull()))]['co2_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'co2_vip']= green_house_climate.iloc[i]['co2_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'co2_vip']) \n",
    "#\n",
    "#\n",
    "##green_house_climate['co2_vip']                       #71   \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['dx_sp'].isnull()))]['dx_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'dx_sp']= green_house_climate.iloc[i]['dx_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'dx_sp'])  \n",
    "#        \n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['dx_sp'].isnull())]['dx_sp']  =    green_house_climate.iloc[np.where(green_house_climate['dx_sp'].isnull())]['dx_vip']                       #718  \n",
    "#        \n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['dx_vip'].isnull()))]['dx_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'dx_vip']= green_house_climate.iloc[i]['dx_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'dx_vip'])  \n",
    "#        \n",
    "#\n",
    "##green_house_climate['dx_vip']                       #71   \n",
    "#        \n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['int_blue_sp'].isnull()))]['int_blue_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'int_blue_sp']= green_house_climate.iloc[i]['int_blue_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'int_blue_sp'])  \n",
    "#        \n",
    "#\n",
    "##green_house_climate['int_blue_sp']                       #173  \n",
    "#\n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['int_blue_vip'].isnull()))]['int_blue_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'int_blue_vip']= green_house_climate.iloc[i]['int_blue_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'int_blue_vip'])  \n",
    "#        \n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['int_blue_vip'].isnull())]['int_blue_vip']  =    green_house_climate.iloc[np.where(green_house_climate['int_blue_vip'].isnull())]['int_blue_sp']                   #22152\n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['int_farred_vip'].isnull()))]['int_farred_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'int_farred_vip']= green_house_climate.iloc[i]['int_farred_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'int_farred_vip'])  \n",
    "#\n",
    "##green_house_climate['int_farred_sp']                   #14   \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['int_farred_sp'].isnull()))]['int_farred_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'int_farred_sp']= green_house_climate.iloc[i]['int_farred_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'int_farred_sp']) \n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['int_farred_vip'].isnull())]['int_farred_vip']  =    green_house_climate.iloc[np.where(green_house_climate['int_farred_vip'].isnull())]['int_farred_sp']                   #22152\n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['int_red_vip'].isnull()))]['int_red_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'int_red_vip']= green_house_climate.iloc[i]['int_red_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'int_red_vip'])  \n",
    "#\n",
    "#\n",
    "#\n",
    "##green_house_climate['int_red_sp']                   #173  \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['int_red_sp'].isnull()))]['int_red_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'int_red_sp']= green_house_climate.iloc[i]['int_red_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'int_red_sp'])  \n",
    "#\n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['int_red_vip'].isnull())]['int_red_vip']  =    green_house_climate.iloc[np.where(green_house_climate['int_red_vip'].isnull())]['int_red_sp']                   #22152\n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['int_white_vip'].isnull()))]['int_white_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'int_white_vip']= green_house_climate.iloc[i]['int_white_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'int_white_vip'])  \n",
    "#\n",
    "#\n",
    "##green_house_climate['int_white_sp']                   #14   \n",
    "#        \n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['int_white_sp'].isnull()))]['int_white_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'int_white_sp']= green_house_climate.iloc[i]['int_white_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'int_white_sp'])  \n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['int_white_vip'].isnull())]['int_white_vip']  =    green_house_climate.iloc[np.where(green_house_climate['int_white_vip'].isnull())]['int_white_sp']                   #22152\n",
    "##green_house_climate['pH_drain_PC']                   #71   \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['scr_blck_sp'].isnull()))]['scr_blck_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'scr_blck_sp']= green_house_climate.iloc[i]['scr_blck_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'scr_blck_sp'])  \n",
    "#\n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['scr_blck_sp'].isnull())]['scr_blck_sp']  =    green_house_climate.iloc[np.where(green_house_climate['scr_blck_sp'].isnull())]['scr_blck_vip']                   #783  \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['scr_blck_vip'].isnull()))]['scr_blck_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'scr_blck_vip']= green_house_climate.iloc[i]['scr_blck_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'scr_blck_vip'])  \n",
    "#\n",
    "#\n",
    "#\n",
    "##green_house_climate['scr_blck_vip']                   #71   \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['scr_enrg_sp'].isnull()))]['scr_enrg_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'scr_enrg_sp']= green_house_climate.iloc[i]['scr_enrg_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'scr_enrg_sp'])  \n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['scr_enrg_sp'].isnull())]['scr_enrg_sp']  =    green_house_climate.iloc[np.where(green_house_climate['scr_enrg_sp'].isnull())]['scr_enrg_vip']                  #785  \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['scr_enrg_vip'].isnull()))]['scr_enrg_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'scr_enrg_vip']= green_house_climate.iloc[i]['scr_enrg_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'scr_enrg_vip'])  \n",
    "#\n",
    "#\n",
    "#\n",
    "##green_house_climate['scr_enrg_vip']                   #71   \n",
    "#\n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['t_grow_min_sp'].isnull()))]['t_grow_min_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'t_grow_min_sp']= green_house_climate.iloc[i]['t_grow_min_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'t_grow_min_sp'])  \n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['t_grow_min_sp'].isnull())]['t_grow_min_sp']  =    green_house_climate.iloc[np.where(green_house_climate['t_grow_min_sp'].isnull())]['t_grow_min_vip']                  #1505 \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['t_grow_min_vip'].isnull()))]['t_grow_min_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'t_grow_min_vip']= green_house_climate.iloc[i]['t_grow_min_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'t_grow_min_vip'])  \n",
    "#\n",
    "##green_house_climate['t_grow_min_vip']                   #71   \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['t_heat_sp'].isnull()))]['t_heat_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'t_heat_sp']= green_house_climate.iloc[i]['t_heat_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'t_heat_sp'])  \n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['t_heat_sp'].isnull())]['t_heat_sp']  =    green_house_climate.iloc[np.where(green_house_climate['t_heat_sp'].isnull())]['t_heat_vip']                   #706  \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['t_heat_vip'].isnull()))]['t_heat_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'t_heat_vip']= green_house_climate.iloc[i]['t_heat_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'t_heat_vip'])  \n",
    "#\n",
    "##green_house_climate['t_heat_vip']                   #71   \n",
    "#\n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['t_rail_min_sp'].isnull()))]['t_rail_min_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'t_rail_min_sp']= green_house_climate.iloc[i]['t_rail_min_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'t_rail_min_sp'])  \n",
    "#\n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['t_rail_min_sp'].isnull())]['t_rail_min_sp']  =    green_house_climate.iloc[np.where(green_house_climate['t_rail_min_sp'].isnull())]['t_rail_min_vip']                   #787  \n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['t_rail_min_vip'].isnull()))]['t_rail_min_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'t_rail_min_vip']= green_house_climate.iloc[i]['t_rail_min_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'t_rail_min_vip'])  \n",
    "#\n",
    "#\n",
    "##green_house_climate['t_rail_min_vip']                   #71   \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['t_vent_sp'].isnull()))]['t_vent_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'t_vent_sp']= green_house_climate.iloc[i]['t_ventlee_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'t_vent_sp'])  \n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['t_vent_sp'].isnull())]['t_vent_sp']  =    green_house_climate.iloc[np.where(green_house_climate['t_vent_sp'].isnull())]['t_ventlee_vip']                   #12576\n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['t_ventlee_vip'].isnull()))]['t_ventlee_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'t_ventlee_vip']= green_house_climate.iloc[i]['t_vent_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'t_ventlee_vip'])  \n",
    "#\n",
    "#\n",
    "#\n",
    "##green_house_climate['t_ventlee_vip']                   #71   \n",
    "##green_house_climate['t_ventwind_vip']                   #71   \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['water_sup_intervals_sp_min'].isnull()))]['water_sup_intervals_sp_min']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'water_sup_intervals_sp_min']= green_house_climate.iloc[i]['Irrigation_Time_Intervals']\n",
    "#        #print(i,green_house_climate.loc[i,'water_sup_intervals_sp_min'])  \n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['water_sup_intervals_sp_min'].isnull())]['water_sup_intervals_sp_min']  =    green_house_climate.iloc[np.where(green_house_climate['water_sup_intervals_sp_min'].isnull())]['Irrigation_Time_Intervals']    #1410 \n",
    "#\n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['window_pos_lee_sp'].isnull()))]['window_pos_lee_sp']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'window_pos_lee_sp']= green_house_climate.iloc[i]['window_pos_lee_vip']\n",
    "#        #print(i,green_house_climate.loc[i,'window_pos_lee_sp'])  \n",
    "#\n",
    "#\n",
    "##green_house_climate.iloc[np.where(green_house_climate['window_pos_lee_sp'].isnull())]['window_pos_lee_sp']  =    green_house_climate.iloc[np.where(green_house_climate['window_pos_lee_sp'].isnull())]['window_pos_lee_vip']    #715  \n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['window_pos_lee_vip'].isnull()))]['window_pos_lee_vip']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'window_pos_lee_vip']= green_house_climate.iloc[i]['window_pos_lee_sp']\n",
    "#        #print(i,green_house_climate.loc[i,'window_pos_lee_vip'])  \n",
    "#\n",
    "##green_house_climate['window_pos_lee_vip']    #71   \n",
    "##green_house_climate['Water_Quantity']    #71   \n",
    "##green_house_climate['Duration_of_Irrigation']    #71   \n",
    "##green_house_climate['Irrigation_Time_Intervals']    #71   \n",
    "#x=green_house_climate.iloc[(np.where(green_house_climate['Irrigation_Time_Intervals'].isnull()))]['Irrigation_Time_Intervals']\n",
    "#for i in x.index:\n",
    "#        #print(i)\n",
    "#        green_house_climate.loc[i,'Irrigation_Time_Intervals']= green_house_climate.iloc[i]['water_sup_intervals_sp_min']\n",
    "#        #print(i,green_house_climate.loc[i,'Irrigation_Time_Intervals'])  \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for gh_attribute in green_house_climate.columns:\n",
    "    x=green_house_climate.iloc[(np.where(green_house_climate[gh_attribute].isnull()))][gh_attribute]\n",
    "    for i in reversed(x.index):\n",
    "        if i.day+(1)< green_house_climate.index[-1].day:\n",
    "            #print(i+(12*24))\n",
    "            d=i\n",
    "            print(d)\n",
    "            \n",
    "            d = d.replace(day=d.day+1)\n",
    "           \n",
    "            print(d)\n",
    "            green_house_climate.loc[i,gh_attribute]= green_house_climate.loc[d,gh_attribute]\n",
    "            #print(i+(12*24),green_house_climate.loc[i,gh_attribute])\n",
    "for gh_attribute in green_house_climate.columns:\n",
    "    x=green_house_climate.iloc[(np.where(green_house_climate[gh_attribute].isnull()))][gh_attribute]\n",
    "    for i in x.index:\n",
    "        if i.day-(1)>= 0:\n",
    "            #print(i+(12*24))\n",
    "            d=i\n",
    "            print(d)\n",
    "            d = d.replace(day=d.day-1)\n",
    "            print(d)\n",
    "            green_house_climate.loc[i,gh_attribute]= green_house_climate.loc[d,gh_attribute]\n",
    "            #print(i+(12*24),green_house_climate.loc[i,gh_attribute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#green_house_climate = green_house_climate.set_index(\"%time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_zone=pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\Master\\\\RoboticsAndControlSystems\\\\Project\\\\Data\\\\Automatoes\\\\GrodanSens.csv\",parse_dates=[\"%time\"],low_memory=False).set_index('%time').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_zone.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_zone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_zone.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=root_zone.iloc[(np.where(root_zone['EC_slab1'].isnull()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for root_attribute in ['EC_slab1'  , 'EC_slab2' , 'WC_slab1' , 'WC_slab2'   , 't_slab1','t_slab2' ]:\n",
    "#    for i in reversed(x.index):\n",
    "#        if i.day+1< int(root_zone.index[-1].day):\n",
    "#            #print(i+(12*24))\n",
    "#            root_zone.loc[i,root_attribute]= root_zone.iloc[i+12*24][root_attribute]\n",
    "#            #print(i+(12*24),root_zone.loc[i,root_attribute])\n",
    "\n",
    "\n",
    "for rz_attribute in root_zone.columns:\n",
    "    x=root_zone.iloc[(np.where(root_zone[rz_attribute].isnull()))][rz_attribute]\n",
    "    for i in reversed(x.index):\n",
    "        if i.day+(1)< root_zone.index[-1].day:\n",
    "            #print(i+(12*24))\n",
    "            d=i\n",
    "            print(d)\n",
    "            \n",
    "            d = d.replace(day=d.day+1)\n",
    "           \n",
    "            print(d)\n",
    "            root_zone.loc[i,rz_attribute]= root_zone.loc[d,rz_attribute]\n",
    "            #print(i+(12*24),root_zone.loc[i,rz_attribute])\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for root_attribute in ['EC_slab1'  , 'EC_slab2' , 'WC_slab1' , 'WC_slab2'   , 't_slab1','t_slab2' ]:\n",
    "#    for i in x.index:\n",
    "#        if i-(12*24)>= 0:\n",
    "#            #print(i+(12*24))\n",
    "#            root_zone.loc[i,root_attribute]= root_zone.iloc[i-12*24][root_attribute]\n",
    "#            #print(i+(12*24),root_zone.loc[i,root_attribute])\n",
    "\n",
    "for rz_attribute in root_zone.columns:\n",
    "    x=root_zone.iloc[(np.where(root_zone[rz_attribute].isnull()))][rz_attribute]\n",
    "    for i in x.index:\n",
    "        if i.day-(1)>= root_zone.index[0].day:\n",
    "            #print(i+(12*24))\n",
    "            d=i\n",
    "            print(d)\n",
    "            \n",
    "            d = d.replace(day=d.day-1)\n",
    "           \n",
    "            print(d)\n",
    "            root_zone.loc[i,rz_attribute]= root_zone.loc[d,rz_attribute]\n",
    "            #print(i+(12*24),root_zone.loc[i,rz_attribute])\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_zone.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_zone = root_zone.sort_values(\"%time\").set_index(\"%time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_zone.hist(bins=50,figsize=(20,15),column=root_zone.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_zone.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house_climate.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house=pd.concat([weather,root_zone,green_house_climate],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.columns.shape+green_house_climate.columns.shape+root_zone.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.concatenate([weather.columns.tolist(),root_zone.columns.tolist(),green_house_climate.columns.tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',43000)\n",
    "print(green_house.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house=pd.DataFrame(green_house,columns=np.concatenate([weather.columns.tolist(),root_zone.columns.tolist(),green_house_climate.columns.tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_day = 12 * 24\n",
    "#for row in range(green_house.shape[0]):\n",
    "#\tfor col in range(green_house.shape[1]):\n",
    "#\t\tif np.isnan(green_house.iloc[row][col]):\n",
    "#\t\t\tgreen_house.iloc[row][col] = green_house.iloc[row - one_day][col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#green_house.iloc[np.where(green_house['EC_slab1'].isnull())]['EC_slab1'] = green_house.iloc[np.where(green_house['EC_slab1'].isnull())]['EC_slab1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for num_attribute in green_house.columns:\n",
    "    \n",
    "    green_house[\"2019-12-16 00:00:00\" : \"2019-12-25 00:00:00\"][num_attribute].plot(ylabel=num_attribute,grid=True, marker=\".\", figsize=(18, 6))\n",
    "    save_fig(f'green_house_{num_attribute}_time_series_plot')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#find a trend \n",
    "#green_house['time']=green_house.index\n",
    "##print(green_house['%time'].astype('string').str.split('.',expand=True).astype('int')[0])\n",
    "#x=green_house['time'].astype('string').str.split('.',expand=True).astype('int')[0]\n",
    "#green_house['time']=x\n",
    "#print('green_house[time]',green_house.index)\n",
    "#print('************')\n",
    "for green_house_attribute in green_house.columns:\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.gca()\n",
    "            \n",
    "            dayly_avg=green_house[green_house_attribute].resample('H').mean()\n",
    "            period=slice(\"2019-12-16 00:00:00\", \"2020-05-30 00:00:00\")\n",
    "            #print(green_house_attribute,green_house.groupby(by=['time'])[green_house_attribute].mean())\n",
    "            green_house[green_house_attribute].plot(ax=ax,ylabel=green_house_attribute,xlabel='Day Index',grid='True', marker=\".\", figsize=(18, 6))\n",
    "            \n",
    "            rolling_average_dayly = dayly_avg[period].rolling(window=24*10).mean()\n",
    "            #print(green_house_attribute,rolling_average_dayly)\n",
    "            rolling_average_dayly.plot(grid=True)\n",
    "            ax.set_xlabel(xlabel='%time')\n",
    "            ax.set_ylabel(ylabel=green_house_attribute)\n",
    "            ax.set_title(green_house_attribute)\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house['%time']=green_house.index\n",
    "green_house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#green_house['%time']=green_house.index\n",
    "#out=green_house['%time'].astype('string').str.split('.',expand=True).reset_index()\n",
    "#out\n",
    "#out['days']=np.zeros(shape=out[0].shape[0])\n",
    "#out['hours']=np.zeros(shape=out[0].shape[0])\n",
    "#out['minutes']=np.zeros(shape=out[0].shape[0])\n",
    "#\n",
    "#out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out=np.load('C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\Master\\\\RoboticsAndControlSystems\\\\Project\\\\Documentation\\\\out.np.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house['years'] =   green_house['%time'].dt.year\n",
    "green_house['months'] =  green_house['%time'].dt.month\n",
    "green_house['days'] =    green_house['%time'].dt.day\n",
    "green_house['hours'] =   green_house['%time'].dt.hour\n",
    "green_house['minutes'] = green_house['%time'].dt.minute\n",
    "green_house.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day_count=1\n",
    "#five_min_counts=1\n",
    "#for i in (out.groupby(by=[0])[0].sum().index):\n",
    "#    #print(i)\n",
    "#    five_min_countsmin_counts=1\n",
    "#    hours_count=1\n",
    "#    for idx in out[0].index:     \n",
    "#        if out[0].iloc[idx]==i:\n",
    "#            out['days'].iloc[idx]=day_count\n",
    "#            #print('done',idx)\n",
    "#            out['hours'].iloc[idx]=hours_count\n",
    "#            out['minutes'].iloc[idx]=five_min_counts*5\n",
    "#            five_min_counts+=1\n",
    "#            if five_min_counts>12:\n",
    "#                hours_count+=1\n",
    "#                five_min_counts=1\n",
    "#    day_count+=1\n",
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day_count=1\n",
    "#min_counts=1\n",
    "#for i in (out.groupby(by=[0])[0].sum().index):\n",
    "#    #print(i)\n",
    "#    min_counts=1\n",
    "#    hours_count=1\n",
    "#    for idx in out[0].index:     \n",
    "#        if out[0].iloc[idx]==i:\n",
    "#            out['days'].iloc[idx]=day_count\n",
    "#            #print('done',idx)\n",
    "#            out['hours'].iloc[idx]=hours_count\n",
    "#            min_counts+=1\n",
    "#            if min_counts>12:\n",
    "#                hours_count+=1\n",
    "#                min_counts=1\n",
    "#    day_count+=1\n",
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(green_house['days'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(green_house['minutes'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(green_house['hours'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house['days'].values\n",
    "green_house['hours'].values\n",
    "green_house['minutes'].values\n",
    "\n",
    "#green_house.drop('%time',inplace=True)\n",
    "#green_house['hours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#green_house.drop(columns=['minutes'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#green_house.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for num_attribute in green_house.columns:\n",
    "    \n",
    "    green_house[\"2019-12-16 00:00:00\" : \"2020-12-26 00:00:00\"][num_attribute].plot(ylabel=num_attribute,xlabel='Day Index',grid=True, marker=\".\", figsize=(18, 6),title=num_attribute)\n",
    "    \n",
    "    save_fig(f'green_house_{num_attribute}_time_series_plot')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house[green_house.columns].corr()['Irrigation_Time_Intervals'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_house[green_house.columns].corr()['BlackScr'\t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr0=green_house[green_house.columns].corr()['Water_Quantity'].sort_values(ascending=False)\n",
    "corr0.drop(['Irrigation_Time_Intervals','Water_Quantity','Duration_of_Irrigation'],axis=0,inplace=True)\n",
    "corr0.plot(kind='bar',title='Water_Quantity Correlation',figsize=(18, 6),fontsize=12)\n",
    "save_fig('Water_Quantity Correlation with features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1=green_house[green_house.columns].corr()['Irrigation_Time_Intervals'].sort_values(ascending=False)\n",
    "corr1.drop(['Irrigation_Time_Intervals','Water_Quantity','Duration_of_Irrigation'],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1.plot(kind='bar',title='Irrigation_Time_Intervals Correlation',figsize=(18, 6),fontsize=12)\n",
    "save_fig('Irrigation_Time_Intervals Correlation with features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr2=green_house[green_house.columns].corr()['Duration_of_Irrigation'].sort_values(ascending=False)\n",
    "corr2.drop(['Irrigation_Time_Intervals','Water_Quantity','Duration_of_Irrigation'],axis=0,inplace=True)\n",
    "corr2.plot(kind='bar',title='Duration_of_Irrigation Correlation',figsize=(18, 6),fontsize=12)\n",
    "save_fig(f'Duration_of_Irrigation Correlation with features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_values=pd.concat([corr0,corr1,corr2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_values\n",
    "headers = {\n",
    "    \"selector\": \"th:not(.index_name)\",\n",
    "    \"props\": \"background-color: #800000; color: white; text-align: center\"\n",
    "}\n",
    "properties = {\"border\": \"1px solid black\", \"width\": \"65px\", \"text-align\": \"center\"}\n",
    "cell_hover = {\n",
    "    \"selector\": \"td:hover\",\n",
    "    \"props\": [(\"background-color\", \"#FFFFE0\")]\n",
    "}\n",
    "index_names = {\n",
    "    \"selector\": \".index_name\",\n",
    "    \"props\": \"font-style: italic; color: darkgrey; font-weight:normal;\"\n",
    "}\n",
    "correlation_values.style.format(precision=2).set_table_styles([cell_hover, index_names, headers]).set_properties(**properties)\n",
    "correlation_values.style.highlight_max(color=\"red\",axis=0)\n",
    "correlation_values.style.highlight_min(color=\"red\",axis=0)\n",
    "\n",
    "def mean_highlighter(x):\n",
    "    style_lt = \"background-color: whight\"\n",
    "    style_gt = \"background-color: red\"\n",
    "    gt_mean = abs(x) > .4\n",
    "    return [style_gt if i else style_lt for i in gt_mean]\n",
    "    \n",
    "    \n",
    "correlation_values.style.apply(mean_highlighter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('green_house.npy',green_house)\n",
    "#print(out['hours'].value_counts())\n",
    "#print(out['days'].value_counts())\n",
    "#out=np.load('out.npy')\n",
    "green_house.to_csv('C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\greenhouse_fill_missing_values.csv')\n",
    "weather.to_csv('C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\weather_fill_missing_values.csv')\n",
    "green_house_climate.to_csv('C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\greenhouse_climate_fill_missing_values.csv')\n",
    "root_zone.to_csv('C:\\\\Users\\\\user\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\root_zone_fill_missing_values.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer,make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor\n",
    "from sklearn.linear_model import Lasso,Ridge,ElasticNet\n",
    "from sklearn.preprocessing import FunctionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=green_house[green_house['Irrigation_Time_Intervals'].isnull()==True].index\n",
    "green_house.drop(index=index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(green_house.loc[:,['Water_Quantity','Duration_of_Irrigation','Irrigation_Time_Intervals']])\n",
    "\n",
    "X=green_house.drop(['Water_Quantity','Duration_of_Irrigation','Irrigation_Time_Intervals'],axis=1).copy()\n",
    "print(y.shape)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0,shuffle=True,\n",
    "                                                    stratify=green_house['Rain'])\n",
    "\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#min_max_scaler=MinMaxScaler()\n",
    "#print(X_train.shape)\n",
    "#y_train=min_max_scaler.fit_transform(y_train)\n",
    "#y_test=min_max_scaler.transform(y_test)\n",
    "#print('y_train',y_train.shape)\n",
    "#print('y_test',y_test.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean(axis=0)\n",
    "y_train.std(axis=0)\n",
    "\n",
    "#11.551732063293457,\n",
    "# 2.028500556945801,\n",
    "# 211.20957946777344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_squared_error,r2_score\n",
    "#from sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate\n",
    "#Water_Quantity_train_results=[] \n",
    "#Irrigation_Time_Intervals_train_results=[]\n",
    "#Duration_of_Irrigation_train_results=[]\n",
    "#Water_Quantity_test_results=[] \n",
    "#Irrigation_Time_Intervals_test_results=[]\n",
    "#Duration_of_Irrigation_test_results=[]\n",
    "#train_rmse=dict()\n",
    "#train_r2=dict()\n",
    "#train_mse=dict()\n",
    "#test_rmse=dict()\n",
    "#test_r2=dict()\n",
    "#test_mse=dict()\n",
    "#confidence_intervals=[]\n",
    "#for key in models.keys():\n",
    "#    full_pipeline = Pipeline([\n",
    "#        ('preprocessing', preprocessing),\n",
    "#        ('models',models[key]),\n",
    "#    ])\n",
    "#    #train\n",
    "#    score=cross_validate(full_pipeline, X_train, y_train,scoring='neg_root_mean_squared_error', cv=10)\n",
    "#    #print(score)\n",
    "#    #train_rmse=-score['test_score'].mean()/y.mean()\n",
    "#    #print(train_rmse)\n",
    "#    train_prediction=cross_val_predict(full_pipeline,X_train,y_train,cv=10)\n",
    "#    test_prediction=cross_val_predict(full_pipeline,X_test,y_test,cv=10)\n",
    "#\n",
    "#    for i,out in zip(range(0,len(y.columns),1),y.columns):\n",
    "#        #train scores\n",
    "#        train_rmse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train[:,i],squared=False)#/y[out].mean()\n",
    "#        train_mse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train[:,i])#/y[out].mean()\n",
    "#        train_r2[out]=r2_score(y_true=y_train[:,i],y_pred=train_prediction[:,i])\n",
    "#\n",
    "#        #test scores\n",
    "#        test_rmse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test[:,i],squared=False)#/y[out].mean()\n",
    "#        test_mse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test[:,i])#/y[out].mean()\n",
    "#        test_r2[out]=r2_score(y_true=y_test[:,i],y_pred=test_prediction[:,i])\n",
    "#\n",
    "#    #train result for each output\n",
    "#    Water_Quantity_train_results.append((key,train_rmse['Water_Quantity'],train_mse['Water_Quantity'],train_r2['Water_Quantity']))\n",
    "#    Duration_of_Irrigation_train_results.append((key,train_rmse['Duration_of_Irrigation'],train_mse['Duration_of_Irrigation'],train_r2['Duration_of_Irrigation']))\n",
    "#    Irrigation_Time_Intervals_train_results.append((key,train_rmse['Irrigation_Time_Intervals'],train_mse['Irrigation_Time_Intervals'],train_r2['Irrigation_Time_Intervals']))\n",
    "#    #test result for each output\n",
    "#    Water_Quantity_test_results.append((key,test_rmse['Water_Quantity'],test_mse['Water_Quantity'],test_r2['Water_Quantity']))\n",
    "#    Duration_of_Irrigation_test_results.append((key,test_rmse['Duration_of_Irrigation'],test_mse['Duration_of_Irrigation'],test_r2['Duration_of_Irrigation']))\n",
    "#    Irrigation_Time_Intervals_test_results.append((key,test_rmse['Irrigation_Time_Intervals'],test_mse['Irrigation_Time_Intervals'],test_r2['Irrigation_Time_Intervals']))\n",
    "#   \n",
    "#print(Water_Quantity_train_results,'\\n',Duration_of_Irrigation_train_results,'\\n',Irrigation_Time_Intervals_train_results,'\\n')\n",
    "#print(Water_Quantity_test_results,'\\n',Duration_of_Irrigation_test_results,'\\n',Irrigation_Time_Intervals_test_results,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##show training results table:\n",
    "#print(Water_Quantity_train_results,Duration_of_Irrigation_train_results,Irrigation_Time_Intervals_train_results)\n",
    "#\n",
    "#print('models train Water_Quantity scores:',Water_Quantity_train_results)\n",
    "#best_Water_Quantity_trained_model_idx=np.array(Water_Quantity_train_results)[:,1].argmin()\n",
    "#\n",
    "#print('models train Duration_of_Irrigation scores:',Duration_of_Irrigation_train_results)\n",
    "#best_Duration_of_Irrigation_trained_model_idx=np.array(Duration_of_Irrigation_train_results)[:,1].argmin()\n",
    "#\n",
    "#print('models train Water_interv scores:',Irrigation_Time_Intervals_train_results)\n",
    "#best_Irrigation_Time_Intervals_trained_model_idx=np.array(Irrigation_Time_Intervals_train_results)[:,1].argmin()\n",
    "#\n",
    "##print('models test scores:',test_results)\n",
    "##best_tested_model_idx=np.array(test_results)[:,1].argmax()\n",
    "#\n",
    "#\n",
    "#print('best trained model:',Water_Quantity_train_results[best_Water_Quantity_trained_model_idx][0],Water_Quantity_train_results[best_Water_Quantity_trained_model_idx][1],'\\n',\n",
    "#                            Duration_of_Irrigation_train_results[best_Duration_of_Irrigation_trained_model_idx][0],Duration_of_Irrigation_train_results[best_Duration_of_Irrigation_trained_model_idx][1],'\\n',\n",
    "#                            Irrigation_Time_Intervals_train_results[best_Irrigation_Time_Intervals_trained_model_idx][0],Irrigation_Time_Intervals_train_results[best_Irrigation_Time_Intervals_trained_model_idx][1],'\\n',\n",
    "#)\n",
    "#\n",
    "##print('best tested model:',test_results[best_tested_model_idx][0],test_results[best_tested_model_idx][1])\n",
    "#Water_Quantity_train_result=pd.DataFrame(Water_Quantity_train_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#\n",
    "#Duration_of_Irrigation_train_result=pd.DataFrame(Duration_of_Irrigation_train_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#\n",
    "#Irrigation_Time_Intervals_train_result=pd.DataFrame(Irrigation_Time_Intervals_train_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "##import dataframe_image as dfi\n",
    "#\n",
    "##dfi.export(train_result, \"ModelsTrainingResultsTable.png\")\n",
    "#Water_Quantity_train_result\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duration_of_Irrigation_train_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Irrigation_Time_Intervals_train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_prediction,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##show testing results table:\n",
    "#print(Water_Quantity_test_results,Duration_of_Irrigation_test_results,Irrigation_Time_Intervals_test_results)\n",
    "#\n",
    "#print('models test Water_Quantity scores:',Water_Quantity_test_results)\n",
    "#best_Water_Quantity_test_model_idx=np.array(Water_Quantity_test_results)[:,1].argmin()\n",
    "#\n",
    "#print('models test Duration_of_Irrigation scores:',Duration_of_Irrigation_test_results)\n",
    "#best_Duration_of_Irrigation_test_model_idx=np.array(Duration_of_Irrigation_test_results)[:,1].argmin()\n",
    "#\n",
    "#print('models test Water_interv scores:',Irrigation_Time_Intervals_test_results)\n",
    "#best_Irrigation_Time_Intervals_test_model_idx=np.array(Irrigation_Time_Intervals_test_results)[:,1].argmin()\n",
    "#\n",
    "##print('models test scores:',test_results)\n",
    "##best_tested_model_idx=np.array(test_results)[:,1].argmax()\n",
    "#\n",
    "#\n",
    "#print('best tested model:',Water_Quantity_test_results[best_Water_Quantity_test_model_idx][0],Water_Quantity_test_results[best_Water_Quantity_test_model_idx][1],'\\n',\n",
    "#                            Duration_of_Irrigation_test_results[best_Duration_of_Irrigation_test_model_idx][0],Duration_of_Irrigation_test_results[best_Duration_of_Irrigation_test_model_idx][1],'\\n',\n",
    "#                            Irrigation_Time_Intervals_test_results[best_Irrigation_Time_Intervals_test_model_idx][0],Irrigation_Time_Intervals_test_results[best_Irrigation_Time_Intervals_test_model_idx][1],'\\n',\n",
    "#)\n",
    "#\n",
    "#Water_Quantity_test_result=pd.DataFrame(Water_Quantity_test_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#\n",
    "#Duration_of_Irrigation_test_result=pd.DataFrame(Duration_of_Irrigation_test_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#\n",
    "#Irrigation_Time_Intervals_test_result=pd.DataFrame(Irrigation_Time_Intervals_test_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "##import dataframe_image as dfi\n",
    "#\n",
    "##dfi.export(test_result, \"ModelstestingResultsTable.png\")\n",
    "#Water_Quantity_test_result\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duration_of_Irrigation_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Irrigation_Time_Intervals_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.multioutput import MultiOutputRegressor\n",
    "#\n",
    "#models={'LinearRegression':LinearRegression(),\n",
    "#    #'GradientBoostingRegressor':GradientBoostingRegressor(random_state=42),\n",
    "#    #'AdaBoostRegressor':AdaBoostRegressor(random_state=42),\n",
    "#    #'Lasso':Lasso(random_state=42),\n",
    "#    #'Ridge':Ridge(random_state=42),\n",
    "#    #'ElasticNet':ElasticNet(random_state=42),\n",
    "#    'KNeighborsRegressor':KNeighborsRegressor(),\n",
    "#    'DecisionTreeRegressor':DecisionTreeRegressor(random_state=42),\n",
    "#    'RandomForestRegressor':RandomForestRegressor(random_state=42),\n",
    "#    #'LinearSVR':LinearSVR(max_iter=50000,random_state=42),\n",
    "#    #'SVR':  SVR(max_iter=50000)\n",
    "#    }\n",
    "#\n",
    "#num_attributes=['pH_drain_PC' ,'WC_slab1','Rhout' ,                        \n",
    "#'WC_slab2','RadSum','Pyrgeo','EC_slab1','AbsHumOut','EC_slab2','t_slab1',                       \n",
    "#'t_slab2','Winddir','HumDef','CO2air','Rhair',                         \n",
    "#'Tout','EC_drain_PC','Tair','Windsp','co2_dos','Iglob','PARout','Tot_PAR', 'days','hours' ] \n",
    "#\n",
    "#\n",
    "#cat_attributes=['Rain']\n",
    "#outputs=['Water_Quantity' , 'Duration_of_Irrigation' , 'Irrigation_Time_Intervals']\n",
    "#\n",
    "#num_pipline=make_pipeline(SimpleImputer(strategy='mean'),StandardScaler())\n",
    "#cat_pipeline=make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder())\n",
    "#\n",
    "##date_pipeline=make_pipeline(FunctionTransformer(lambda x:pd.DatetimeIndex(x).day))\n",
    "#preprocessing=ColumnTransformer([('num', num_pipline, num_attributes),\n",
    "#                                ('cat',cat_pipeline,cat_attributes),\n",
    "#                                #('day',date_pipeline,['dteday'])\n",
    "#                                ])\n",
    "#\n",
    "##data_prepared=preprocessing.fit_transform()\n",
    "##data_prepared.shape\n",
    "#\n",
    "#from sklearn.metrics import mean_squared_error,r2_score\n",
    "#from sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate\n",
    "#Water_Quantity_train_results=[] \n",
    "#Irrigation_Time_Intervals_train_results=[]\n",
    "#Duration_of_Irrigation_train_results=[]\n",
    "#Water_Quantity_test_results=[] \n",
    "#Irrigation_Time_Intervals_test_results=[]\n",
    "#Duration_of_Irrigation_test_results=[]\n",
    "#train_rmse=dict()\n",
    "#train_r2=dict()\n",
    "#train_mse=dict()\n",
    "#test_rmse=dict()\n",
    "#test_r2=dict()\n",
    "#test_mse=dict()\n",
    "#confidence_intervals=[]\n",
    "#\n",
    "#for key in models.keys():\n",
    "#    #models['MultiOutputRegressor'] = MultiOutputRegressor(models[key],n_jobs=-1)\n",
    "#\n",
    "#    full_pipeline = Pipeline([\n",
    "#        ('preprocessing', preprocessing),\n",
    "#        ('MultiOutputRegressor',MultiOutputRegressor(models[key],n_jobs=-1)),\n",
    "#    ])\n",
    "#    #train\n",
    "#    score=cross_validate(full_pipeline, X_train, y_train,scoring='neg_root_mean_squared_error', cv=10)\n",
    "#    #print(score)\n",
    "#    #train_rmse=-score['test_score'].mean()/y.mean()\n",
    "#    #print(train_rmse)\n",
    "#    train_prediction=cross_val_predict(full_pipeline,X_train,y_train,cv=10)\n",
    "#    test_prediction=cross_val_predict(full_pipeline,X_test,y_test,cv=10)\n",
    "#\n",
    "#    for i,out in zip(range(0,len(y.columns),1),y.columns):\n",
    "#        #train scores\n",
    "#        train_rmse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train[:,i],squared=False)#/y[out].mean()\n",
    "#        train_mse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train[:,i])#/y[out].mean()\n",
    "#        train_r2[out]=r2_score(y_true=y_train[:,i],y_pred=train_prediction[:,i])\n",
    "#\n",
    "#        #test scores\n",
    "#        test_rmse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test[:,i],squared=False)#/y[out].mean()\n",
    "#        test_mse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test[:,i])#/y[out].mean()\n",
    "#        test_r2[out]=r2_score(y_true=y_test[:,i],y_pred=test_prediction[:,i])\n",
    "#\n",
    "#    #train result for each output\n",
    "#    Water_Quantity_train_results.append((key,train_rmse['Water_Quantity'],train_mse['Water_Quantity'],train_r2['Water_Quantity']))\n",
    "#    Duration_of_Irrigation_train_results.append((key,train_rmse['Duration_of_Irrigation'],train_mse['Duration_of_Irrigation'],train_r2['Duration_of_Irrigation']))\n",
    "#    Irrigation_Time_Intervals_train_results.append((key,train_rmse['Irrigation_Time_Intervals'],train_mse['Irrigation_Time_Intervals'],train_r2['Irrigation_Time_Intervals']))\n",
    "#    #test result for each output\n",
    "#    Water_Quantity_test_results.append((key,test_rmse['Water_Quantity'],test_mse['Water_Quantity'],test_r2['Water_Quantity']))\n",
    "#    Duration_of_Irrigation_test_results.append((key,test_rmse['Duration_of_Irrigation'],test_mse['Duration_of_Irrigation'],test_r2['Duration_of_Irrigation']))\n",
    "#    Irrigation_Time_Intervals_test_results.append((key,test_rmse['Irrigation_Time_Intervals'],test_mse['Irrigation_Time_Intervals'],test_r2['Irrigation_Time_Intervals']))\n",
    "#   \n",
    "#\n",
    "#print(Water_Quantity_train_results,'\\n',Duration_of_Irrigation_train_results,'\\n',Irrigation_Time_Intervals_train_results,'\\n')\n",
    "#print(Water_Quantity_test_results,'\\n',Duration_of_Irrigation_test_results,'\\n',Irrigation_Time_Intervals_test_results,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##show training results table:\n",
    "#print(Water_Quantity_train_results,Duration_of_Irrigation_train_results,Irrigation_Time_Intervals_train_results)\n",
    "#\n",
    "#print('models train Water_Quantity scores:',Water_Quantity_train_results)\n",
    "#best_Water_Quantity_trained_model_idx=np.array(Water_Quantity_train_results)[:,1].argmin()\n",
    "#\n",
    "#print('models train Duration_of_Irrigation scores:',Duration_of_Irrigation_train_results)\n",
    "#best_Duration_of_Irrigation_trained_model_idx=np.array(Duration_of_Irrigation_train_results)[:,1].argmin()\n",
    "#\n",
    "#print('models train Water_interv scores:',Irrigation_Time_Intervals_train_results)\n",
    "#best_Irrigation_Time_Intervals_trained_model_idx=np.array(Irrigation_Time_Intervals_train_results)[:,1].argmin()\n",
    "#\n",
    "##print('models test scores:',test_results)\n",
    "##best_tested_model_idx=np.array(test_results)[:,1].argmax()\n",
    "#\n",
    "#\n",
    "#print('best trained model:',Water_Quantity_train_results[best_Water_Quantity_trained_model_idx][0],Water_Quantity_train_results[best_Water_Quantity_trained_model_idx][1],'\\n',\n",
    "#                            Duration_of_Irrigation_train_results[best_Duration_of_Irrigation_trained_model_idx][0],Duration_of_Irrigation_train_results[best_Duration_of_Irrigation_trained_model_idx][1],'\\n',\n",
    "#                            Irrigation_Time_Intervals_train_results[best_Irrigation_Time_Intervals_trained_model_idx][0],Irrigation_Time_Intervals_train_results[best_Irrigation_Time_Intervals_trained_model_idx][1],'\\n',\n",
    "#)\n",
    "#\n",
    "##print('best tested model:',test_results[best_tested_model_idx][0],test_results[best_tested_model_idx][1])\n",
    "#Water_Quantity_train_result=pd.DataFrame(Water_Quantity_train_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#\n",
    "#Duration_of_Irrigation_train_result=pd.DataFrame(Duration_of_Irrigation_train_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#\n",
    "#Irrigation_Time_Intervals_train_result=pd.DataFrame(Irrigation_Time_Intervals_train_results,\n",
    "#           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "##import dataframe_image as dfi\n",
    "#\n",
    "##dfi.export(train_result, \"ModelsTrainingResultsTable.png\")\n",
    "#Water_Quantity_train_result\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duration_of_Irrigation_train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Irrigation_Time_Intervals_train_result\n",
    "green_house.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chained Multioutput Regression\n",
    "\n",
    "\n",
    "models={#'LinearRegression':LinearRegression(),\n",
    "    'RandomForest':RandomForestRegressor(random_state=42,max_depth=8),\n",
    "    'GradientBoosting':GradientBoostingRegressor(random_state=42,max_depth=8),\n",
    "    'AdaBoost':AdaBoostRegressor(random_state=42,base_estimator=DecisionTreeRegressor(random_state=42,max_depth=8)),\n",
    "    #'Lasso':Lasso(random_state=42),\n",
    "    #'Ridge':Ridge(random_state=42),\n",
    "    #'ElasticNet':ElasticNet(random_state=42),\n",
    "    #'KNeighborsRegressor':KNeighborsRegressor(),\n",
    "    'DecisionTree':DecisionTreeRegressor(random_state=42,max_depth=8),\n",
    "    #'LinearSVR':LinearSVR(max_iter=50000,random_state=42),\n",
    "    #'SVR':  SVR(max_iter=50000)\n",
    "    }\n",
    "\n",
    "#num_attributes=['pH_drain_PC' ,'WC_slab1','Rhout' ,                        \n",
    "#'WC_slab2','RadSum','Pyrgeo','EC_slab1','AbsHumOut','EC_slab2','t_slab1',                       \n",
    "#'t_slab2','Winddir','HumDef','CO2air','Rhair',                         \n",
    "#'Tout','EC_drain_PC','Tair','Windsp','co2_dos','Iglob','PARout','Tot_PAR','days','hours' ] \n",
    "num_attributes=green_house.columns.drop(['Water_Quantity' ,'Duration_of_Irrigation', 'Irrigation_Time_Intervals' ])\n",
    "\n",
    "cat_attributes=[]#['Rain']\n",
    "outputs=['Water_Quantity' ,'Duration_of_Irrigation', 'Irrigation_Time_Intervals']\n",
    "\n",
    "num_pipline=make_pipeline(SimpleImputer(strategy='mean'),StandardScaler())\n",
    "cat_pipeline=make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder())\n",
    "\n",
    "preprocessing=ColumnTransformer([('num', num_pipline, num_attributes),\n",
    "                                ('cat',cat_pipeline,cat_attributes),\n",
    "                                ])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate\n",
    "\n",
    "outputs=y.columns\n",
    "train_results=dict()\n",
    "train_results[outputs[0]]=[] \n",
    "train_results[outputs[1]]=[]\n",
    "train_results[outputs[2]]=[]\n",
    "test_results=dict()\n",
    "test_results[outputs[0]]=[] \n",
    "test_results[outputs[1]]=[]\n",
    "test_results[outputs[2]]=[]\n",
    "conf_interv_results=dict()\n",
    "conf_interv_results[outputs[0]]=[]\n",
    "conf_interv_results[outputs[1]]=[]\n",
    "conf_interv_results[outputs[2]]=[]\n",
    "train_rmse=dict()\n",
    "train_r2=dict()\n",
    "train_mse=dict()\n",
    "test_rmse=dict()\n",
    "test_r2=dict()\n",
    "test_mse=dict()\n",
    "c1=dict()\n",
    "c2=dict()\n",
    "\n",
    "for key in models.keys():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0,shuffle=True,\n",
    "                                                    stratify=green_house['Rain'])\n",
    "\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('models',RegressorChain(models[key],order=[0,1,2],random_state=42)),\n",
    "    ])\n",
    "    #train\n",
    "    score=cross_validate(full_pipeline, X_train, y_train,scoring='neg_root_mean_squared_error', cv=3)\n",
    "    train_prediction=cross_val_predict(full_pipeline,X_train,y_train,cv=3)\n",
    "    #train_prediction=min_max_scaler.inverse_transform(train_prediction)\n",
    "    test_prediction=cross_val_predict(full_pipeline,X_test,y_test,cv=3)\n",
    "    #train_prediction=min_max_scaler.inverse_transform(test_prediction)\n",
    "\n",
    "    for i,out in zip(range(0,len(y.columns),1),y.columns):\n",
    "        #train scores:\n",
    "        train_rmse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train[out],squared=False)/y[out].mean()\n",
    "        train_mse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train[out])/y[out].mean()\n",
    "        train_r2[out]=r2_score(y_true=y_train[out],y_pred=train_prediction[:,i])\n",
    "        #train result for each output\n",
    "        train_results[out].append((key,train_rmse[out],train_mse[out],train_r2[out]))\n",
    "       \n",
    "        #test scores:\n",
    "        test_rmse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test[out],squared=False)/y[out].mean()\n",
    "        test_mse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test[out])/y[out].mean()\n",
    "        test_r2[out]=r2_score(y_true=y_test[out],y_pred=test_prediction[:,i])\n",
    "        #test result for each output\n",
    "        test_results[out].append((key,test_rmse[out],test_mse[out],test_r2[out]))\n",
    "        \n",
    "        from scipy import stats\n",
    "        #confidence_interval:\n",
    "        confidence = 0.95\n",
    "        squared_errors = (test_prediction[:,i] - y_test[out]) ** 2\n",
    "        zscore = stats.norm.ppf((1 + confidence) / 2)\n",
    "        zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(len(squared_errors))\n",
    "        c1[out] = np.sqrt(squared_errors.mean() - zmargin)/y[out].mean()\n",
    "        c2[out]= np.sqrt(squared_errors.mean() + zmargin)/y[out].mean()\n",
    "        #confidence_intervals for each output:\n",
    "        conf_interv_results[out].append((key,test_rmse[out],c1[out],c2[out],c2[out]-c1[out]))\n",
    "        \n",
    "\n",
    "print(train_results[outputs[0]],'\\n',train_results[outputs[1]],'\\n',train_results[outputs[2]],'\\n')\n",
    "print(test_results[outputs[0]],'\\n',test_results[outputs[1]],'\\n',test_results[outputs[2]],'\\n')\n",
    "#print(conf_interv_results[outputs[0]],'\\n',conf_interv_results[outputs[1]],'\\n',conf_interv_results[outputs[2]],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show training results table:\n",
    "best_trained_model_idx=dict()\n",
    "#print('models train Water_Quantity scores:',train_results[outputs[0]])\n",
    "best_trained_model_idx[outputs[0]]=np.array(train_results[outputs[0]])[:,1].argmin()\n",
    "\n",
    "#print('models train Duration_of_Irrigation scores:',train_results[outputs[1]])\n",
    "best_trained_model_idx[outputs[1]]=np.array(train_results[outputs[1]])[:,1].argmin()\n",
    "\n",
    "#print('models train Water_interv scores:',train_results[outputs[2]])\n",
    "best_trained_model_idx[outputs[2]]=np.array(train_results[outputs[2]])[:,1].argmin()\n",
    "\n",
    "#print('models test scores:',test_results)\n",
    "#best_tested_model_idx=np.array(test_results)[:,1].argmax()\n",
    "\n",
    "\n",
    "print('best trained model:',train_results[outputs[0]][best_trained_model_idx[outputs[0]]][0],train_results[outputs[0]][best_trained_model_idx[outputs[0]]][1],'\\n',\n",
    "                            train_results[outputs[1]][best_trained_model_idx[outputs[1]]][0],train_results[outputs[1]][best_trained_model_idx[outputs[1]]][1],'\\n',\n",
    "                            train_results[outputs[2]][best_trained_model_idx[outputs[2]]][0],train_results[outputs[2]][best_trained_model_idx[outputs[2]]][1],'\\n',\n",
    ")\n",
    "train_result=dict()\n",
    "#print('best tested model:',test_results[best_tested_model_idx][0],test_results[best_tested_model_idx][1])\n",
    "train_result[outputs[0]]=pd.DataFrame(train_results[outputs[0]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "\n",
    "train_result[outputs[1]]=pd.DataFrame(train_results[outputs[1]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "\n",
    "train_result[outputs[2]]=pd.DataFrame(train_results[outputs[2]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#import dataframe_image as dfi\n",
    "\n",
    "#dfi.export(train_result, \"ModelsTrainingResultsTable.png\")\n",
    "train_result[outputs[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[outputs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[outputs[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_interv_results[outputs[0]],'\\n',conf_interv_results[outputs[1]],'\\n',conf_interv_results[outputs[2]],'\\n')\n",
    "\n",
    "conf_interv_result=dict()\n",
    "conf_interv_result[outputs[0]]=pd.DataFrame(conf_interv_results[outputs[0]],\n",
    "           columns=['Model','RMSE','C1','C2','Confidence Interval']).sort_values(by='RMSE',ignore_index=True)\n",
    "\n",
    "conf_interv_result[outputs[1]]=pd.DataFrame(conf_interv_results[outputs[1]],\n",
    "           columns=['Model','RMSE','C1','C2','Confidence Interval']).sort_values(by='RMSE',ignore_index=True)\n",
    "\n",
    "conf_interv_result[outputs[2]]=pd.DataFrame(conf_interv_results[outputs[2]],\n",
    "           columns=['Model','RMSE','C1','C2','Confidence Interval']).sort_values(by='RMSE',ignore_index=True)\n",
    "\n",
    "conf_interv_result[outputs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interv_result[outputs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interv_result[outputs[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xticks(range(len(conf_interv_result[outputs[0]])),list(conf_interv_result[outputs[0]]['Model']),\n",
    "            rotation = 90, ha=\"right\")\n",
    "plt.title('RMSE Confidence Intevals of Water_Quantity Output\\n for Classical ML Models',fontsize=12)\n",
    "for C1,C2,b in zip(conf_interv_result[outputs[0]]['C1'],conf_interv_result[outputs[0]]['C2'],\n",
    "                    range(len(conf_interv_result[outputs[0]]))):\n",
    "    color='#2187bb'\n",
    "    horizontal_line_width=0.25\n",
    "    left = b - horizontal_line_width / 4\n",
    "    right = b + horizontal_line_width / 4\n",
    "    #plt.plot((b,b),(C1,C2),'ro-',color='orange')\n",
    "    plt.plot([b, b], [C2, C1], color=color)\n",
    "    plt.plot([left, right], [C2, C2], color=color)\n",
    "    plt.plot([left, right], [C1, C1], color=color)\n",
    "    plt.plot(b, conf_interv_result[outputs[0]]['RMSE'][b], 'o', color='#f44336')\n",
    "save_fig('RMSE_Confidence_Intevals_of_WQ_for_Classical_ML_Models')\n",
    "\n",
    "#plt.xticks(range(len(confidence_interval_results)),list(confidence_interval_results['Model']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xticks(range(len(conf_interv_result[outputs[1]])),list(conf_interv_result[outputs[1]]['Model']),\n",
    "            rotation = 90, ha=\"right\")\n",
    "plt.title('RMSE Confidence Intevals of Duration of Irrigation\\n Output for Classical ML Models',fontsize=12)\n",
    "for C1,C2,b in zip(conf_interv_result[outputs[1]]['C1'],conf_interv_result[outputs[1]]['C2'],\n",
    "                    range(len(conf_interv_result[outputs[1]]))):\n",
    "    color='#2187bb'\n",
    "    horizontal_line_width=0.25\n",
    "    left = b - horizontal_line_width / 4\n",
    "    right = b + horizontal_line_width / 4\n",
    "    #plt.plot((b,b),(C1,C2),'ro-',color='orange')\n",
    "    plt.plot([b, b], [C2, C1], color=color)\n",
    "    plt.plot([left, right], [C2, C2], color=color)\n",
    "    plt.plot([left, right], [C1, C1], color=color)\n",
    "    plt.plot(b, conf_interv_result[outputs[1]]['RMSE'][b], 'o', color='#f44336')\n",
    "save_fig('RMSE_Confidence_Intevals_of_DI_for_Classical_ML_Models')\n",
    "\n",
    "#plt.xticks(range(len(confidence_interval_results)),list(confidence_interval_results['Model']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xticks(range(len(conf_interv_result[outputs[2]])),list(conf_interv_result[outputs[2]]['Model']),\n",
    "            rotation = 90, ha=\"right\")\n",
    "plt.title('RMSE Confidence Intevals of Irrigation time Intevals\\n Output for Classical ML Models',fontsize=12)\n",
    "for C1,C2,b in zip(conf_interv_result[outputs[2]]['C1'],conf_interv_result[outputs[2]]['C2'],\n",
    "                    range(len(conf_interv_result[outputs[2]]))):\n",
    "    color='#2187bb'\n",
    "    horizontal_line_width=0.25\n",
    "    left = b - horizontal_line_width / 4\n",
    "    right = b + horizontal_line_width / 4\n",
    "    #plt.plot((b,b),(C1,C2),'ro-',color='orange')\n",
    "    plt.plot([b, b], [C2, C1], color=color)\n",
    "    plt.plot([left, right], [C2, C2], color=color)\n",
    "    plt.plot([left, right], [C1, C1], color=color)\n",
    "    plt.plot(b, conf_interv_result[outputs[2]]['RMSE'][b], 'o', color='#f44336')\n",
    "save_fig('RMSE_Confidence_Intevals_of_ITI_for_Classical_ML_Models')\n",
    "\n",
    "#plt.xticks(range(len(confidence_interval_results)),list(confidence_interval_results['Model']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.multioutput import RegressorChain\n",
    "\n",
    "\n",
    "\n",
    "outputs=y.columns\n",
    "train_results=dict()\n",
    "train_results[outputs[0]]=[] \n",
    "train_results[outputs[1]]=[]\n",
    "train_results[outputs[2]]=[]\n",
    "test_results=dict()\n",
    "test_results[outputs[0]]=[] \n",
    "test_results[outputs[1]]=[]\n",
    "test_results[outputs[2]]=[]\n",
    "conf_interv_results=dict()\n",
    "conf_interv_results[outputs[0]]=[]\n",
    "conf_interv_results[outputs[1]]=[]\n",
    "conf_interv_results[outputs[2]]=[]\n",
    "train_rmse=dict()\n",
    "train_r2=dict()\n",
    "train_mse=dict()\n",
    "test_rmse=dict()\n",
    "test_r2=dict()\n",
    "test_mse=dict()\n",
    "c1=dict()\n",
    "c2=dict()\n",
    "\n",
    "\n",
    "\n",
    "estimator=RandomForestRegressor(n_estimators= 100,criterion='squared_error',\n",
    "    max_depth= None,min_samples_split= 2,min_samples_leaf= 1,min_weight_fraction_leaf= 0,max_features='sqrt',\n",
    "    max_leaf_nodes= None,min_impurity_decrease= 0,bootstrap= True,oob_score=False,n_jobs= -1,\n",
    "    ccp_alpha=0,max_samples= None,random_state=42)\n",
    "\n",
    "best_models=dict()\n",
    "\n",
    "best_models['Chain_RF']=RegressorChain(base_estimator=estimator,order=[0,1,2],random_state=42)\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('Chain_RF',best_models['Chain_RF'] )\n",
    "                        ])\n",
    "       \n",
    "param_grid=[{ \n",
    "                'Chain_RF__base_estimator__max_depth':[8],\n",
    "                'Chain_RF__base_estimator__n_estimators':[300],\n",
    "                'Chain_RF__base_estimator__criterion':['squared_error'],\n",
    "                'Chain_RF__base_estimator__min_samples_split':[2],\n",
    "                'Chain_RF__base_estimator__max_samples':[None],\n",
    "                'Chain_RF__base_estimator__max_features':['sqrt'],\n",
    "                'Chain_RF__base_estimator__ccp_alpha':[0],              \n",
    "            }]\n",
    "grid_search=GridSearchCV(full_pipeline,param_grid,cv=10,scoring='neg_root_mean_squared_error',verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction=cross_val_predict(grid_search.best_estimator_,X_train,y_train,cv=10)\n",
    "test_prediction=cross_val_predict(grid_search.best_estimator_,X_test,y_test,cv=10)\n",
    "\n",
    "#rmse_percentage=rmse.mean()/y.mean()\n",
    "#print('rmse percentage:',rmse_percentage)\n",
    "#print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i,out in zip(range(0,len(y.columns),1),y.columns):\n",
    "    #train scores:\n",
    "    train_rmse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train[out],squared=False)/y[out].mean()\n",
    "    train_mse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train[out])/y[out].mean()\n",
    "    train_r2[out]=r2_score(y_true=y_train[out],y_pred=train_prediction[:,i])\n",
    "    #train result for each output\n",
    "    train_results[out].append(('Chain_RF',train_rmse[out],train_mse[out],train_r2[out]))\n",
    "   \n",
    "    #test scores:\n",
    "    test_rmse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test[out],squared=False)/y[out].mean()\n",
    "    test_mse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test[out])/y[out].mean()\n",
    "    test_r2[out]=r2_score(y_true=y_test[out],y_pred=test_prediction[:,i])\n",
    "    #test result for each output\n",
    "    test_results[out].append(('Chain_RF',test_rmse[out],test_mse[out],test_r2[out]))\n",
    "    \n",
    "    # Plot predicted vs actual\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.scatter(y_test[out], test_prediction[:,i],color='steelblue')\n",
    "    plt.xlabel(f'Actual {out}')\n",
    "    plt.ylabel(f'Predicted {out}')\n",
    "    plt.title(f'predicted vs actual {out} \\nplot for Chain RF',fontsize=12)\n",
    "    # overlay the regression line\n",
    "    z = np.polyfit(y_test[out], test_prediction[:,i], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(y_test[out],p(y_test[out]),color='red')\n",
    "    #plt.grid()\n",
    "    save_fig(f'predicted vs actual {out} plot for Chain RF')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    from scipy import stats\n",
    "    #confidence_interval:\n",
    "    confidence = 0.95\n",
    "    squared_errors = (test_prediction[:,i] - y_test[out]) ** 2\n",
    "    zscore = stats.norm.ppf((1 + confidence) / 2)\n",
    "    zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(len(squared_errors))\n",
    "    c1[out] = np.sqrt(squared_errors.mean() - zmargin)/y[out].mean()\n",
    "    c2[out] = np.sqrt(squared_errors.mean() + zmargin)/y[out].mean()\n",
    "    #confidence_intervals for each output:\n",
    "    conf_interv_results[out].append(('Chain_RF',test_rmse[out],c1[out],c2[out],c2[out]-c1[out]))\n",
    "    \n",
    "print(train_results[outputs[0]],'\\n',train_results[outputs[1]],'\\n',train_results[outputs[2]],'\\n')\n",
    "print(test_results[outputs[0]],'\\n',test_results[outputs[1]],'\\n',test_results[outputs[2]],'\\n')\n",
    "print(conf_interv_results[outputs[0]],'\\n',conf_interv_results[outputs[1]],'\\n',conf_interv_results[outputs[2]],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[out].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_out,i in zip(outputs,range(len(outputs))):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(test_prediction[:,i][15:50],label='predected', marker=\".\")\n",
    "    plt.plot(y_test[pred_out].values[15:50],label='Actual',marker=\".\")\n",
    "\n",
    "    plt.title(f\"Predicted Vs. Actual of {pred_out} \\nsamples of Chain RF\",fontsize=12)\n",
    "    plt.xlabel(\"samples\")\n",
    "    plt.ylabel(f\"{pred_out}\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    save_fig(f\"Predicted Vs. Actual of {pred_out} in discrete time series points of Chain RF\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search_test_rmse=dict()\n",
    "#for i,out in zip(range(0,len(y.columns),1),y.columns):\n",
    "#\n",
    "#    grid_search_test_rmse[out]=mean_squared_error(y_pred=pred[:,i],y_true=y_train[out],squared=False)/y[out].mean()\n",
    "#print(grid_search_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('best score=',-grid_search.best_score_/y.mean())\n",
    "#rmse=-cross_val_score(full_pipeline,X_train,y_train,scoring='neg_root_mean_squared_error',cv=10)\n",
    "#rmse.mean()\n",
    "#rmse_percentage=rmse.mean()/y.mean()\n",
    "#print('rmse percentage:',rmse_percentage)\n",
    "#print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=grid_search.best_params_\n",
    "#best_params['RandomForestRegressor__base_estimator']=grid_search.best_estimator_['RandomForestRegressor__base_estimator'].n_estimators_\n",
    "GBR_best_param=pd.DataFrame.from_dict(grid_search.best_params_,orient='index',columns=['best value'])\n",
    "GBR_best_param['best value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances=dict()\n",
    "features_imoprtance=dict()\n",
    "for out,estimator in zip(outputs,grid_search.best_estimator_['Chain_RF'].estimators_):\n",
    "    feature_importances[out]=estimator.feature_importances_.round(2)\n",
    "    features_imoprtance[out]=sorted(zip(feature_importances[out],\n",
    "           grid_search.best_estimator_[\"preprocessing\"].get_feature_names_out()),reverse=True)\n",
    "\n",
    "    features_imoprtance[out]=pd.DataFrame(features_imoprtance[out],\n",
    "               columns=['importance','features'])\n",
    "\n",
    "    #import dataframe_image as dfi\n",
    "    #\n",
    "    #dfi.export(features_imoprtance, \"FeatureImportanceTable.png\")\n",
    "#features_imoprtance[outputs[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_imoprtance[outputs[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_imoprtance[outputs[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,out in zip(range(0,len(outputs),1),outputs):\n",
    "    #features_imoprtance[out].set_index('features')\n",
    "    features_imoprtance[out].index=features_imoprtance[out]['features']\n",
    "    #plt.subplot(1,3,i+1)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    ax = fig.gca()\n",
    "    (features_imoprtance[out]['importance']).plot.bar(ax=ax,color='steelblue')\n",
    "\n",
    "    #(features_imoprtance[out]['importance']/features_imoprtance[out]['importance'].max()*100).plot.bar(ax=ax)\n",
    "    ax.set_title(f'{out} Features Importance',fontsize=12)\n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_ylabel(\"Features Importance\")\n",
    "    save_fig(f'Features Importance of {out} Output')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using ANN for a multioutput regresion task:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras import layers\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "y=pd.DataFrame(green_house.loc[:,['Water_Quantity','Duration_of_Irrigation','Irrigation_Time_Intervals']])\n",
    "\n",
    "X=green_house.drop(['Duration_of_Irrigation','Water_Quantity','Irrigation_Time_Intervals'],axis=1).copy()\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0,shuffle=True,\n",
    "                                                    stratify=green_house['Rain'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,test_size=0.10, random_state=42)\n",
    "\n",
    "X_train=preprocessing.fit_transform(X_train)\n",
    "X_test=preprocessing.transform(X_test)\n",
    "X_valid=preprocessing.transform(X_valid)\n",
    "\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('X_valid',X_valid.shape)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler=MinMaxScaler()\n",
    "\n",
    "y_train=min_max_scaler.fit_transform(y_train)\n",
    "y_test=min_max_scaler.transform(y_test)\n",
    "y_valid=min_max_scaler.transform(y_valid)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)\n",
    "print('y_valid',y_valid.shape)\n",
    "\n",
    "\n",
    "y_train_pred=[]\n",
    "print(X_train.shape[1:][0])\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "leaky_relu = keras.layers.LeakyReLU(alpha=0.3)  # defaults to alpha=0.3\n",
    "best_models['Parallel_Out_ANN'] = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(500, activation='swish',kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(150, activation='swish',kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(500, activation='swish',kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(150, activation='swish',kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(500, activation='swish',kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(150, activation='swish',kernel_initializer=\"he_normal\"),\n",
    "\n",
    "    tf.keras.layers.Dense(3,activation='linear',kernel_initializer=\"he_normal\")\n",
    "])\n",
    "checkpoint_path = \"training_1/par_cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                   save_weights_only=True)\n",
    "\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=30,\n",
    "                                                     restore_best_weights=True)\n",
    "best_models['Parallel_Out_ANN'].compile(loss=\"huber\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = best_models['Parallel_Out_ANN'].fit(X_train, y_train, epochs=500,\n",
    "                    validation_data=(X_valid, y_valid),batch_size=64,callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "#mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
    "#X_new = X_test[:3]\n",
    "# plot loss during training\n",
    "plt.title('Train Vs.Valid Loss of Parallel_Out_ANN',fontsize=12)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='valid')\n",
    "plt.legend()\n",
    "save_fig('Train Vs.Valid Loss of Parallel_Out_ANN')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RMSE = best_models['Parallel_Out_ANN'].evaluate(X_train, y_train,return_dict=True, verbose=1)\n",
    "val_RMSE = best_models['Parallel_Out_ANN'].evaluate(X_valid, y_valid,return_dict=True, verbose=1)\n",
    "\n",
    "test_RMSE = best_models['Parallel_Out_ANN'].evaluate(X_test, y_test,return_dict=True, verbose=1)\n",
    "print(f'Train: {train_RMSE},\\n val: {val_RMSE},\\nTest: {test_RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = best_models['Parallel_Out_ANN'].predict((X_train))\n",
    "train_prediction.reshape(len(y_train),3)\n",
    "\n",
    "test_prediction = best_models['Parallel_Out_ANN'].predict((X_test))\n",
    "test_prediction.reshape(len(y_test),3)\n",
    "\n",
    "test_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction=min_max_scaler.inverse_transform(train_prediction)\n",
    "test_prediction=min_max_scaler.inverse_transform(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_inv=min_max_scaler.inverse_transform(y_train)\n",
    "y_test_inv=min_max_scaler.inverse_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,out in zip(range(0,len(y.columns),1),y.columns):\n",
    "    #train scores:\n",
    "    train_rmse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train_inv[:,i],squared=False)/y[out].mean()\n",
    "    train_mse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train_inv[:,i])/y[out].mean()\n",
    "    train_r2[out]=r2_score(y_true=y_train_inv[:,i],y_pred=train_prediction[:,i])\n",
    "    #train result for each output\n",
    "    train_results[out].append(('Parallel_Out_ANN',train_rmse[out],train_mse[out],train_r2[out]))\n",
    "   \n",
    "    #test scores:\n",
    "    test_rmse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test_inv[:,i],squared=False)/y[out].mean()\n",
    "    test_mse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test_inv[:,i])/y[out].mean()\n",
    "    test_r2[out]=r2_score(y_true=y_test_inv[:,i],y_pred=test_prediction[:,i])\n",
    "    #test result for each output\n",
    "    test_results[out].append(('Parallel_Out_ANN',test_rmse[out],test_mse[out],test_r2[out]))\n",
    "    \n",
    "    # Plot predicted vs actual\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.scatter(y_test_inv[:,i], test_prediction[:,i],color='steelblue')\n",
    "    plt.xlabel(f'Actual {out}')\n",
    "    plt.ylabel(f'Predicted {out}')\n",
    "    plt.title(f'predicted vs actual {out} plot \\nfor Parallel_Out_ANN',fontsize=12)\n",
    "    # overlay the regression line\n",
    "    z = np.polyfit(y_test_inv[:,i], test_prediction[:,i], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(y_test_inv[:,i],p(y_test_inv[:,i]),color='red')\n",
    "    #plt.grid()\n",
    "    save_fig(f'predicted vs actual {out} plot for Parallel_Out_ANN')\n",
    "    plt.show()\n",
    "\n",
    "    from scipy import stats\n",
    "    #confidence_interval:\n",
    "    confidence = 0.95\n",
    "    squared_errors = (test_prediction[:,i] - y_test_inv[:,i]) ** 2\n",
    "    zscore = stats.norm.ppf((1 + confidence) / 2)\n",
    "    zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(len(squared_errors))\n",
    "    c1[out],c2[out] = np.sqrt(squared_errors.mean() - zmargin)/y[out].mean(), np.sqrt(squared_errors.mean() \n",
    "                    + zmargin)/y[out].mean()\n",
    "    #confidence_intervals for each output:\n",
    "    conf_interv_results[out].append(('Parallel_Out_ANN',test_rmse[out],c1[out],c2[out],c2[out]-c1[out]))\n",
    "        \n",
    "\n",
    "print(train_results[outputs[0]],'\\n',train_results[outputs[1]],'\\n',train_results[outputs[2]],'\\n')\n",
    "print(test_results[outputs[0]],'\\n',test_results[outputs[1]],'\\n',test_results[outputs[2]],'\\n')\n",
    "print(conf_interv_results[outputs[0]],'\\n',conf_interv_results[outputs[1]],'\\n',conf_interv_results[outputs[2]],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_out,i in zip(outputs,range(len(outputs))):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    predictions=test_prediction[:,i]\n",
    "    plt.figure()\n",
    "    plt.plot(predictions[15:50],label='predected', marker=\".\")\n",
    "    plt.plot(y_test_inv[:,i][15:50],label='Actual',marker=\".\")\n",
    "\n",
    "    plt.title(f\"Predicted Vs. Actual of {pred_out} \\nsamples of Parallel_Out_ANN\",fontsize=12)\n",
    "    plt.xlabel(\"samples\")\n",
    "    plt.ylabel(f\"{pred_out}\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    save_fig(f\"Predicted Vs. Actual of {pred_out} in discrete time series points of Parallel_Out_ANN\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred=best_models['Parallel_Out_ANN'].predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=model.load_weights(\"my_checkpoints\")\n",
    "model_rms_train=mean_squared_error(y_pred=y_train_pred[:,0],y_true=y_train[:,0],squared=False)#/y_train[outputs[0]].mean()\n",
    "model_rms_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_train=mean_squared_error(y_pred=y_train_pred[:,1],y_true=y_train[:,1],squared=False)#/y[outputs[1]].mean()\n",
    "model_rms_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_train=mean_squared_error(y_pred=y_train_pred[:,2],y_true=y_train[:,2],squared=False)#/y[outputs[2]].mean()\n",
    "model_rms_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = best_models['Parallel_Out_ANN'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_test=mean_squared_error(y_pred=y_pred_test[:,0],y_true=y_test[:,0],squared=False)#/y_train[outputs[0]].mean()\n",
    "model_rms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_test=mean_squared_error(y_pred=y_pred_test[:,1],y_true=y_test[:,1],squared=False)#/y_train[outputs[1]].mean()\n",
    "model_rms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_train=mean_squared_error(y_pred=y_pred_test[:,2],y_true=y_test[:,2],squared=False)#/y_train[outputs[1]].mean()\n",
    "model_rms_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "#tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0,shuffle=True,\n",
    "                                                    stratify=green_house['Rain'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,test_size=0.10, random_state=42)\n",
    "\n",
    "X_train=preprocessing.fit_transform(X_train)\n",
    "X_test=preprocessing.transform(X_test)\n",
    "X_valid=preprocessing.transform(X_valid)\n",
    "\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('X_valid',X_valid.shape)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler=MinMaxScaler()\n",
    "\n",
    "y_train=min_max_scaler.fit_transform(y_train)\n",
    "y_test=min_max_scaler.transform(y_test)\n",
    "y_valid=min_max_scaler.transform(y_valid)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)\n",
    "print('y_valid',y_valid.shape)\n",
    "\n",
    "\n",
    "y_train_pred=[]\n",
    "print(X_train.shape[1:][0])\n",
    "\n",
    "input_wide = layers.Input(shape=X_train.shape[1:])  # features 0 to 4\n",
    "#input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
    "norm_layer_wide = layers.Normalization()\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "#norm_deep = norm_layer_deep(input_deep)\n",
    "\n",
    "#hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = layers.Dense(500, activation='swish',kernel_initializer=\"he_normal\")(norm_wide)\n",
    "hidden3 = layers.Dense(150, activation='swish',kernel_initializer=\"he_normal\")(hidden2)\n",
    "\n",
    "hidden4 = layers.Dense(500, activation='swish',kernel_initializer=\"he_normal\")(norm_wide)\n",
    "hidden5 = layers.Dense(150, activation='swish',kernel_initializer=\"he_normal\")(hidden4)\n",
    "\n",
    "hidden6 = layers.Dense(500, activation='swish',kernel_initializer=\"he_normal\")(norm_wide)\n",
    "hidden7 = layers.Dense(150, activation='swish',kernel_initializer=\"he_normal\")(hidden6)\n",
    "\n",
    "output_0 = layers.Dense(1,activation='linear')(hidden3)\n",
    "norm_out_0=layers.BatchNormalization()(output_0)\n",
    "concat_0 = tf.keras.layers.concatenate([hidden5, norm_out_0])\n",
    "output_1 = tf.keras.layers.Dense(1,activation='linear')(concat_0)\n",
    "norm_out_1=layers.BatchNormalization()(output_1)\n",
    "\n",
    "concat_1 = tf.keras.layers.concatenate([hidden7, norm_out_0,norm_out_1])\n",
    "output_2 = tf.keras.layers.Dense(1,activation='linear')(concat_1)\n",
    "\n",
    "\n",
    "best_models['Cascaded_Out_ANN'] = tf.keras.Model(inputs=[input_wide],\n",
    "                       outputs=[output_0, output_1,output_2])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "checkpoint_path = \"training_1/cas_cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                   save_best_model=True)\n",
    "\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=30,\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "\n",
    "best_models['Cascaded_Out_ANN'].compile(loss=(\"huber\", \"huber\",\"huber\"), loss_weights=(1,1, 1), optimizer=optimizer,\n",
    "              metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "\n",
    "norm_layer_wide.adapt(X_train)\n",
    "history = best_models['Cascaded_Out_ANN'].fit(\n",
    "    (X_train), (y_train[:,0], y_train[:,1],y_train[:,2]), epochs=300,\n",
    "    validation_data=((X_valid), (y_valid[:,0], y_valid[:,1],y_valid[:,2])),\n",
    "    batch_size=128,callbacks=[checkpoint_cb, early_stopping_cb]\n",
    ")\n",
    "\n",
    "plt.title('Train Vs.Valid Loss of Cascaded_Out_ANN ',fontsize=12)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='valid')\n",
    "save_fig('Train Vs.Valid Loss of Cascaded_Out_ANN ')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(checkpoint_dir)\n",
    "##best_models['Cascaded_Out_ANN']=best_models['Cascaded_Out_ANN'].load_weights(checkpoint_path)\n",
    "#best_models['Cascaded_Out_ANN']=keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RMSE = best_models['Cascaded_Out_ANN'].evaluate((X_train), (y_train[:,0],y_train[:,1],y_train[:,2]),return_dict=True, verbose=1)\n",
    "val_RMSE = best_models['Cascaded_Out_ANN'].evaluate((X_valid), (y_valid[:,0],y_valid[:,1],y_valid[:,2]),return_dict=True, verbose=1)\n",
    "\n",
    "test_RMSE = best_models['Cascaded_Out_ANN'].evaluate((X_test), (y_test[:,0],y_test[:,1],y_test[:,2]),return_dict=True, verbose=1)\n",
    "print(f'Train: {train_RMSE},\\n val: {val_RMSE},\\n Test: {test_RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out1,train_out2,train_out3 = best_models['Cascaded_Out_ANN'].predict((X_train))\n",
    "train_prediction=np.c_[train_out1,train_out2,train_out3]\n",
    "train_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out1,test_out2,test_out3 = best_models['Cascaded_Out_ANN'].predict((X_test))\n",
    "test_prediction=np.c_[test_out1,test_out2,test_out3]\n",
    "test_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_out1,train_out2,train_out3 = best_models['Cascaded_Out_ANN'].predict((X_train))\n",
    "##train_prediction=np.asarray(train_prediction)[:,:,0].reshape(len(y_train),3)\n",
    "#train_prediction=pd.concat([train_out1,train_out2,train_out3],axis=1)\n",
    "##train_prediction[0]=train_prediction[0].reshape(1,-1)\n",
    "##train_prediction[1]=train_prediction[1].reshape(1,-1)\n",
    "##train_prediction[2]=train_prediction[2].reshape(1,-1)\n",
    "##train_prediction=np.asarray(train_prediction)[:,:,0]\n",
    "#test_prediction = best_models['Cascaded_Out_ANN'].predict((X_test))\n",
    "#test_prediction=np.asarray(test_prediction)[:,:,0].reshape(len(y_test),3)\n",
    "#\n",
    "##test_prediction[0]=test_prediction[0].reshape(1,-1)\n",
    "##test_prediction[1]=test_prediction[1].reshape(1,-1)\n",
    "##test_prediction[2]=test_prediction[2].reshape(1,-1)\n",
    "##test_prediction=np.asarray(test_prediction)[:,:,0]\n",
    "test_prediction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction=np.array(test_prediction)[:,:,0]\n",
    "#test_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction=test_prediction.reshape(-1,3)\n",
    "#test_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_prediction=np.array(train_prediction)[:,:,0].reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction=np.c_[test_prediction[0],test_prediction[1],test_prediction[2]]\n",
    "#train_prediction=np.c_[train_prediction[0],train_prediction[1],train_prediction[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction=min_max_scaler.inverse_transform(train_prediction)\n",
    "test_prediction=min_max_scaler.inverse_transform(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_inv=min_max_scaler.inverse_transform(y_train)\n",
    "y_test_inv=min_max_scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models['Cascaded_Out_ANN']=keras.models.load_model(\"my_checkpoints_best\")\n",
    "#train_prediction = best_models['Cascaded_Out_ANN'].predict((X_train))\n",
    "#test_prediction = best_models['Cascaded_Out_ANN'].predict((X_test))\n",
    "#train_prediction = np.array(train_prediction).reshape(-1,3)\n",
    "#test_prediction=np.array(test_prediction).reshape(-1,3)\n",
    "for i,out in zip(range(0,len(y.columns),1),y.columns):\n",
    "    #train scores:\n",
    "    train_rmse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train_inv[:,i],squared=False)/y[out].mean()\n",
    "    train_mse[out]=mean_squared_error(y_pred=train_prediction[:,i],y_true=y_train_inv[:,i])/y[out].mean()\n",
    "    train_r2[out]=r2_score(y_true=y_train_inv[:,i],y_pred=train_prediction[:,i])\n",
    "    #train result for each output\n",
    "    train_results[out].append(('Cascaded_Out_ANN',train_rmse[out],train_mse[out],train_r2[out]))\n",
    "   \n",
    "    #test scores:\n",
    "    test_rmse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test_inv[:,i],squared=False)/y[out].mean()\n",
    "    test_mse[out]=mean_squared_error(y_pred=test_prediction[:,i],y_true=y_test_inv[:,i])/y[out].mean()\n",
    "    test_r2[out]=r2_score(y_true=y_test_inv[:,i],y_pred=test_prediction[:,i])\n",
    "    #test result for each output\n",
    "    test_results[out].append(('Cascaded_Out_ANN',test_rmse[out],test_mse[out],test_r2[out]))\n",
    "    \n",
    "    # Plot predicted vs actual\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.scatter(y_test_inv[:,i], test_prediction[:,i],color='steelblue')\n",
    "    plt.xlabel(f'Actual {out}')\n",
    "    plt.ylabel(f'Predicted {out}')\n",
    "    plt.title(f'predicted vs actual {out} plot\\n for Cascaded_Out_ANN',fontsize=12)\n",
    "    # overlay the regression line\n",
    "    z = np.polyfit(y_test_inv[:,i], test_prediction[:,i], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(y_test_inv[:,i],p(y_test_inv[:,i]),color='red')\n",
    "    #plt.grid()\n",
    "    save_fig(f'predicted vs actual {out} plot for Cascaded_Out_ANN')\n",
    "    plt.show()\n",
    "\n",
    "    from scipy import stats\n",
    "    #confidence_interval:\n",
    "    confidence = 0.95\n",
    "    squared_errors = (test_prediction[:,i] - y_test_inv[:,i]) ** 2\n",
    "    zscore = stats.norm.ppf((1 + confidence) / 2)\n",
    "    zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(len(squared_errors))\n",
    "    c1[out],c2[out] = np.sqrt(squared_errors.mean() - zmargin)/y[out].mean(), np.sqrt(squared_errors.mean() \n",
    "                    + zmargin)/y[out].mean()\n",
    "    #confidence_intervals for each output:\n",
    "    conf_interv_results[out].append(('Cascaded_Out_ANN',test_rmse[out],c1[out],c2[out],c2[out]-c1[out]))\n",
    "        \n",
    "\n",
    "#print(train_results[outputs[0]],'\\n',train_results[outputs[1]],'\\n',train_results[outputs[2]],'\\n')\n",
    "print(test_results[outputs[0]],'\\n',test_results[outputs[1]],'\\n',test_results[outputs[2]],'\\n')\n",
    "#print(conf_interv_results[outputs[0]],'\\n',conf_interv_results[outputs[1]],'\\n',conf_interv_results[outputs[2]],'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(test_prediction)[0,:].reshape(1,-1)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = best_models['Cascaded_Out_ANN'].evaluate((X_test), (y_test[:,0], y_test[:,1], y_test[:,2]))\n",
    "#weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0, y_pred_1,y_pred_2 = best_models['Cascaded_Out_ANN'].predict((X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_train=mean_squared_error(y_pred=y_pred_0,y_true=y_train[:,0],squared=False)#/y_train[outputs[0]].mean()\n",
    "model_rms_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_train=mean_squared_error(y_pred=y_pred_1,y_true=y_train[:,1],squared=False)#/y_train[outputs[1]].mean()\n",
    "model_rms_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_train=mean_squared_error(y_pred=y_pred_2,y_true=y_train[:,2],squared=False)#/y_train[outputs[1]].mean()\n",
    "model_rms_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=model.load_weights(\"my_checkpoints\")\n",
    "#model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0, y_pred_1,y_pred_2 = best_models['Cascaded_Out_ANN'].predict((X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_test=mean_squared_error(y_pred=y_pred_0,y_true=y_test[:,0],squared=False)#/y_train[outputs[0]].mean()\n",
    "model_rms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_test=mean_squared_error(y_pred=y_pred_1,y_true=y_test[:,1],squared=False)#/y_train[outputs[1]].mean()\n",
    "model_rms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms_test=mean_squared_error(y_pred=y_pred_2,y_true=y_test[:,2],squared=False)#/y_train[outputs[1]].mean()\n",
    "model_rms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show training results table:\n",
    "best_trained_model_idx=dict()\n",
    "for out in outputs:\n",
    "    print('best models train {out} scores:',train_results[outputs[out]])\n",
    "    best_trained_model_idx[outputs[out]]=np.array(train_results[outputs[out]])[:,1].argmin()\n",
    "\n",
    "\n",
    "    #print('best models test scores:',test_results)\n",
    "    #best_tested_model_idx=np.array(test_results)[:,1].argmax()\n",
    "\n",
    "print('best trained model:',train_results[outputs[out]][best_trained_model_idx[outputs[out]]][0],\n",
    "                            train_results[outputs[out]][best_trained_model_idx[outputs[out]]][1],'\\n')\n",
    "train_result=dict()\n",
    "#print('best tested model:',test_results[best_tested_model_idx][0],test_results[best_tested_model_idx][1])\n",
    "train_result[outputs[0]]=pd.DataFrame(train_results[outputs[0]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "\n",
    "train_result[outputs[1]]=pd.DataFrame(train_results[outputs[1]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "\n",
    "train_result[outputs[2]]=pd.DataFrame(train_results[outputs[2]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#import dataframe_image as dfi\n",
    "#\n",
    "#dfi.export(train_result[outputs[0]], f'ModelsTrainingResults{outputs[0]}Table.png')\n",
    "#dfi.export(train_result[outputs[1]], f\"ModelsTrainingResults{outputs[1]}Table.png\")\n",
    "#dfi.export(train_result[outputs[2]], f\"ModelsTrainingResults{outputs[2]}Table.png\")\n",
    "\n",
    "train_result[outputs[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[outputs[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[outputs[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show testing results table:\n",
    "best_tested_model_idx=dict()\n",
    "print('best models test Water_Quantity scores:',test_results[outputs[0]])\n",
    "best_tested_model_idx[outputs[0]]=np.array(test_results[outputs[0]])[:,1].argmin()\n",
    "\n",
    "print('best models test Duration_of_Irrigation scores:',test_results[outputs[1]])\n",
    "best_tested_model_idx[outputs[1]]=np.array(test_results[outputs[1]])[:,1].argmin()\n",
    "\n",
    "print('best models test Water_interv scores:',test_results[outputs[2]])\n",
    "best_tested_model_idx[outputs[2]]=np.array(test_results[outputs[2]])[:,1].argmin()\n",
    "\n",
    "#print('best models test scores:',test_results)\n",
    "#best_tested_model_idx=np.array(test_results)[:,1].argmax()\n",
    "\n",
    "\n",
    "print('best tested model:',test_results[outputs[0]][best_tested_model_idx[outputs[0]]][0],test_results[outputs[0]][best_tested_model_idx[outputs[0]]][1],'\\n',\n",
    "                            test_results[outputs[1]][best_tested_model_idx[outputs[1]]][0],test_results[outputs[1]][best_tested_model_idx[outputs[1]]][1],'\\n',\n",
    "                            test_results[outputs[2]][best_tested_model_idx[outputs[2]]][0],test_results[outputs[2]][best_tested_model_idx[outputs[2]]][1],'\\n',\n",
    ")\n",
    "test_result=dict()\n",
    "#print('best tested model:',test_results[best_tested_model_idx][0],test_results[best_tested_model_idx][1])\n",
    "test_result[outputs[0]]=pd.DataFrame(test_results[outputs[0]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "\n",
    "test_result[outputs[1]]=pd.DataFrame(test_results[outputs[1]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "\n",
    "test_result[outputs[2]]=pd.DataFrame(test_results[outputs[2]],\n",
    "           columns=['Model','RMSE','MSE','R2_Score']).sort_values(by='RMSE')\n",
    "#import dataframe_image as dfi\n",
    "\n",
    "#dfi.export(test_result, \"ModelstestingResultsTable.png\")\n",
    "test_result[outputs[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[outputs[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[outputs[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_interv_results[outputs[0]],'\\n',conf_interv_results[outputs[1]],'\\n',conf_interv_results[outputs[2]],'\\n')\n",
    "\n",
    "conf_interv_result=dict()\n",
    "conf_interv_result[outputs[0]]=pd.DataFrame(conf_interv_results[outputs[0]],\n",
    "           columns=['Model','RMSE','C1','C2','Confidence Interval']).sort_values(by='RMSE',ignore_index=True)\n",
    "\n",
    "conf_interv_result[outputs[1]]=pd.DataFrame(conf_interv_results[outputs[1]],\n",
    "           columns=['Model','RMSE','C1','C2','Confidence Interval']).sort_values(by='RMSE',ignore_index=True)\n",
    "\n",
    "conf_interv_result[outputs[2]]=pd.DataFrame(conf_interv_results[outputs[2]],\n",
    "           columns=['Model','RMSE','C1','C2','Confidence Interval']).sort_values(by='RMSE',ignore_index=True)\n",
    "\n",
    "conf_interv_result[outputs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interv_result[outputs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interv_result[outputs[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf_interv_result[outputs[0]].drop('KNeighborsRegressor')\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.xticks(range(len(conf_interv_result[outputs[0]])),list(conf_interv_result[outputs[0]]['Model']),\n",
    "            rotation = 90, ha=\"right\")\n",
    "plt.title('RMSE Confidence Intervals Comparison of\\n Water_Quantity Output',fontsize=12)\n",
    "for C1,C2,b in zip(conf_interv_result[outputs[0]]['C1'],conf_interv_result[outputs[0]]['C2'],\n",
    "                    range(len(conf_interv_result[outputs[0]]))):\n",
    "    color='#2187bb'\n",
    "    horizontal_line_width=0.25\n",
    "    left = b - horizontal_line_width / 4\n",
    "    right = b + horizontal_line_width / 4\n",
    "    #plt.plot((b,b),(C1,C2),'ro-',color='orange')\n",
    "    plt.plot([b, b], [C2, C1], color=color)\n",
    "    plt.plot([left, right], [C2, C2], color=color)\n",
    "    plt.plot([left, right], [C1, C1], color=color)\n",
    "    plt.plot(b, conf_interv_result[outputs[0]]['RMSE'][b], 'o', color='#f44336')\n",
    "\n",
    "save_fig(f'RMSE Confidence Intervals of Water_Quantity Output')\n",
    "#plt.xticks(range(len(confidence_interval_results)),list(confidence_interval_results['Model']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf_interv_result[outputs[1]].drop('KNeighborsRegressor')\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.xticks(range(len(conf_interv_result[outputs[1]])),list(conf_interv_result[outputs[1]]['Model']),\n",
    "            rotation = 90, ha=\"right\")\n",
    "plt.title('RMSE Confidence Intervals Comparison of \\nDuration_of_Irrigation Output',fontsize=12)\n",
    "for C1,C2,b in zip(conf_interv_result[outputs[1]]['C1'],conf_interv_result[outputs[1]]['C2'],\n",
    "                    range(len(conf_interv_result[outputs[1]]))):\n",
    "    color='#2187bb'\n",
    "    horizontal_line_width=0.25\n",
    "    left = b - horizontal_line_width / 4\n",
    "    right = b + horizontal_line_width / 4\n",
    "    #plt.plot((b,b),(C1,C2),'ro-',color='orange')\n",
    "    plt.plot([b, b], [C2, C1], color=color)\n",
    "    plt.plot([left, right], [C2, C2], color=color)\n",
    "    plt.plot([left, right], [C1, C1], color=color)\n",
    "    plt.plot(b, conf_interv_result[outputs[1]]['RMSE'][b], 'o', color='#f44336')\n",
    "\n",
    "save_fig(f'RMSE Confidence Intervals of Duration_of_Irrigation Output')\n",
    "#plt.xticks(range(len(confidence_interval_results)),list(confidence_interval_results['Model']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf_interv_result[outputs[2]].drop('KNeighborsRegressor')\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.xticks(range(len(conf_interv_result[outputs[2]])),list(conf_interv_result[outputs[2]]['Model']),\n",
    "            rotation = 90, ha=\"right\")\n",
    "plt.title('RMSE Confidence Intervals Comparison of \\nIrrigation_Time_Intervals Output',fontsize=12)\n",
    "for C1,C2,b in zip(conf_interv_result[outputs[2]]['C1'],conf_interv_result[outputs[2]]['C2'],\n",
    "                    range(len(conf_interv_result[outputs[2]]))):\n",
    "    color='#2187bb'\n",
    "    horizontal_line_width=0.25\n",
    "    left = b - horizontal_line_width / 4\n",
    "    right = b + horizontal_line_width / 4\n",
    "    #plt.plot((b,b),(C1,C2),'ro-',color='orange')\n",
    "    plt.plot([b, b], [C2, C1], color=color)\n",
    "    plt.plot([left, right], [C2, C2], color=color)\n",
    "    plt.plot([left, right], [C1, C1], color=color)\n",
    "    plt.plot(b, conf_interv_result[outputs[2]]['RMSE'][b], 'o', color='#f44336')\n",
    "\n",
    "save_fig(f'RMSE Confidence Intervals of Irrigation_Time_Intervals Output')\n",
    "\n",
    "#plt.xticks(range(len(confidence_interval_results)),list(confidence_interval_results['Model']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions=np.array(test_prediction)[2,:].reshape(1,-1)[0,:]\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs\n",
    "#range(len(outputs)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_out,i in zip(outputs,range(len(outputs))):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(test_prediction[:,i][15:50],label='predected', marker=\".\")\n",
    "    plt.plot(y_test_inv[:,i][15:50],label='Actual',marker=\".\")\n",
    "\n",
    "    plt.title(f\"Predicted Vs. Actual of {pred_out} \\nsamples of Cascaded_Out_ANN\",fontsize=12)\n",
    "    plt.xlabel(\"samples\")\n",
    "    plt.ylabel(f\"{pred_out}\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    save_fig(f\"Predicted Vs. Actual of {pred_out} in discrete time series of Cascaded_Out_ANN\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model,model_to_dot\n",
    "from graphviz import Source\n",
    "import pydot\n",
    "\n",
    "#model_to_dot(best_models['Cascaded_Out_ANN'])\n",
    "plot_model(best_models['Cascaded_Out_ANN'], to_file='Cascaded_Out_ANN_Archeticture.png', show_shapes=True, show_layer_names=True)\n",
    "#\n",
    "#\n",
    "#Source.from_file(\"Cascaded_Out_ANN_Archeticture.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.models.save_model(models['Cascaded_Out_ANN'],'Cascaded_Out_ANN.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(\n",
    "#models['Cascaded_Out_ANN'],\n",
    "#to_file=r\"C:\\Users\\user\\Desktop\\iman\\Master\\RoboticsAndControlSystems\\Project\\Code\\IrrigationControlSystem\\model.png\",\n",
    "#show_shapes=True,\n",
    "#show_dtype=False,\n",
    "#show_layer_names=True,\n",
    "#rankdir=\"TB\",\n",
    "#expand_nested=True,\n",
    "#dpi=96,\n",
    "#layer_range=None,\n",
    "#show_layer_activations=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(\n",
    "#models['Parallel_ANN'],\n",
    "#to_file=r\"C:\\Users\\user\\Desktop\\iman\\Master\\RoboticsAndControlSystems\\Project\\Code\\IrrigationControlSystem\\model.png\",\n",
    "#show_shapes=True,\n",
    "#show_dtype=False,\n",
    "#show_layer_names=True,\n",
    "#rankdir=\"TB\",\n",
    "#expand_nested=True,\n",
    "#dpi=96,\n",
    "#layer_range=None,\n",
    "#show_layer_activations=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#tf.keras.utils.model_to_dot(\n",
    "#    models['Cascaded_Out_ANN'],\n",
    "#    show_shapes=False,\n",
    "#    show_dtype=False,\n",
    "#    show_layer_names=True,\n",
    "#    rankdir=\"TB\",\n",
    "#    expand_nested=False,\n",
    "#    dpi=96,\n",
    "#    subgraph=False,\n",
    "#    layer_range=None,\n",
    "#    show_layer_activations=False,\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
