{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt \n",
    "IMAGES_PATH = Path() / \"images\" / \"ReinforcementLearningResults\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install stable_baselines3\n",
    "!pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from stable_baselines3 import PPO , DDPG, SAC #,RecurrentPPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "# Load the pre-trained models\n",
    "crop_parameters_estimator = load_model('wcp_LSTM_model_model.h5')\n",
    "resource_consumption_estimator = joblib.load('GradientBoostingDailyResourceConsumptionEstimator.pkl')\n",
    "\n",
    "rewards = []\n",
    "\n",
    "class GreenhouseEnv(gym.Env):\n",
    "    def __init__(self, crop_parameters_estimator, resource_consumption_estimator, weather_data):\n",
    "        super(GreenhouseEnv, self).__init__()\n",
    "        self.crop_parameters_estimator = crop_parameters_estimator\n",
    "        self.resource_consumption_estimator = resource_consumption_estimator\n",
    "        \n",
    "        # Action space: shape (2016, 34) - 2016 time steps for 34 control setpoints\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(2016, 34), dtype=np.float64)\n",
    "        \n",
    "        # Observation space: weather (2016, 10), crop parameters (1, 3), resource consumption (1, 5)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'weather': spaces.Box(low=0, high=1, shape=(2016, 10), dtype=np.float64),\n",
    "            'crop_params': spaces.Box(low=0, high=1, shape=(1, 3), dtype=np.float64),\n",
    "            'resource_consumption': spaces.Box(low=0, high=7, shape=(1, 5), dtype=np.float64)\n",
    "        })\n",
    "        \n",
    "        # Initial state\n",
    "        self.weather_data = weather_data\n",
    "        self.current_step = 0\n",
    "        self.steps = 0\n",
    "        self.max_steps = 23\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Random initial control setpoints\n",
    "        control_setpoints = np.random.uniform(0, 1, size=(2016, 34)).reshape(2016, 34)\n",
    "        day = np.array([(i // 288 + 1) / 166 * (self.steps + 1) for i in range(2016)]).reshape(2016, 1)\n",
    "        initial_control_setpoints = np.concatenate([control_setpoints, day], axis=1).reshape(1, 2016, 35)\n",
    "        \n",
    "        initial_crop_params = np.zeros(3).reshape(1, 3)\n",
    "        initial_resource_consumption = np.zeros(5).reshape(1, 5)\n",
    "        \n",
    "        # Return the initial state\n",
    "        self.state = {\n",
    "            'weather': self.weather_data[self.steps  * 2016:(self.steps  + 1) * 2016],\n",
    "            'crop_params': initial_crop_params,\n",
    "            'resource_consumption': initial_resource_consumption\n",
    "        }\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        day = np.array([(i // 288 + 1) / 166 * (self.steps + 1) for i in range(2016)]).reshape(2016, 1)\n",
    "        control_setpoints = np.concatenate([action.reshape(2016, 34), day], axis=1).reshape(1, 2016, 35)\n",
    "        \n",
    "        # Predict crop parameters using the crop LSTM model\n",
    "        weekly_crop_params = self.crop_parameters_estimator.predict(control_setpoints)\n",
    "        \n",
    "        # Average daily control setpoints for resource consumption\n",
    "        a = action.reshape(7, 288, 34)\n",
    "        actions = np.array([a[i].mean(axis=0) for i in range(7)])\n",
    "        actions = pd.DataFrame(actions, columns=[\n",
    "            'AssimLight', 'BlackScr', 'CO2air', 'Cum_irr', 'EC_drain_PC', 'EnScr', 'HumDef', \n",
    "            'PipeGrow', 'PipeLow', 'Rhair', 'Tair', 'Tot_PAR', 'Tot_PAR_Lamps', 'VentLee', \n",
    "            'Ventwind', 'assim_vip', 'co2_dos', 'co2_vip', 'dx_vip', 'int_blue_vip', 'int_farred_vip', \n",
    "            'int_red_vip', 'int_white_vip', 'pH_drain_PC', 'scr_blck_vip', 'scr_enrg_vip', \n",
    "            't_grow_min_vip', 't_heat_vip', 't_rail_min_vip', 't_ventlee_vip', 't_ventwind_vip', \n",
    "            'water_sup', 'water_sup_intervals_vip_min', 'window_pos_lee_vip'\n",
    "        ])\n",
    "        daily_resource_consumption = self.resource_consumption_estimator.predict(actions).sum(axis=0)[:-1].reshape(1, 5)\n",
    "        #print(self.resource_consumption_estimator.predict(actions)[:])\n",
    "        for i in self.resource_consumption_estimator.predict(actions)[:]:\n",
    "            #print(np.any(i>=.9))\n",
    "            high_rc = np.any(i>=.9)\n",
    "            #print(high_rc)\n",
    "            if high_rc:\n",
    "                #print('break')\n",
    "                break\n",
    "        # Update state\n",
    "        self.state = {\n",
    "            'weather': self.weather_data[self.steps * 2016:(self.steps + 1) * 2016],\n",
    "            'crop_params': weekly_crop_params,\n",
    "            'resource_consumption': daily_resource_consumption\n",
    "        }\n",
    "        print(np.max(action),np.min(action),np.any(action>1),np.any(action<0))\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(weekly_crop_params[0], daily_resource_consumption[0],high_rc,action)\n",
    "        #print(np.any(weekly_crop_params[0]>=1 ) , np.any(daily_resource_consumption[0]>=7),high_rc)\n",
    "        done = bool((self.steps >= self.max_steps)) #or np.any(weekly_crop_params[0]>=.9 ) or np.any(daily_resource_consumption[0]>=6) or high_rc)\n",
    "        #print('done',done)\n",
    "        #rewards.append(reward)\n",
    "        \n",
    "        return self.state, reward, done, done, {}\n",
    "\n",
    "    def calculate_reward(self, crop_params, resource_consumption,high_rc,current_actions):\n",
    "        punishment=0\n",
    "        big_reward=0\n",
    "        alpha, beta, delta,gamma = 1, 1, .1,0.01 #1, 0.5, 0.1\n",
    "        w1, w2, w3 = 0.6, 0.6, 0.6\n",
    "        p1, p2, p3, p4 = 0.20, 0.35, 0.3, 0.15\n",
    "        #w1, w2, w3 = 1, 1, 1\n",
    "        #p1, p2, p3, p4 = 1, 1, 1, 1\n",
    "        max_stem_elong, max_stem_thick, max_cum_trusses = 1.0, 1.0, 1.0\n",
    "        max_heat, max_co2, max_electricity, max_irrigation = 7.0, 7.0, 7.0, 7.0\n",
    "\n",
    "        crop_reward = (w1 * (crop_params[0] / max_stem_elong) +\n",
    "                       w2 * (crop_params[1] / max_stem_thick) +\n",
    "                       w3 * (crop_params[2] / max_cum_trusses))\n",
    "        resource_penalty = (p1 * (resource_consumption[0] / max_heat) +\n",
    "                            p2 * (resource_consumption[1] / max_co2) +\n",
    "                            p3 * ((resource_consumption[2] + resource_consumption[3]) / max_electricity) +\n",
    "                            p4 * (resource_consumption[4] / max_irrigation))\n",
    "        #if high_rc:\n",
    "        #    punishment=-.1\n",
    "        if np.any(current_actions>1 ) or np.any(current_actions<0):\n",
    "            punishment-=.9\n",
    "        if np.any(resource_consumption>7):\n",
    "            print('resource_consumption>7',np.any(resource_consumption>7))\n",
    "            punishment-=.9\n",
    "        if np.any(crop_params<.5):\n",
    "            print('crop_params<.5',np.any(crop_params<.5))\n",
    "            punishment-=.22\n",
    "        if np.all(crop_params>=.5):\n",
    "            print('crop_params>=.5',np.all(crop_params>=.5))\n",
    "            big_reward=.01\n",
    "        if np.any(crop_params>=.7):\n",
    "            print('crop_params>=.7',np.any(crop_params>=.7))\n",
    "            big_reward+=.1\n",
    "        if np.any(crop_params>=.8):\n",
    "            print('crop_params>=.8',np.any(crop_params>=.8))\n",
    "            big_reward+=1\n",
    "        if np.all(crop_params>=.5):\n",
    "            print('all crop_params>=.5',np.all(crop_params>=.5))\n",
    "            big_reward+=1\n",
    "        if np.all(crop_params>=.7):\n",
    "            print('all crop_params>=.7',np.a11(crop_params>=.7))\n",
    "            big_reward+=1\n",
    "        \n",
    "        efficiency_factor = crop_reward / (1 + resource_penalty)\n",
    "        # Stability penalty\n",
    "        s = 0.01\n",
    "        max_delta_action = 34.0  # Assuming actions are normalized between 0 and 1\n",
    "        stability=0\n",
    "        #print(len(current_actions)-1)\n",
    "        for idx in range(len(current_actions)-1):\n",
    "            stability+=np.sum(np.abs(current_actions[idx]-current_actions[idx+1]))/ max_delta_action\n",
    "            #print('Action Difference',stability)\n",
    "        stability_penalty = s * stability\n",
    "        #print(stability_penalty)\n",
    "        reward = alpha * crop_reward - beta * resource_penalty + delta * efficiency_factor+punishment+big_reward- gamma * stability_penalty\n",
    "        print(reward)\n",
    "        #reward = alpha * crop_reward - beta * resource_penalty + delta * efficiency_factor+punishment+big_reward\n",
    "        #print(reward)\n",
    "        return reward\n",
    "    \n",
    "# Assuming weather_data is a preprocessed time series array for the environment\n",
    "weather_data = pd.read_csv('C:\\\\Users\\\\Iman.Hindi\\\\Desktop\\\\Iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\weather_fill_missing_values.csv',index_col='%time')#pd.read_csv('D:\\\\Iman\\\\AGHC\\\\CherryTomato\\\\Data\\\\Weather\\\\Weather.csv',index_col='%time') #  # Example placeholder data #np.random.rand(48384, 10)  # Placeholder data\n",
    "#print(weather_data.head())\n",
    "#print(weather_data.shape)\n",
    "#print(weather_data.isnull().sum())\n",
    "scaler=MinMaxScaler()\n",
    "weather_data=scaler.fit_transform(weather_data)[:2016*23]\n",
    "#print(weather_data[:10])\n",
    "#print(weather_data.shape)\n",
    "\n",
    "# Initialize the environment\n",
    "env = GreenhouseEnv(crop_parameters_estimator, resource_consumption_estimator, weather_data)\n",
    "check_env(env)\n",
    "import os\n",
    "# Define a folder for saving logs and models\n",
    "log_dir = \"./PPO_logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Define the policy and agent\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0, learning_rate=1e-5,tensorboard_log=log_dir,device=\"cuda\")# Neural network architecture) #MultiInputLstmPolicy #MultiInputPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the agent and tracking rewards\n",
    "n_episodes = 10000\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    print(f'Episode {episode + 1}')\n",
    "    state = env.reset(seed=42)[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    model.learn(total_timesteps=100000)\n",
    "    while not done:\n",
    "        action = model.predict(state,deterministic=True)[0]\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "    episode_rewards.append(episode_reward.astype('float32'))\n",
    "    print(f'Episode Reward: {episode_reward.astype(\"float32\")}')\n",
    "    \n",
    "    \n",
    "    \n",
    "# Plot training progress\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(episode_rewards, label='Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model with advanced evaluation\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, return_episode_rewards=True)\n",
    "print(f\"Mean Reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Save the trained agent\n",
    "model.save(\"ppo_greenhouse_agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "orgin_path = \"C:\\\\Users\\\\Iman.Hindi\\\\Desktop\\\\iman\\\\AGHC\\\\AutonomousGreenHouseChallenge\\\\Code\\\\Training\\\\\"\n",
    "filenames = ['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators'\n",
    "             ]\n",
    "\n",
    "T1_Action=pd.read_csv(orgin_path+f'{filenames[0]}Actions.csv',index_col='%time')\n",
    "T2_Action=pd.read_csv(orgin_path+f'{filenames[1]}Actions.csv',index_col='%time')\n",
    "T3_Action=pd.read_csv(orgin_path+f'{filenames[2]}Actions.csv',index_col='%time')\n",
    "T4_Action=pd.read_csv(orgin_path+f'{filenames[3]}Actions.csv',index_col='%time')\n",
    "T5_Action=pd.read_csv(orgin_path+f'{filenames[4]}Actions.csv',index_col='%time')\n",
    "T6_Action=pd.read_csv(orgin_path+f'{filenames[5]}Actions.csv',index_col='%time')\n",
    "\n",
    "\n",
    "T1_Results=pd.read_csv(orgin_path+f'{filenames[0]}Results.csv',index_col='%time')\n",
    "T2_Results=pd.read_csv(orgin_path+f'{filenames[1]}Results.csv',index_col='%time')\n",
    "T3_Results=pd.read_csv(orgin_path+f'{filenames[2]}Results.csv',index_col='%time')\n",
    "T4_Results=pd.read_csv(orgin_path+f'{filenames[3]}Results.csv',index_col='%time')\n",
    "T5_Results=pd.read_csv(orgin_path+f'{filenames[4]}Results.csv',index_col='%time')\n",
    "T6_Results=pd.read_csv(orgin_path+f'{filenames[5]}Results.csv',index_col='%time')\n",
    "\n",
    "T1_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[0]}resource_consumption.csv',index_col='%time')\n",
    "T2_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[1]}resource_consumption.csv',index_col='%time')\n",
    "T3_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[2]}resource_consumption.csv',index_col='%time')\n",
    "T4_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[3]}resource_consumption.csv',index_col='%time')\n",
    "T5_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[4]}resource_consumption.csv',index_col='%time')\n",
    "T5_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[5]}resource_consumption.csv',index_col='%time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#episode_rewards = []\n",
    "#model_actions=np.empty(action.shape)\n",
    "#model_results=[]\n",
    "#model_r_consumption=[]\n",
    "#n_episodes = 10\n",
    "resources=['Heat_cons','ElecHigh','ElecLow', 'CO2_cons','Irr']\n",
    "crop_param=['Stem_elong' ,'Stem_thick','Cum_trusses']\n",
    "#for episode in range(n_episodes):\n",
    "#    print(f'Episode {episode + 1}')\n",
    "#    state = env.reset(seed=42)[0]\n",
    "#    done = False\n",
    "#    episode_reward = 0\n",
    "#    model.learn(total_timesteps=10000)\n",
    "#    while not done:\n",
    "#        action = model.predict(state,deterministic=True)[0]\n",
    "#        model_actions=np.concatenate([np.array(model_actions),action],axis=0)\n",
    "#        state, reward, done, _, _ = env.step(action)\n",
    "#        model_results.append(state['crop_params'])\n",
    "#        model_r_consumption.append(state['resource_consumption'])\n",
    "#        episode_reward += reward\n",
    "#    episode_rewards.append(episode_reward.astype('float32'))\n",
    "#    print(f'Episode Reward: {episode_reward.astype(\"float32\")}')\n",
    "#    \n",
    "#    model_results=pd.DataFrame(np.array(model_results),columns=crop_param)\n",
    "#    model_r_consumption=pd.DataFrame(np.array(model_r_consumption),columns=resources)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "#model_actions=np.empty(action.shape)\n",
    "model_results=[]\n",
    "model_r_consumption=[]\n",
    "state = env.reset(seed=42)[0]\n",
    "done = False\n",
    "episode_reward = 0\n",
    "i=0\n",
    "\n",
    "while not done:\n",
    "        action = model.predict(state,deterministic=True)[0]\n",
    "        if i==0:\n",
    "          model_actions=action   \n",
    "        else: \n",
    "          model_actions=np.concatenate([model_actions,action],axis=0)\n",
    "        print(model_actions.shape)\n",
    "        i+=1\n",
    "        #model_actions=np.concatenate([np.array(model_actions),action],axis=0)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        model_results.append(state['crop_params'])\n",
    "        model_r_consumption.append(state['resource_consumption'])\n",
    "        episode_reward += reward\n",
    "episode_rewards.append(episode_reward.astype('float32'))\n",
    "print(f'Episode Reward: {episode_reward.astype(\"float32\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results=pd.DataFrame(np.array(model_results).reshape(23,3),columns=crop_param)\n",
    "model_r_consumption=pd.DataFrame(np.array(model_r_consumption).reshape(23,5),columns=resources)\n",
    "model_actions=pd.DataFrame(np.array(model_actions),columns=T1_Action.drop('days',axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.set_index(T1_Results.index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r_consumption.set_index(T1_resource_Consumption[:-1].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_actions=model_actions[:len(T5_Action.index)].set_index(T5_Action.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_actions.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams=['Team1','Team2','Team3','Team4','Team5','Ours']\n",
    "teams_Results={ 'Team1': T1_Results,\n",
    "                'Team2': T2_Results,\n",
    "                'Team3': T3_Results,\n",
    "                'Team4': T4_Results,\n",
    "                'Team5': T5_Results,\n",
    "                'Ours':  model_results,\n",
    "                    \n",
    "                    }\n",
    "teams_Actions={ 'Team1': T1_Action,\n",
    "                'Team2': T2_Action,\n",
    "                'Team3': T3_Action,\n",
    "                'Team4': T4_Action,\n",
    "                'Team5': T5_Action,\n",
    "                'Ours':  model_actions,\n",
    "                    }\n",
    "teams_rc={      'Team1': T1_resource_Consumption,\n",
    "                'Team2': T2_resource_Consumption,\n",
    "                'Team3': T3_resource_Consumption,\n",
    "                'Team4': T4_resource_Consumption,\n",
    "                'Team5': T5_resource_Consumption,\n",
    "                'Ours':  model_r_consumption,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Teams results\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for result in teams_Results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        teams_Results[teams[i]][result].plot(ylabel=result,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for action in teams_Actions[teams[5]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        teams_Actions[teams[i]][\"2019-12-16 00:00:00\" : \"2019-12-26 00:00:00\"][action].plot(ylabel=action,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{action}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{action}_time_series_plot ')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Teams rc\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in teams_rc[teams[0]].columns[:-1]:\n",
    "    for i in range(len(teams)):\n",
    "        teams_rc[teams[i]][resource].plot(ylabel=resource,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_episode_rewards = {}\n",
    "for team in teams:\n",
    "    print(f'Evaluate {team} Actions....')\n",
    "    \n",
    "    teams_model_actions=np.empty((2016, 35))\n",
    "    Our_model_actions=np.empty((2016, 34))\n",
    "    \n",
    "    model_results=[]\n",
    "    model_r_consumption=[]\n",
    "    state = env.reset(seed=42)[0]\n",
    "    done = False\n",
    "    episode_reward = []\n",
    "    i=0\n",
    "    while not done:\n",
    "            if team=='Ours':\n",
    "                action = model.predict(state,deterministic=True)[0]\n",
    "                Our_model_actions=np.concatenate([np.array(Our_model_actions),np.array(action)],axis=0)\n",
    "                state, reward, done, _, _ = env.step(np.array(action))\n",
    "            else:\n",
    "                action = teams_Actions[team].iloc[2016*i:2016*(i+1)]\n",
    "                teams_model_actions=np.concatenate([np.array(teams_model_actions),np.array(action)],axis=0)\n",
    "                print(action.shape)\n",
    "                state, reward, done, _, _ = env.step(np.array(action.drop('days',axis=1)))\n",
    "            i+=1\n",
    "            \n",
    "            \n",
    "            model_results.append(state['crop_params'])\n",
    "            model_r_consumption.append(state['resource_consumption'])\n",
    "            episode_reward.append(reward.astype('float32'))\n",
    "    print(episode_reward)\n",
    "    teams_episode_rewards[team]=np.array(episode_reward)\n",
    "    print(f'{team} Episode Reward: {np.array(teams_episode_rewards[team]).sum().astype(\"float32\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_episode_rewards['Team1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_episode_rewards_df=pd.DataFrame(teams_episode_rewards,columns=teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_episode_rewards_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=pd.DataFrame(teams_episode_rewards_df.sum(axis=0).sort_values(ascending=False))\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=final_result.index,height=final_result[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
