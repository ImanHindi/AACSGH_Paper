{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from stable_baselines3 import PPO , DDPG, SAC,TD3 #,RecurrentPPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "# Print the name of each available GPU\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Load the pre-trained models\n",
    "crop_parameters_estimator = load_model('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/wcp_LSTM_model_fs_model.h5')\n",
    "resource_consumption_estimator = load_model('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/rc_LSTM_model_fs_model.h5')\n",
    "gh_climate_estimator = load_model('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/ghc_mlp_model_fs_model.h5')\n",
    "\n",
    "rewards = []\n",
    "CP_important_feature=[ 'Tair', 'pH_drain_PC', 'Cum_irr', 't_heat_vip', 'water_sup', 'Tot_PAR', 'water_sup_intervals_vip_min', 'PipeGrow', 'EC_drain_PC',\n",
    "                        'BlackScr', 'co2_dos', 'Tot_PAR_Lamps', 'scr_enrg_vip', 'Rhair', 'HumDef', 'days']\n",
    "\n",
    "GH_C_important_feature=['PARout',  'Tout',  'Iglob',  'RadSum',  'scr_enrg_vip',  't_heat_vip',  'int_white_vip',  'scr_blck_vip',  'pH_drain_PC',  'co2_vip',\n",
    "                        't_ventlee_vip',  'days']\n",
    "\n",
    "RC_important_feature=[     'Cum_irr',  'BlackScr',  'water_sup_intervals_vip_min',  'EC_drain_PC',  'pH_drain_PC',  'CO2air',  'water_sup',  'HumDef',\n",
    "                            'Rhair',   'Tot_PAR' ]\n",
    "\n",
    "actions_sp=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min',  'days'] #10 Actions \n",
    "\n",
    "important_ghc=['BlackScr','CO2air','Cum_irr','EC_drain_PC','PipeGrow','HumDef','Rhair','Tair','Tot_PAR','Tot_PAR_Lamps']\n",
    "\n",
    "weather_sp=[ 'PARout',   'Tout', 'Iglob',  'RadSum']\n",
    "\n",
    "GH_C_Out_columns=['AssimLight','BlackScr','CO2air','Cum_irr','EC_drain_PC','EnScr','HumDef','PipeGrow','PipeLow','Rhair','Tair','Tot_PAR',\n",
    "                  'Tot_PAR_Lamps','VentLee','Ventwind' ,'assim_vip','co2_dos' ]\n",
    "class GreenhouseEnv(gym.Env):\n",
    "    def __init__(self, crop_parameters_estimator, resource_consumption_estimator, gh_climate_estimator,weather_data):\n",
    "        super(GreenhouseEnv, self).__init__()\n",
    "        self.crop_parameters_estimator = crop_parameters_estimator\n",
    "        self.resource_consumption_estimator = resource_consumption_estimator\n",
    "        self.gh_climate_estimator=gh_climate_estimator\n",
    "        # Action space: shape (2016, 34) - 2016 time steps for 34 control setpoints\n",
    "        self.action_space = spaces.Box(low=0,high=1,shape=(2016 * 9,),dtype=np.float64) #Flattened shape: 2016*34\n",
    "    \n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'weather': spaces.Box(low=0, high=1, shape=(2016, 10), dtype=np.float64),\n",
    "            'crop_params': spaces.Box(low=0, high=1, shape=(1, 3), dtype=np.float64),\n",
    "            'resource_consumption': spaces.Box(low=0, high=7, shape=(1, 5), dtype=np.float64),\n",
    "            'gh_climate': spaces.Box(low=-10, high=10, shape=(2016, 10), dtype=np.float64)\n",
    "        })\n",
    "        \n",
    "        # Initial state\n",
    "        self.weather_data = weather_data\n",
    "        self.current_step = 0\n",
    "        self.steps = 0\n",
    "        self.max_steps = 23\n",
    "        self.days=np.array([(i // 288 ) / 166  for i in range(2016*(self.max_steps))]).reshape(2016*(self.max_steps), 1)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.steps = 0\n",
    "        self.days=np.array([(i // 288 ) / 166  for i in range(2016*(self.max_steps))]).reshape(2016*(self.max_steps), 1)\n",
    "        \n",
    "        initial_crop_params = np.zeros(3).reshape(1, 3)+np.random.uniform(0, .1, size=(1,3))\n",
    "        initial_resource_consumption = np.zeros(5).reshape(1, 5)+np.random.uniform(0, .1, size=(1,5))\n",
    "        initial_daily_resource_consumption = np.zeros(shape=(7,1,5)).reshape(7,1,5)+np.random.uniform(0, .1, size=(7,1,5))\n",
    "        initial_gh_climate=np.zeros(shape=(1,2016,10)).reshape(2016,10)+np.random.uniform(0, .1, size=(2016,10))\n",
    "        # Return the initial state\n",
    "        self.state = {\n",
    "            'weather': np.array(self.weather_data[self.steps * 2016:(self.steps + 1) * 2016]),\n",
    "            'crop_params': initial_crop_params,\n",
    "            'resource_consumption': initial_resource_consumption,\n",
    "            'gh_climate': initial_gh_climate\n",
    "        }\n",
    "        print(self.state['weather'].shape)\n",
    "        self.daily_res_cons=initial_daily_resource_consumption\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        action = action.reshape((2016, 9))\n",
    "        day = np.array(self.days[self.steps * 2016:(self.steps + 1) * 2016]).reshape(2016, 1) #np.array([(i // 288 + 1) / 166 * (self.steps + 1) for i in range(2016)]).reshape(2016, 1)\n",
    "        self.steps += 1\n",
    "        print('day',day)\n",
    "        control_setpoints = np.concatenate([action.reshape(2016, 9), day], axis=1)\n",
    "                \n",
    "        control_setpoints = pd.DataFrame(control_setpoints, columns=actions_sp)\n",
    "        print(type(self.state['weather']))\n",
    "        print(type(control_setpoints))\n",
    "        weather=pd.DataFrame(self.state['weather'],columns=w_columns)\n",
    "        #predict gh_climate:\n",
    "        w_sp_data = pd.concat([weather, control_setpoints], axis=1)\n",
    "        GH_C_Estimator_Input=np.array(w_sp_data[GH_C_important_feature])\n",
    "\n",
    "        print('w_sp_data',w_sp_data.shape)\n",
    "        ghclimate=self.gh_climate_estimator.predict(GH_C_Estimator_Input).reshape(2016,17)\n",
    "        gh_climate=pd.DataFrame(ghclimate,columns=GH_C_Out_columns)\n",
    "        \n",
    "        # Predict crop parameters using the crop LSTM model\n",
    "        CP_Estimator_Input = np.array(pd.concat([gh_climate, control_setpoints], axis=1)[CP_important_feature]).reshape(1,2016,16)\n",
    "        weekly_crop_params = self.crop_parameters_estimator.predict(CP_Estimator_Input).reshape(1,3)\n",
    "\n",
    "        RC_Estimator_Input = np.array(pd.concat([gh_climate, control_setpoints], axis=1)[RC_important_feature]).reshape(7, 288, 10)\n",
    "        \n",
    "        daily_resource_consumption = self.resource_consumption_estimator.predict(RC_Estimator_Input)\n",
    "        print(RC_Estimator_Input.shape) #(7, 288, 11)\n",
    "\n",
    "        print(daily_resource_consumption.shape) #(7, 288, 5)\n",
    "        self.daily_res_cons=daily_resource_consumption\n",
    "        weekly_resource_consumption = self.resource_consumption_estimator.predict(RC_Estimator_Input).sum(axis=0).reshape(1, 5)\n",
    "        print(weekly_resource_consumption.shape)\n",
    "        for i in self.resource_consumption_estimator.predict(RC_Estimator_Input)[:]:\n",
    "            high_rc = np.any(i>=1)\n",
    "            if high_rc:\n",
    "                break\n",
    "        # Update state\n",
    "        self.state = {\n",
    "            'weather': self.weather_data[self.steps * 2016:(self.steps + 1) * 2016],\n",
    "            'crop_params': weekly_crop_params,\n",
    "            'resource_consumption': weekly_resource_consumption,\n",
    "            'gh_climate': np.array(gh_climate[important_ghc]).reshape(2016,10)\n",
    "        }\n",
    "        print(weekly_crop_params[0].shape)\n",
    "        print(weekly_resource_consumption[0].shape)\n",
    "        print(weekly_resource_consumption)\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(weekly_crop_params[0], weekly_resource_consumption[0],high_rc,action)\n",
    "        done = bool((self.steps >= self.max_steps) or np.any(weekly_crop_params[0]<0.2 ) or np.any(weekly_resource_consumption[0]>7) or np.all(weekly_crop_params[0]<0.5) ) #or high_rc)\n",
    "        \n",
    "        return self.state, reward, done, done, {}\n",
    "    def calculate_reward(self, crop_params, resource_consumption,high_rc,current_actions):\n",
    "        punishment=0\n",
    "        big_reward=0\n",
    "        alpha, beta, delta,gamma = 1, 1, 0.02,0.002 # Adjusted delta and gamma values\n",
    "        w1, w2, w3 = 1, 1, 1\n",
    "        p1, p2, p3, p4 = 1, 1, 1, 1\n",
    "        max_stem_elong, max_stem_thick, max_cum_trusses = 1.0, 1.0, 1.0\n",
    "        max_heat, max_co2, max_electricity, max_irrigation = 7.0, 7.0, 7.0, 7.0\n",
    "\n",
    "        crop_reward = (w1 * (crop_params[0] / max_stem_elong) +\n",
    "                       w2 * (crop_params[1] / max_stem_thick) +\n",
    "                       w3 * (crop_params[2] / max_cum_trusses))\n",
    "        resource_penalty = (p1 * (resource_consumption[0] / max_heat) +\n",
    "                            p2 * (resource_consumption[1] / max_co2) +\n",
    "                            p3 * ((resource_consumption[2] + resource_consumption[3]) / max_electricity) +\n",
    "                            p4 * (resource_consumption[4] / max_irrigation))\n",
    "\n",
    "        if np.any(resource_consumption>7):\n",
    "            print('resource_consumption>7',np.any(resource_consumption>7))\n",
    "            punishment-=1\n",
    "        if np.any(crop_params<.5):\n",
    "            print('crop_params<.5',np.any(crop_params<.5))\n",
    "            punishment-=0.8\n",
    "        \n",
    "        if np.all(crop_params<.5):\n",
    "            print('all crop_params<.5',np.all(crop_params<.5))\n",
    "            punishment-=1\n",
    "        if np.any(crop_params>=.7):\n",
    "            print('crop_params>=.7',np.any(crop_params>=.7))\n",
    "            big_reward+=.5\n",
    "        if np.any(crop_params>=.8):\n",
    "            print('crop_params>=.8',np.any(crop_params>=.8))\n",
    "            big_reward+=.8\n",
    "        if np.all(crop_params>=.5):\n",
    "            print('all crop_params>=.5',np.all(crop_params>=.5))\n",
    "            big_reward+=.8\n",
    "        if np.all(crop_params>=.7):\n",
    "            print('all crop_params>=.7',np.all(crop_params>=.7))\n",
    "            big_reward+=.8\n",
    "        if np.all(crop_params>=.8):\n",
    "            print('all crop_params>=.8',np.all(crop_params>=.8))\n",
    "            big_reward+=1\n",
    "        efficiency_factor = crop_reward / (1 + resource_penalty)\n",
    "        print(efficiency_factor,'efficiency_factor')\n",
    "        #Stability penalty\n",
    "        s = 0.01\n",
    "        max_delta_action = 16.0  # Assuming actions are normalized between 0 and 1\n",
    "        stability=0\n",
    "        for idx in range(len(current_actions)-1):\n",
    "            stability+=np.sum(np.abs(current_actions[idx]-current_actions[idx+1]))/ max_delta_action\n",
    "        stability_penalty = s * stability\n",
    "\n",
    "        reward = alpha * crop_reward - beta * resource_penalty +punishment+big_reward + delta * efficiency_factor - gamma * stability_penalty\n",
    "        print(reward)\n",
    "        return reward\n",
    "    \n",
    "# Assuming weather_data is a preprocessed time series array for the environment\n",
    "weather_data = pd.read_csv('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/weather_fill_missing_values.csv',index_col='%time')#pd.read_csv('D:\\\\Iman\\\\AGHC\\\\CherryTomato\\\\Data\\\\Weather\\\\Weather.csv',index_col='%time') #  # Example placeholder data #np.random.rand(48384, 10)  # Placeholder data\n",
    "w_columns=weather_data.columns\n",
    "print(weather_data.shape)\n",
    "scaler=MinMaxScaler()\n",
    "weather_data=scaler.fit_transform(weather_data)\n",
    "\n",
    "print(weather_data.shape)\n",
    "# Random generation of weather data\n",
    "weather_data = np.random.rand(48384, 10)\n",
    "\n",
    "# Initialize the environment\n",
    "env = GreenhouseEnv(crop_parameters_estimator, resource_consumption_estimator,gh_climate_estimator, weather_data)\n",
    "check_env(env)\n",
    "\n",
    "import os\n",
    "# Define a folder for saving logs_fs and models\n",
    "log_dir = \"./TD3_logs_fs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment in a monitor to track performance\n",
    "env = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of actions in the environment (for noise generation)\n",
    "n_actions = env.action_space.shape[-1]\n",
    "# Define action noise for exploration (Ornstein-Uhlenbeck noise)\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions), theta=0.15)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(device)\n",
    "# Define the action noise for exploration (NormalActionNoise)\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Optimized TD3 hyperparameters\n",
    "TD3_model = TD3(\n",
    "    \"MultiInputPolicy\",  # Use the default MLP policy\n",
    "    env, \n",
    "    verbose=1,  # Print training progress\n",
    "    learning_rate=1e-4,  # Learning rate for the actor and critic networks\n",
    "    buffer_size=100_000,  # Replay buffer size\n",
    "    batch_size=256,  # Batch size for training\n",
    "    tau=0.005,  # Soft update coefficient for target networks\n",
    "    gamma=0.99,  # Discount factor for future rewards\n",
    "    train_freq=(1, \"episode\"),  # Train after every episode\n",
    "    gradient_steps=100,  # Number of gradient steps to perform after each episode\n",
    "    learning_starts=1000,  # Start learning after 10,000 timesteps\n",
    "    policy_delay=2,  # Policy update delay\n",
    "    target_policy_noise=0.2,  # Target smoothing noise for the next state\n",
    "    target_noise_clip=0.5,  # Clip noise\n",
    "    action_noise=action_noise,  # Exploration noise\n",
    "    policy_kwargs=dict(net_arch=[256, 256,128]),  # Network architecture for the actor and critic networks\n",
    "    tensorboard_log=log_dir,  # Optional: Tensorboard log directory\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create evaluation callback to monitor agent performance every 100k steps\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./TD3_logs_fs/best_model/',\n",
    "                             log_path='./TD3_logs_fs/results/', eval_freq=10_000, deterministic=True, render=False)\n",
    "\n",
    "# Save models every 100k steps\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10_000, save_path='./TD3_logs_fs/checkpoints/', name_prefix='TD3_greenhouse')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "TD3_model.learn(total_timesteps=100_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the trained model\n",
    "TD3_model.save(\"TD3_greenhouse_final_model\")\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(TD3_model, env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create evaluation callback to monitor agent performance every 100k steps\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./TD3_logs_fs/best_model/',\n",
    "                             log_path='./TD3_logs_fs/results/', eval_freq=10000, deterministic=True, render=False)\n",
    "\n",
    "# Save models every 100k steps\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./TD3_logs_fs/checkpoints/', name_prefix='TD3_greenhouse')\n",
    "\n",
    "import os\n",
    "from stable_baselines3 import TD3\n",
    "\n",
    "# Define the path to the checkpoint directory\n",
    "checkpoint_dir = './TD3_logs_fs/checkpoints/'\n",
    "\n",
    "# latest_checkpoint = checkpoints[latest_checkpoint_index]\n",
    "latest_checkpoint_path = os.path.join(checkpoint_dir, 'TD3_greenhouse_50000_steps.zip')\n",
    "\n",
    "# Load the model from the latest checkpoint\n",
    "print(f\"Loading model from {latest_checkpoint_path}\")\n",
    "TD3_model = TD3.load(latest_checkpoint_path, env=env)\n",
    "\n",
    "# Continue training\n",
    "TD3_model.learn(total_timesteps=80_000, callback=[eval_callback, checkpoint_callback])  # Adjust timesteps as needed\n",
    "\n",
    "# Optionally save the model again after continuing training\n",
    "TD3_model.save(\"TD3_greenhouse_final_model_after_resume\")\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(TD3_model, env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO,DDPG,TD3\n",
    "\n",
    "# Define the path to the best model directory\n",
    "best_model_dir = r'C:\\Users\\Iman.Hindi\\Desktop\\AACSGH_Paper\\models\\weights'\n",
    "\n",
    "# Check if the best model file exists\n",
    "best_model_filename = 'TD3_greenhouse_final_model.zip'  # Change this to your actual best model filename if different\n",
    "best_model_path = os.path.join(best_model_dir, best_model_filename)\n",
    "\n",
    "# Load the best model\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Loading the best model from {best_model_path}\")\n",
    "    best_model = TD3.load(best_model_path, env=env)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No best model found at {best_model_path}\")\n",
    "\n",
    "# Optionally, evaluate the best model\n",
    "mean_reward, std_reward = evaluate_policy(best_model, env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward of the best model: {mean_reward} +/- {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "\n",
    "import os\n",
    "from stable_baselines3 import TD3\n",
    "\n",
    "# Define the path to the checkpoint directory\n",
    "checkpoint_dir = 'C:\\\\Users\\\\Iman.Hindi\\\\Desktop\\\\AACSGH_Paper\\\\results\\\\best_training_logs\\\\TD3_logs_fs\\\\checkpoints\\\\'\n",
    "\n",
    "latest_checkpoint_path = os.path.join(checkpoint_dir, 'TD3_greenhouse_50000_steps.zip')\n",
    "\n",
    "# Load the model from the latest checkpoint\n",
    "best_model = TD3.load(latest_checkpoint_path)\n",
    "\n",
    "# Continue training\n",
    "#TD3_model.learn(total_timesteps=80_000, callback=[eval_callback, checkpoint_callback])  # Adjust timesteps as needed\n",
    "\n",
    "# Optionally save the model again after continuing training\n",
    "best_model.save(\"TD3_greenhouse_final_model_after_resume\")\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(best_model,  n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "orgin_path = \"/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/\"\n",
    "filenames = ['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators'\n",
    "             ]\n",
    "\n",
    "T1_Action=pd.read_csv(orgin_path+f'{filenames[0]}Actions.csv',index_col='%time')\n",
    "T2_Action=pd.read_csv(orgin_path+f'{filenames[1]}Actions.csv',index_col='%time')\n",
    "T3_Action=pd.read_csv(orgin_path+f'{filenames[2]}Actions.csv',index_col='%time')\n",
    "T4_Action=pd.read_csv(orgin_path+f'{filenames[3]}Actions.csv',index_col='%time')\n",
    "T5_Action=pd.read_csv(orgin_path+f'{filenames[4]}Actions.csv',index_col='%time')\n",
    "T6_Action=pd.read_csv(orgin_path+f'{filenames[5]}Actions.csv',index_col='%time')\n",
    "\n",
    "\n",
    "T1_Results=pd.read_csv(orgin_path+f'{filenames[0]}Results.csv',index_col='%time')[:18]\n",
    "T2_Results=pd.read_csv(orgin_path+f'{filenames[1]}Results.csv',index_col='%time')[:18]\n",
    "T3_Results=pd.read_csv(orgin_path+f'{filenames[2]}Results.csv',index_col='%time')[:18]\n",
    "T4_Results=pd.read_csv(orgin_path+f'{filenames[3]}Results.csv',index_col='%time')[:18]\n",
    "T5_Results=pd.read_csv(orgin_path+f'{filenames[4]}Results.csv',index_col='%time')[:18]\n",
    "T6_Results=pd.read_csv(orgin_path+f'{filenames[5]}Results.csv',index_col='%time')[:18]\n",
    "\n",
    "T1_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[0]}resource_consumption.csv',index_col='%time')\n",
    "T2_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[1]}resource_consumption.csv',index_col='%time')\n",
    "T3_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[2]}resource_consumption.csv',index_col='%time')\n",
    "T4_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[3]}resource_consumption.csv',index_col='%time')\n",
    "T5_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[4]}resource_consumption.csv',index_col='%time')\n",
    "T6_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[5]}resource_consumption.csv',index_col='%time')\n",
    "\n",
    "actions_sp=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min',  'days'] #10 Actions \n",
    "resources=['Heat_cons','ElecHigh','ElecLow', 'CO2_cons','Irr']\n",
    "crop_param=['Stem_elong' ,'Stem_thick','Cum_trusses']\n",
    "\n",
    "episode_rewards = []\n",
    "#model_actions=np.empty(action.shape)\n",
    "model_results=[]\n",
    "model_r_consumption=[]\n",
    "env = GreenhouseEnv(crop_parameters_estimator, resource_consumption_estimator,gh_climate_estimator, weather_data)\n",
    "check_env(env)\n",
    "state = env.reset(seed=42)[0]\n",
    "done = False\n",
    "episode_reward = 0\n",
    "i=0\n",
    "model_action=[]\n",
    "while not done:\n",
    "        action = best_model.predict(state,deterministic=True)[0]\n",
    "        if i==0:\n",
    "          model_actions=action \n",
    "          print('model_actions',model_actions)  \n",
    "        else: \n",
    "          model_actions=np.concatenate([model_actions,action],axis=0)\n",
    "        print(model_actions)\n",
    "        print(model_actions.shape)\n",
    "        i+=1\n",
    "        #model_actions=np.concatenate([np.array(model_actions),action],axis=0)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        model_results.append(state['crop_params'])\n",
    "        model_r_consumption.append(state['resource_consumption'])\n",
    "        model_action.append(action)\n",
    "        episode_reward += reward\n",
    "episode_rewards.append(episode_reward.astype('float32'))\n",
    "print(f'Episode Reward: {episode_reward.astype(\"float32\")}')\n",
    "sp=actions_sp\n",
    "sp_noday=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min'] #10 Actions \n",
    "model_actions.shape\n",
    "model_results=pd.DataFrame(np.array(model_results).reshape(23,3),columns=crop_param)[:18]\n",
    "model_r_consumption=pd.DataFrame(np.array(model_r_consumption).reshape(23,5),columns=resources)\n",
    "model_actions=pd.DataFrame(np.array(model_action).reshape(23*2016,9),columns=sp_noday)\n",
    "model_results.set_index(T1_Results.index,inplace=True)\n",
    "model_r_consumption.set_index(T1_resource_Consumption[:-1].index,inplace=True)\n",
    "\n",
    "model_actions=model_actions[:len(T5_Action.index)].set_index(T5_Action.index[:len(model_actions.index)])\n",
    "\n",
    "model_actions.index\n",
    "teams=['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators','Ours']\n",
    "teams_Results={ teams[0]: T1_Results,\n",
    "                 teams[1]: T2_Results,\n",
    "                 teams[2]: T3_Results,\n",
    "                 teams[3]: T4_Results,\n",
    "                 teams[4]: T5_Results,\n",
    "                 teams[5]: T6_Results,\n",
    "                 teams[6]:  model_results,\n",
    "                    \n",
    "                    }\n",
    "teams_Actions={ teams[0]: T1_Action[actions_sp],\n",
    "                teams[1]: T2_Action[actions_sp],\n",
    "                teams[2]: T3_Action[actions_sp],\n",
    "                teams[3]: T4_Action[actions_sp],\n",
    "                teams[4]: T5_Action[actions_sp],\n",
    "                teams[5]: T6_Action[actions_sp],\n",
    "                teams[6]:  model_actions[sp_noday],\n",
    "                    }\n",
    "teams_rc={      teams[0]: T1_resource_Consumption,\n",
    "                teams[1]: T2_resource_Consumption,\n",
    "                teams[2]: T3_resource_Consumption,\n",
    "                teams[3]: T4_resource_Consumption,\n",
    "                teams[4]: T5_resource_Consumption,\n",
    "                teams[5]: T6_resource_Consumption,\n",
    "                teams[6]:  model_r_consumption,\n",
    "    \n",
    "}\n",
    "#plot Teams results\n",
    "print(teams_Results[teams[0]].columns)\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for result in teams_Results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        teams_Results[teams[i]][result].plot(ylabel=result,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "#plot Teams actions\n",
    "# import matplotlib.pyplot as plt\n",
    "# for action in teams_Actions[teams[5]].columns:\n",
    "#     for i in range(len(teams)):\n",
    "#         teams_Actions[teams[i]][\"2019-12-16 00:00:00\" : \"2019-12-17 00:00:00\"][action].plot(ylabel=action,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "#     plt.title(f'green_house_{action}_time_series_plot for all Teams ')\n",
    "#     plt.legend()\n",
    "#     #save_fig(f'green_house_{action}_time_series_plot ')\n",
    "\n",
    "#     plt.show()\n",
    "#plot Teams rc\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in teams_rc[teams[0]].columns[:-1]:\n",
    "    for i in range(len(teams)):\n",
    "        teams_rc[teams[i]][resource].plot(ylabel=resource,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "teams_episode_rewards = {}\n",
    "for team in teams:\n",
    "    print(f'Evaluate {team} Actions....')\n",
    "    \n",
    "    teams_model_actions=np.empty((2016, 10))\n",
    "    Our_model_actions=np.empty((2016, 9))\n",
    "    \n",
    "    model_results=[]\n",
    "    model_r_consumption=[]\n",
    "    state = env.reset(seed=42)[0]\n",
    "    done = False\n",
    "    episode_reward = []\n",
    "    i=0\n",
    "    sp=actions_sp\n",
    "    while not done:\n",
    "            if team=='Ours':\n",
    "                action = best_model.predict(state,deterministic=True)[0]\n",
    "                Our_model_actions=np.concatenate([np.array(Our_model_actions),np.array(action).reshape(2016,9)],axis=0)\n",
    "                state, reward, done, _, _ = env.step(np.array(action))\n",
    "            else:\n",
    "                action = teams_Actions[team].iloc[2016*i:2016*(i+1)][sp]\n",
    "                teams_model_actions=np.concatenate([np.array(teams_model_actions),np.array(action)],axis=0)\n",
    "                print(action.shape)\n",
    "                state, reward, done, _, _ = env.step(np.array(action.drop('days',axis=1)).reshape(2016*9,))\n",
    "            i+=1\n",
    "            \n",
    "            \n",
    "            model_results.append(state['crop_params'][0])\n",
    "            model_r_consumption.append(state['resource_consumption'])\n",
    "            episode_reward.append(reward.astype('float32'))\n",
    "    print(episode_reward)\n",
    "    teams_episode_rewards[team]=np.array(episode_reward)\n",
    "    print(f'{team} Episode Reward: {np.array(teams_episode_rewards[team]).sum().astype(\"float32\")}')\n",
    "print(model_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "orgin_path = \"/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/\"\n",
    "filenames = ['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators'\n",
    "             ]\n",
    "\n",
    "T1_Action=pd.read_csv(orgin_path+f'{filenames[0]}Actions.csv',index_col='%time')\n",
    "T2_Action=pd.read_csv(orgin_path+f'{filenames[1]}Actions.csv',index_col='%time')\n",
    "T3_Action=pd.read_csv(orgin_path+f'{filenames[2]}Actions.csv',index_col='%time')\n",
    "T4_Action=pd.read_csv(orgin_path+f'{filenames[3]}Actions.csv',index_col='%time')\n",
    "T5_Action=pd.read_csv(orgin_path+f'{filenames[4]}Actions.csv',index_col='%time')\n",
    "T6_Action=pd.read_csv(orgin_path+f'{filenames[5]}Actions.csv',index_col='%time')\n",
    "\n",
    "\n",
    "T1_Results=pd.read_csv(orgin_path+f'{filenames[0]}Results.csv',index_col='%time')[:18]\n",
    "T2_Results=pd.read_csv(orgin_path+f'{filenames[1]}Results.csv',index_col='%time')[:18]\n",
    "T3_Results=pd.read_csv(orgin_path+f'{filenames[2]}Results.csv',index_col='%time')[:18]\n",
    "T4_Results=pd.read_csv(orgin_path+f'{filenames[3]}Results.csv',index_col='%time')[:18]\n",
    "T5_Results=pd.read_csv(orgin_path+f'{filenames[4]}Results.csv',index_col='%time')[:18]\n",
    "T6_Results=pd.read_csv(orgin_path+f'{filenames[5]}Results.csv',index_col='%time')[:18]\n",
    "\n",
    "T1_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[0]}resource_consumption.csv',index_col='%time')\n",
    "T2_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[1]}resource_consumption.csv',index_col='%time')\n",
    "T3_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[2]}resource_consumption.csv',index_col='%time')\n",
    "T4_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[3]}resource_consumption.csv',index_col='%time')\n",
    "T5_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[4]}resource_consumption.csv',index_col='%time')\n",
    "T6_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[5]}resource_consumption.csv',index_col='%time')\n",
    "\n",
    "actions_sp=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min',  'days'] #10 Actions \n",
    "resources=['Heat_cons','ElecHigh','ElecLow', 'CO2_cons','Irr']\n",
    "crop_param=['Stem_elong' ,'Stem_thick','Cum_trusses']\n",
    "\n",
    "episode_rewards = []\n",
    "#model_actions=np.empty(action.shape)\n",
    "model_results=[]\n",
    "model_r_consumption=[]\n",
    "env = GreenhouseEnv(crop_parameters_estimator, resource_consumption_estimator,gh_climate_estimator, weather_data)\n",
    "check_env(env)\n",
    "state = env.reset(seed=42)[0]\n",
    "done = False\n",
    "episode_reward = 0\n",
    "i=0\n",
    "\n",
    "while not done:\n",
    "        action = best_model.predict(state,deterministic=True)[0]\n",
    "        print(action.shape)\n",
    "        if i==0:\n",
    "          model_actions=action.reshape(2016,9)\n",
    "        else: \n",
    "          model_actions=np.concatenate([model_actions,action.reshape(2016,9)],axis=0)\n",
    "        print(model_actions.shape)\n",
    "        i+=1\n",
    "        #model_actions=np.concatenate([np.array(model_actions),action],axis=0)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        model_results.append(state['crop_params'])\n",
    "        model_r_consumption.append(state['resource_consumption'])\n",
    "        episode_reward += reward\n",
    "episode_rewards.append(episode_reward.astype('float32'))\n",
    "print(f'Episode Reward: {episode_reward.astype(\"float32\")}')\n",
    "sp=actions_sp\n",
    "sp_noday=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min'] #10 Actions \n",
    "model_actions.shape\n",
    "model_results=pd.DataFrame(np.array(model_results).reshape(23,3),columns=crop_param)[:18]\n",
    "model_r_consumption=pd.DataFrame(np.array(model_r_consumption).reshape(23,5),columns=resources)\n",
    "model_actions=pd.DataFrame(np.array(model_actions),columns=sp_noday)\n",
    "model_results.set_index(T1_Results.index,inplace=True)\n",
    "model_r_consumption.set_index(T1_resource_Consumption[:-1].index,inplace=True)\n",
    "\n",
    "model_actions=model_actions[:len(T5_Action.index)].set_index(T5_Action.index[:len(model_actions.index)])\n",
    "\n",
    "model_actions.index\n",
    "teams=['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators','Ours']\n",
    "teams_Results={ teams[0]: T1_Results,\n",
    "                 teams[1]: T2_Results,\n",
    "                 teams[2]: T3_Results,\n",
    "                 teams[3]: T4_Results,\n",
    "                 teams[4]: T5_Results,\n",
    "                 teams[5]: T6_Results,\n",
    "                 teams[6]:  model_results,\n",
    "                    \n",
    "                    }\n",
    "teams_Actions={ teams[0]: T1_Action[actions_sp],\n",
    "                teams[1]: T2_Action[actions_sp],\n",
    "                teams[2]: T3_Action[actions_sp],\n",
    "                teams[3]: T4_Action[actions_sp],\n",
    "                teams[4]: T5_Action[actions_sp],\n",
    "                teams[5]: T6_Action[actions_sp],\n",
    "                teams[6]:  model_actions[sp_noday],\n",
    "                    }\n",
    "teams_rc={      teams[0]: T1_resource_Consumption,\n",
    "                teams[1]: T2_resource_Consumption,\n",
    "                teams[2]: T3_resource_Consumption,\n",
    "                teams[3]: T4_resource_Consumption,\n",
    "                teams[4]: T5_resource_Consumption,\n",
    "                teams[5]: T6_resource_Consumption,\n",
    "                teams[6]:  model_r_consumption,\n",
    "    \n",
    "}\n",
    "#plot Teams results\n",
    "print(teams_Results[teams[0]].columns)\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for result in teams_Results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        teams_Results[teams[i]][result].plot(ylabel=result,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in teams_rc[teams[0]].columns[:-1]:\n",
    "    for i in range(len(teams)):\n",
    "        teams_rc[teams[i]][resource].plot(ylabel=resource,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "teams_episode_rewards = {}\n",
    "all_teams_results={}\n",
    "all_teams_r_consumption={}\n",
    "all_teams_episode_rewards={}\n",
    "for team in teams:\n",
    "    print(f'Evaluate {team} Actions....')\n",
    "    \n",
    "    teams_model_actions=np.empty((2016, 9))\n",
    "    Our_model_actions=np.empty((2016, 9))\n",
    "    \n",
    "    model_results=[]\n",
    "    \n",
    "    model_r_consumption=[]\n",
    "    state = env.reset(seed=42)[0]\n",
    "    done = False\n",
    "    episode_reward = []\n",
    "    i=0\n",
    "    sp=actions_sp\n",
    "    while not done:\n",
    "            if team=='Ours':\n",
    "                action = best_model.predict(state,deterministic=True)[0]\n",
    "                Our_model_actions=np.concatenate([np.array(Our_model_actions),np.array(action).reshape(2016,9)],axis=0)\n",
    "                state, reward, done, _, _ = env.step(np.array(action))\n",
    "                \n",
    "            else:\n",
    "                action = teams_Actions[team].iloc[2016*i:2016*(i+1)][sp_noday]\n",
    "                teams_model_actions=np.concatenate([np.array(teams_model_actions),np.array(action)],axis=0)\n",
    "                print(action.shape)\n",
    "                state, reward, done, _, _ = env.step(np.array(action))\n",
    "            i+=1\n",
    "            \n",
    "            \n",
    "            model_results.append(state['crop_params'])\n",
    "            model_r_consumption.append(state['resource_consumption'])\n",
    "            episode_reward.append(reward.astype('float32'))\n",
    "           \n",
    "    model_results=pd.DataFrame(np.array(model_results).reshape(23,3),columns=crop_param)[:18]\n",
    "    model_r_consumption=pd.DataFrame(np.array(model_r_consumption).reshape(23,5),columns=resources)\n",
    "    model_actions=pd.DataFrame(np.array(model_actions),columns=sp_noday)\n",
    "    model_results.set_index(T1_Results.index,inplace=True)\n",
    "    model_r_consumption.set_index(T1_resource_Consumption[:-1].index,inplace=True)\n",
    "    all_teams_results[team]=model_results\n",
    "    all_teams_r_consumption[team]=model_r_consumption\n",
    "    print(episode_reward)\n",
    "    teams_episode_rewards[team]=np.array(episode_reward)\n",
    "    print(f'{team} Episode Reward: {np.array(teams_episode_rewards[team]).sum().astype(\"float32\")}')\n",
    "    all_teams_episode_rewards[team]=np.array(teams_episode_rewards[team]).sum().astype(\"float32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Teams results\n",
    "print(all_teams_results[teams[0]].columns)\n",
    "line=['*','o','>','^','x','v','.']\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for result in all_teams_results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        all_teams_results[teams[i]][result].plot(ylabel=result,grid=True, marker=line[i], figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#plot Teams actions\n",
    "line=['*','o','>','^','x','v','.']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in all_teams_r_consumption[teams[0]].columns[:]:\n",
    "    for i in range(len(teams)):\n",
    "        #all_teams_r_consumption[teams[i]]['Irr']=all_teams_r_consumption[teams[i]]['Irr']*(.0001*x)+all_teams_r_consumption[teams[i]]['Irr']\n",
    "        all_teams_r_consumption[teams[i]][resource].plot(ylabel=resource,grid=True, marker=line[i], figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_df=pd.read_csv('./irrigation_time_series.csv')\n",
    "markers = ['*', 'o', '>', '^', 'x', 'v', '.']\n",
    "# Replotting with the correct column names\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, team in enumerate(data_df.columns[1:]):  # Skip 'Time' column\n",
    "    plt.plot(data_df['Timestamp'], data_df[team], marker=markers[i], label=team)\n",
    "\n",
    "plt.title(\"Greenhouse Irrigation Time Series Plot for All Teams\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Irrigation (Irr)\")\n",
    "plt.legend(title=\"Teams\", loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teams_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teams_r_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for result in teams_Results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        teams_Results[teams[i]][result].plot(ylabel=result,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in teams_rc[teams[0]].columns[:-1]:\n",
    "    for i in range(len(teams)):\n",
    "        teams_rc[teams[i]][resource].plot(ylabel=resource,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams_episode_rewards['Ours'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards=[]\n",
    "for team_reward in teams_episode_rewards:\n",
    "    print(team_reward)\n",
    "    print(len(teams_episode_rewards[team_reward]))\n",
    "    print(teams_episode_rewards[team_reward].sum())\n",
    "    rewards.append(teams_episode_rewards[team_reward].sum())\n",
    "teams_episode_rewards_df=pd.DataFrame(np.array(rewards).reshape(1,7),columns=teams)\n",
    "print(teams_episode_rewards_df)\n",
    "final_result=pd.DataFrame(teams_episode_rewards_df.sum(axis=0).sort_values(ascending=False))\n",
    "final_result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(final_result.index, final_result[0])\n",
    "plt.xticks(rotation=90)  # Rotate the x-axis labels by 90 degrees\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(final_result.index, final_result[0])\n",
    "plt.xticks(rotation=90)  # Rotate the x-axis labels by 90 degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mean_rc={}\n",
    "\n",
    "for i in range(len(teams)):\n",
    "        mean_rc[teams[i]]=all_teams_r_consumption[teams[i]].sum(axis=0)[['CO2_cons','ElecHigh','ElecLow','Heat_cons','Irr']].round(2)\n",
    "\n",
    "mean_rc_df=pd.DataFrame.from_dict(mean_rc)\n",
    "mean_rc_df.to_csv('sum_rc_df_data_TD3.csv', index=True)  # Set index=True to include the index column\n",
    "mean_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mean_cp={}\n",
    "\n",
    "for i in range(len(teams)):\n",
    "        mean_cp[teams[i]]=all_teams_results[teams[i]].mean(axis=0).round(2)\n",
    "\n",
    "mean_cp\n",
    "mean_cp_df=pd.DataFrame.from_dict(mean_cp)\n",
    "mean_cp_df.to_csv('mean_cp_df_data_TD3.csv', index=True)  # Set index=True to include the index column\n",
    "mean_cp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign_language_ai5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
