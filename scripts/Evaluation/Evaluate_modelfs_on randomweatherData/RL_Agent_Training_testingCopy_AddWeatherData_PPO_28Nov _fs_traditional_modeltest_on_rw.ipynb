{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from stable_baselines3 import PPO , DDPG, SAC #,RecurrentPPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "# Print the name of each available GPU\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Load the pre-trained models\n",
    "crop_parameters_estimator = load_model('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/wcp_LSTM_model_fs_model.h5')\n",
    "resource_consumption_estimator = load_model('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/rc_LSTM_model_fs_model.h5')\n",
    "gh_climate_estimator = load_model('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/ghc_mlp_model_fs_model.h5')\n",
    "\n",
    "rewards = []\n",
    "CP_important_feature=[ 'Tair', 'pH_drain_PC', 'Cum_irr', 't_heat_vip', 'water_sup', 'Tot_PAR', 'water_sup_intervals_vip_min', 'PipeGrow', 'EC_drain_PC',\n",
    "                        'BlackScr', 'co2_dos', 'Tot_PAR_Lamps', 'scr_enrg_vip', 'Rhair', 'HumDef', 'days']\n",
    "\n",
    "GH_C_important_feature=['PARout',  'Tout',  'Iglob',  'RadSum',  'scr_enrg_vip',  't_heat_vip',  'int_white_vip',  'scr_blck_vip',  'pH_drain_PC',  'co2_vip',\n",
    "                        't_ventlee_vip',  'days']\n",
    "\n",
    "RC_important_feature=[     'Cum_irr',  'BlackScr',  'water_sup_intervals_vip_min',  'EC_drain_PC',  'pH_drain_PC',  'CO2air',  'water_sup',  'HumDef',\n",
    "                            'Rhair',  'days',  'Tot_PAR' ]\n",
    "\n",
    "actions_sp=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min',  'days'] #10 Actions \n",
    "\n",
    "important_ghc=['BlackScr','CO2air','Cum_irr','EC_drain_PC','PipeGrow','HumDef','Rhair','Tair','Tot_PAR','Tot_PAR_Lamps']\n",
    "\n",
    "weather_sp=[ 'PARout',   'Tout', 'Iglob',  'RadSum']\n",
    "\n",
    "GH_C_Out_columns=['AssimLight','BlackScr','CO2air','Cum_irr','EC_drain_PC','EnScr','HumDef','PipeGrow','PipeLow','Rhair','Tair','Tot_PAR',\n",
    "                  'Tot_PAR_Lamps','VentLee','Ventwind' ,'assim_vip','co2_dos' ]\n",
    "class GreenhouseEnv(gym.Env):\n",
    "    def __init__(self, crop_parameters_estimator, resource_consumption_estimator, gh_climate_estimator,weather_data):\n",
    "        super(GreenhouseEnv, self).__init__()\n",
    "        self.crop_parameters_estimator = crop_parameters_estimator\n",
    "        self.resource_consumption_estimator = resource_consumption_estimator\n",
    "        self.gh_climate_estimator=gh_climate_estimator\n",
    "        # Action space: shape (2016, 34) - 2016 time steps for 34 control setpoints\n",
    "        #self.action_space = spaces.Box(low=0, high=1, shape=(2016, 9),dtype=np.float64)\n",
    "        self.action_space = spaces.Box(low=0,high=1,shape=(2016 * 9,),dtype=np.float64) #Flattened shape: 2016*34\n",
    "    \n",
    "\n",
    "        # Observation space: weather (2016, 10), crop parameters (1, 3), resource consumption (1, 5)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'weather': spaces.Box(low=0, high=1, shape=(2016, 10), dtype=np.float64),\n",
    "            'crop_params': spaces.Box(low=0, high=1, shape=(1, 3), dtype=np.float64),\n",
    "            'resource_consumption': spaces.Box(low=0, high=7, shape=(1, 5), dtype=np.float64),\n",
    "            'gh_climate': spaces.Box(low=-10, high=10, shape=(2016, 10), dtype=np.float64)\n",
    "        })\n",
    "        \n",
    "        # Initial state\n",
    "        self.weather_data = weather_data\n",
    "        self.current_step = 0\n",
    "        self.steps = 0\n",
    "        self.max_steps = 23\n",
    "        self.days=np.array([(i // 288 ) / 166  for i in range(2016*(self.max_steps))]).reshape(2016*(self.max_steps), 1)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.steps = 0\n",
    "        self.days=np.array([(i // 288 ) / 166  for i in range(2016*(self.max_steps))]).reshape(2016*(self.max_steps), 1)\n",
    "        # Random initial control setpoints\n",
    "        #control_setpoints = np.random.uniform(0, 1, size=(1,2016, 17)).reshape(2016, 17)\n",
    "        #day = np.array([(i // 288 + 1) / 166 * (self.steps + 1) for i in range(2016)]).reshape(2016, 1)\n",
    "        #initial_control_setpoints = np.concatenate([control_setpoints, day], axis=1).reshape(1, 2016, 18)\n",
    "        \n",
    "        initial_crop_params = np.zeros(3).reshape(1, 3)+np.random.uniform(0, .1, size=(1,3))\n",
    "        initial_resource_consumption = np.zeros(5).reshape(1, 5)+np.random.uniform(0, .1, size=(1,5))\n",
    "        initial_daily_resource_consumption = np.zeros(shape=(7,1,5)).reshape(7,1,5)+np.random.uniform(0, .1, size=(7,1,5))\n",
    "        initial_gh_climate=np.zeros(shape=(1,2016,10)).reshape(2016,10)+np.random.uniform(0, .1, size=(2016,10))\n",
    "        # Return the initial state\n",
    "        self.state = {\n",
    "            'weather': np.array(self.weather_data[self.steps * 2016:(self.steps + 1) * 2016]),\n",
    "            'crop_params': initial_crop_params,\n",
    "            'resource_consumption': initial_resource_consumption,\n",
    "            'gh_climate': initial_gh_climate\n",
    "        }\n",
    "        print(self.state['weather'].shape)\n",
    "        self.daily_res_cons=initial_daily_resource_consumption\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        action = action.reshape((2016, 9))\n",
    "        day = np.array(self.days[self.steps * 2016:(self.steps + 1) * 2016]).reshape(2016, 1) #np.array([(i // 288 + 1) / 166 * (self.steps + 1) for i in range(2016)]).reshape(2016, 1)\n",
    "        self.steps += 1\n",
    "        print('day',day)\n",
    "        control_setpoints = np.concatenate([action.reshape(2016, 9), day], axis=1)\n",
    "        # current_crop_par=self.state['crop_params'].reshape(1,1,3)\n",
    "        # print(current_crop_par.shape)\n",
    "        # current_res_cons=self.daily_res_cons\n",
    "        \n",
    "        \n",
    "        control_setpoints = pd.DataFrame(control_setpoints, columns=actions_sp)\n",
    "        print(type(self.state['weather']))\n",
    "        print(type(control_setpoints))\n",
    "        weather=pd.DataFrame(self.state['weather'],columns=w_columns)\n",
    "        #predict gh_clmate:\n",
    "        w_sp_data = pd.concat([weather, control_setpoints], axis=1)\n",
    "        GH_C_Estimator_Input=np.array(w_sp_data[GH_C_important_feature])\n",
    "\n",
    "        print('w_sp_data',w_sp_data.shape)\n",
    "        ghclimate=self.gh_climate_estimator.predict(GH_C_Estimator_Input).reshape(2016,17)\n",
    "        gh_climate=pd.DataFrame(ghclimate,columns=GH_C_Out_columns)\n",
    "        \n",
    "        # Predict crop parameters using the crop LSTM model\n",
    "        #gh_sp_data = np.concatenate([gh_climate, control_setpoints], axis=1).reshape(1,2016,35)\n",
    "        CP_Estimator_Input = np.array(pd.concat([gh_climate, control_setpoints], axis=1)[CP_important_feature]).reshape(1,2016,16)\n",
    "        weekly_crop_params = self.crop_parameters_estimator.predict(CP_Estimator_Input).reshape(1,3)\n",
    "\n",
    "        RC_Estimator_Input = np.array(pd.concat([gh_climate, control_setpoints], axis=1)[RC_important_feature]).reshape(7, 288, 11)\n",
    "        # Average daily control setpoints for resource consumption\n",
    "        #daily_actions = gh_sp_data.reshape(7, 288, 35)\n",
    "        #actions = np.array([a[i].mean(axis=0) for i in range(7)])\n",
    "        \n",
    "        daily_resource_consumption = self.resource_consumption_estimator.predict(RC_Estimator_Input)\n",
    "        print(RC_Estimator_Input.shape) #(7, 288, 11)\n",
    "\n",
    "        print(daily_resource_consumption.shape) #(7, 288, 5)\n",
    "        self.daily_res_cons=daily_resource_consumption\n",
    "        weekly_resource_consumption = self.resource_consumption_estimator.predict(RC_Estimator_Input).sum(axis=0).reshape(1, 5)\n",
    "        print(weekly_resource_consumption.shape)\n",
    "        #print(self.resource_consumption_estimator.predict(actions)[:])\n",
    "        for i in self.resource_consumption_estimator.predict(RC_Estimator_Input)[:]:\n",
    "            #print(i)\n",
    "            high_rc = np.any(i>=1)\n",
    "            #print(high_rc)\n",
    "            if high_rc:\n",
    "                #print('break')\n",
    "                break\n",
    "        # Update state\n",
    "        self.state = {\n",
    "            'weather': self.weather_data[self.steps * 2016:(self.steps + 1) * 2016],\n",
    "            'crop_params': weekly_crop_params,\n",
    "            'resource_consumption': weekly_resource_consumption,\n",
    "            'gh_climate': np.array(gh_climate[important_ghc]).reshape(2016,10)\n",
    "        }\n",
    "        #print(np.max(action),np.min(action),np.any(action>1),np.any(action<0))\n",
    "        # Calculate reward\n",
    "        print(weekly_crop_params[0].shape)\n",
    "        print(weekly_resource_consumption[0].shape)\n",
    "        print(weekly_resource_consumption)\n",
    "\n",
    "        reward = self.calculate_reward(weekly_crop_params[0], weekly_resource_consumption[0],high_rc,action)\n",
    "        #print(np.any(weekly_crop_params[0]>=1 ) , np.any(daily_resource_consumption[0]>=7),high_rc)\n",
    "        done = bool((self.steps >= self.max_steps) ) #or np.any(weekly_crop_params[0]<0.2 ) or np.any(weekly_resource_consumption[0]>7) or np.all(weekly_crop_params[0]<0.5) ) #or high_rc)\n",
    "        #print('done',done)\n",
    "        #rewards.append(reward)\n",
    "        \n",
    "        return self.state, reward, done, done, {}\n",
    "    def calculate_reward(self, crop_params, resource_consumption,high_rc,current_actions):\n",
    "        punishment=0\n",
    "        big_reward=0\n",
    "        alpha, beta, delta,gamma = 1, .5, 0.02,0.002 #1, 0.5, 0.1\n",
    "        w1, w2, w3 = 1, 1, 1\n",
    "        p1, p2, p3, p4 = 1, 1, 1, 1\n",
    "        #w1, w2, w3 = 1, 1, 1\n",
    "        #p1, p2, p3, p4 = 1, 1, 1, 1\n",
    "        max_stem_elong, max_stem_thick, max_cum_trusses = 1.0, 1.0, 1.0\n",
    "        max_heat, max_co2, max_electricity, max_irrigation = 7.0, 7.0, 7.0, 7.0\n",
    "        action_variation_penalty = 1/np.std(current_actions, axis=1).mean()  # Penalize low standard deviation\n",
    "\n",
    "        crop_reward = (w1 * (crop_params[0] / max_stem_elong) +\n",
    "                       w2 * (crop_params[1] / max_stem_thick) +\n",
    "                       w3 * (crop_params[2] / max_cum_trusses))\n",
    "        resource_penalty = (p1 * (resource_consumption[0] / max_heat) +\n",
    "                            p2 * (resource_consumption[1] / max_co2) +\n",
    "                            p3 * ((resource_consumption[2] + resource_consumption[3]) / max_electricity) +\n",
    "                            p4 * (resource_consumption[4] / max_irrigation))\n",
    "        #if high_rc:\n",
    "        #    punishment=-.1\n",
    "        #if np.any(current_actions>1 ) or np.any(current_actions<0):\n",
    "        #    punishment-=.9\n",
    "        if np.any(resource_consumption>7):\n",
    "            print('resource_consumption>7',np.any(resource_consumption>7))\n",
    "            punishment-=1\n",
    "        if np.any(crop_params<.5):\n",
    "            print('crop_params<.5',np.any(crop_params<.5))\n",
    "            punishment-=0.8\n",
    "        \n",
    "        if np.all(crop_params<.5):\n",
    "            print('all crop_params<.5',np.all(crop_params<.5))\n",
    "            punishment-=1\n",
    "        if np.any(crop_params>=.7):\n",
    "            print('crop_params>=.7',np.any(crop_params>=.7))\n",
    "            big_reward+=.5\n",
    "        if np.any(crop_params>=.8):\n",
    "            print('crop_params>=.8',np.any(crop_params>=.8))\n",
    "            big_reward+=.8\n",
    "        if np.all(crop_params>=.5):\n",
    "            print('all crop_params>=.5',np.all(crop_params>=.5))\n",
    "            big_reward+=.8\n",
    "        if np.all(crop_params>=.7):\n",
    "            print('all crop_params>=.7',np.all(crop_params>=.7))\n",
    "            big_reward+=.8\n",
    "        if np.all(crop_params>=.8):\n",
    "            print('all crop_params>=.8',np.all(crop_params>=.8))\n",
    "            big_reward+=1\n",
    "        efficiency_factor = crop_reward / (1 + resource_penalty)\n",
    "        print(efficiency_factor,'efficiency_factor')\n",
    "        #Stability penalty\n",
    "        #s = 0.01\n",
    "        #max_delta_action = 16.0  # Assuming actions are normalized between 0 and 1\n",
    "        # stability=0\n",
    "        # #print(len(current_actions)-1)\n",
    "        # for idx in range(len(current_actions)-1):\n",
    "        #     stability+=np.sum(np.abs(current_actions[idx]-current_actions[idx+1]))/ max_delta_action\n",
    "        #     #print('Action Difference',stability)\n",
    "        # stability_penalty = s * stability\n",
    "        # print(stability_penalty)\n",
    "        reward = alpha * crop_reward - beta * resource_penalty + delta * (crop_reward / (1 + resource_penalty)) + punishment + big_reward - gamma * action_variation_penalty\n",
    "\n",
    "        # reward = alpha * crop_reward - beta * resource_penalty +punishment+big_reward + delta * efficiency_factor - gamma * stability_penalty\n",
    "        print(reward)\n",
    "        #reward = alpha * crop_reward - beta * resource_penalty + delta * efficiency_factor+punishment+big_reward\n",
    "        #print(reward)\n",
    "        return reward\n",
    "    \n",
    "    # def calculate_reward(self, crop_params, resource_consumption,high_rc,current_actions):\n",
    "    #     punishment=0\n",
    "    #     big_reward=0\n",
    "    #     alpha, beta, delta,gamma = 1, 0.2, 0.1,0.01 #1, 0.5, 0.1\n",
    "    #     w1, w2, w3 = 0.40, 0.30, 0.30\n",
    "    #     p1, p2, p3, p4 = 0.2, 0.3, 0.2, 0.3\n",
    "    #     #w1, w2, w3 = 1, 1, 1\n",
    "    #     #p1, p2, p3, p4 = 1, 1, 1, 1\n",
    "    #     max_stem_elong, max_stem_thick, max_cum_trusses = 1.0, 1.0, 1.0\n",
    "    #     max_heat, max_co2, max_electricity, max_irrigation = 7.0, 7.0, 7.0, 7.0\n",
    "\n",
    "    #     crop_reward = (w1 * (crop_params[0] / max_stem_elong) +\n",
    "    #                    w2 * (crop_params[1] / max_stem_thick) +\n",
    "    #                    w3 * (crop_params[2] / max_cum_trusses))\n",
    "    #     resource_penalty = (p1 * (resource_consumption[0] / max_heat) +\n",
    "    #                         p2 * (resource_consumption[1] / max_co2) +\n",
    "    #                         p3 * ((resource_consumption[2] + resource_consumption[3]) / max_electricity) +\n",
    "    #                         p4 * (resource_consumption[4] / max_irrigation))\n",
    "    #     #if high_rc:\n",
    "    #     #    punishment=-.1\n",
    "    #     #if np.any(current_actions>1 ) or np.any(current_actions<0):\n",
    "    #     #    punishment-=.9\n",
    "    #     if np.any(resource_consumption>7):\n",
    "    #         print('resource_consumption>7',np.any(resource_consumption>7))\n",
    "    #         punishment-=1\n",
    "    #     if np.any(resource_consumption>5):\n",
    "    #         print('resource_consumption>5',np.any(resource_consumption>5))\n",
    "    #         punishment-=.1\n",
    "    #     if np.any(crop_params<.5):\n",
    "    #         print('crop_params<.5',np.any(crop_params<.5))\n",
    "    #         punishment-=0.8\n",
    "        \n",
    "    #     if np.all(crop_params<.5):\n",
    "    #         print('all crop_params<.5',np.all(crop_params<.5))\n",
    "    #         punishment-=1\n",
    "    #     if np.any(crop_params>=.7):\n",
    "    #         print('crop_params>=.7',np.any(crop_params>=.7))\n",
    "    #         big_reward+=.2\n",
    "    #     if np.any(crop_params>=.8):\n",
    "    #         print('crop_params>=.8',np.any(crop_params>=.8))\n",
    "    #         big_reward+=.2\n",
    "    #     if np.all(crop_params>=.5):\n",
    "    #         print('all crop_params>=.5',np.all(crop_params>=.5))\n",
    "    #         big_reward+=.2\n",
    "    #     if np.all(crop_params>=.7):\n",
    "    #         print('all crop_params>=.7',np.all(crop_params>=.7))\n",
    "    #         big_reward+=.2\n",
    "    #     if np.all(crop_params>=.8):\n",
    "    #         print('all crop_params>=.8',np.all(crop_params>=.8))\n",
    "    #         big_reward+=.2\n",
    "    #     # efficiency_factor = crop_reward / (1 + resource_penalty)\n",
    "    #     # print(efficiency_factor,'efficiency_factor')\n",
    "    #     # Stability penalty\n",
    "    #     # s = 0.01\n",
    "    #     # max_delta_action = 16.0  # Assuming actions are normalized between 0 and 1\n",
    "    #     # stability=0\n",
    "    #     # #print(len(current_actions)-1)\n",
    "    #     # for idx in range(len(current_actions)-1):\n",
    "    #     #     stability+=np.sum(np.abs(current_actions[idx]-current_actions[idx+1]))/ max_delta_action\n",
    "    #     #     #print('Action Difference',stability)\n",
    "    #     # stability_penalty = s * stability\n",
    "    #     #print(stability_penalty)\n",
    "    #     reward = alpha * crop_reward - beta * resource_penalty +punishment+big_reward #+ delta * efficiency_factor #- gamma * stability_penalty\n",
    "    #     print(reward)\n",
    "    #     #reward = alpha * crop_reward - beta * resource_penalty + delta * efficiency_factor+punishment+big_reward\n",
    "    #     #print(reward)\n",
    "    #     return reward\n",
    "\n",
    "# Assuming weather_data is a preprocessed time series array for the environment\n",
    "weather_data = pd.read_csv('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/weather_fill_missing_values.csv',index_col='%time')#pd.read_csv('D:\\\\Iman\\\\AGHC\\\\CherryTomato\\\\Data\\\\Weather\\\\Weather.csv',index_col='%time') #  # Example placeholder data #np.random.rand(48384, 10)  # Placeholder data\n",
    "w_columns=weather_data.columns\n",
    "print(weather_data.shape)\n",
    "#print(weather_data.isnull().sum())\n",
    "scaler=MinMaxScaler()\n",
    "weather_data=scaler.fit_transform(weather_data)\n",
    "#weather_data=pd.DataFrame(weather_data,columns=w_columns)\n",
    "\n",
    "#print(weather_data[:10])\n",
    "print(weather_data.shape)\n",
    "weather_data = np.random.rand(48384, 10)\n",
    "\n",
    "# Initialize the environment\n",
    "env = GreenhouseEnv(crop_parameters_estimator, resource_consumption_estimator,gh_climate_estimator, weather_data)\n",
    "check_env(env)\n",
    "\n",
    "import os\n",
    "# Define a folder for saving logs_fs and models\n",
    "log_dir = \"./logs_fs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Define the policy and agent\n",
    "#model = PPO(\"MultiInputPolicy\", env, verbose=0, learning_rate=1e-5,tensorboard_log=log_dir) #MultiInputLstmPolicy #MultiInputPolicy\n",
    "# Get the number of actions in the environment (for noise generation)\n",
    "\n",
    "# Wrap the environment in a monitor to track performance\n",
    "env = Monitor(env, log_dir)\n",
    "# Reduced learning rate for more stable learning, increased exploration noise\n",
    "# Define the policy and agent\n",
    "#model = PPO(\"MultiInputPolicy\", env, verbose=0, learning_rate=1e-5,tensorboard_log=log_dir) #MultiInputLstmPolicy #MultiInputPolicy\n",
    "# Assuming `model` is a PyTorch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.torch_layers import MlpExtractor\n",
    "\n",
    "# # Custom policy network for PPO\n",
    "# class CustomPolicy(nn.Module):\n",
    "#     def __init__(self, obs_space, action_space):\n",
    "#         super(CustomPolicy, self).__init__()\n",
    "#         self.fc1 = nn.Linear(obs_space['weather'].shape[1], 512)\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.action_head = nn.Linear(256, action_space.shape[1])  # Output for each feature\n",
    "\n",
    "#     def forward(self, obs):\n",
    "#         x = th.relu(self.fc1(obs['weather']))\n",
    "#         x = th.relu(self.fc2(x))\n",
    "#         actions = th.tanh(self.action_head(x))  # Each feature has unique action values\n",
    "#         return actions\n",
    "\n",
    "# # Use the custom policy in the PPO model\n",
    "# policy_kwargs = {\n",
    "#     'features_extractor_class': CustomPolicy,\n",
    "#     'net_arch': [512, 256]\n",
    "# }\n",
    "# #model = PPO(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=1, learning_rate=1e-4)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(device)\n",
    "ppo_model = PPO(\n",
    "    \"MultiInputPolicy\", \n",
    "    env, \n",
    "    #policy_kwargs=policy_kwargs,\n",
    "    verbose=1, \n",
    "    learning_rate=1e-5,\n",
    "    n_steps=2048,  # Larger number of steps before each policy update\n",
    "    batch_size=64,  # Smaller batches for more frequent updates\n",
    "    n_epochs=20,  # More epochs for stable learning\n",
    "    gamma=0.99,  # Discount factor\n",
    "    clip_range=0.2,  # PPO clipping range\n",
    "    ent_coef=0.09,  # Entropy coefficient for exploration\n",
    "    gae_lambda=0.95,  # GAE lambda for advantage estimation\n",
    "    vf_coef=0.5,  # Value function coefficient\n",
    "    max_grad_norm=0.5,  # Gradient clipping\n",
    "    tensorboard_log=log_dir,\n",
    "    #tensorboard_log=\"./ppo_greenhouse_tensorboard/\",  # Optional: Tensorboard log directory\n",
    "    policy_kwargs=dict(net_arch=[256, 256,128]) , # Smaller policy network for faster learning\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# class CustomPolicy(ActorCriticPolicy):\n",
    "#     def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "#         super(CustomPolicy, self).__init__(observation_space, action_space, lr_schedule, **kwargs)\n",
    "\n",
    "#         # Here we define the MlpExtractor for the policy and value features.\n",
    "#         # Adjust net_arch according to your needs.\n",
    "#         self.mlp_extractor = MlpExtractor(\n",
    "#             self.features_dim,   # features_dim is computed by parent class\n",
    "#             net_arch=[256, 256, 128],\n",
    "#             activation_fn=nn.ReLU\n",
    "#         )\n",
    "\n",
    "#         # The MlpExtractor returns separate latent vectors for policy and value\n",
    "#         latent_dim_pi = 128\n",
    "#         latent_dim_vf = 128\n",
    "\n",
    "#         self.action_net = nn.Linear(latent_dim_pi, action_space.shape[0])\n",
    "#         self.value_net = nn.Linear(latent_dim_vf, 1)\n",
    "\n",
    "#         # Weight initialization\n",
    "#         nn.init.orthogonal_(self.action_net.weight, gain=0.01)\n",
    "#         nn.init.constant_(self.action_net.bias, 0)\n",
    "#         nn.init.orthogonal_(self.value_net.weight, gain=1)\n",
    "#         nn.init.constant_(self.value_net.bias, 0)\n",
    "\n",
    "#         # Register the layers so the policy knows what parameters to optimize\n",
    "#         self._build(lr_schedule)\n",
    "\n",
    "#     def _build(self, lr_schedule):\n",
    "#         # Setup optimizer\n",
    "#         self.optimizer = th.optim.Adam(self.parameters(), lr=lr_schedule(1))\n",
    "\n",
    "#     def forward(self, obs):\n",
    "#         # Called by the parent to get the distribution and value estimates\n",
    "#         features = self.extract_features(obs)\n",
    "#         latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "#         action_logits = self.action_net(latent_pi)\n",
    "#         value = self.value_net(latent_vf)\n",
    "#         return action_logits, value\n",
    "    \n",
    "    \n",
    "# ppo_model = PPO(\n",
    "#     policy=CustomPolicy,\n",
    "#     env=env,\n",
    "#     verbose=1, \n",
    "#     learning_rate=1e-5,\n",
    "#     n_steps=2048,  # Larger number of steps before each policy update\n",
    "#     batch_size=64,  # Smaller batches for more frequent updates\n",
    "#     n_epochs=20,  # More epochs for stable learning\n",
    "#     gamma=0.99,  # Discount factor\n",
    "#     clip_range=0.2,  # PPO clipping range\n",
    "#     ent_coef=0.05,  # Entropy coefficient for exploration\n",
    "#     gae_lambda=0.95,  # GAE lambda for advantage estimation\n",
    "#     vf_coef=0.5,  # Value function coefficient\n",
    "#     max_grad_norm=0.5,  # Gradient clipping\n",
    "#     tensorboard_log=log_dir,\n",
    "#     device=device\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "# from stable_baselines3.common.torch_layers import MlpExtractor\n",
    "# import torch as th\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# print(device)\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "# from stable_baselines3.common.torch_layers import MlpExtractor\n",
    "# import torch as th\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "# class CustomPolicy(ActorCriticPolicy):\n",
    "#     def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "#         super(CustomPolicy, self).__init__(observation_space, action_space, lr_schedule, **kwargs)\n",
    "#         # Do not define mlp_extractor here, it will be defined in _build_mlp_extractor\n",
    "\n",
    "#     def _build_mlp_extractor(self):\n",
    "#         # This method is called by the parent class after features_dim is known\n",
    "#         mlp_extractor = MlpExtractor(\n",
    "#             self.features_dim,\n",
    "#             net_arch=[256, 256, 128],\n",
    "#             activation_fn=nn.ReLU\n",
    "#         )\n",
    "\n",
    "#         # Set the mlp_extractor as an attribute of self\n",
    "#         self.mlp_extractor = mlp_extractor\n",
    "\n",
    "#         # Define the actor and value networks after the MlpExtractor\n",
    "#         latent_dim_pi = 128\n",
    "#         latent_dim_vf = 128\n",
    "\n",
    "#         self.action_net = nn.Linear(latent_dim_pi, self.action_space.shape[0])\n",
    "#         self.value_net = nn.Linear(latent_dim_vf, 1)\n",
    "\n",
    "#         nn.init.orthogonal_(self.action_net.weight, gain=0.01)\n",
    "#         nn.init.constant_(self.action_net.bias, 0)\n",
    "#         nn.init.orthogonal_(self.value_net.weight, gain=1)\n",
    "#         nn.init.constant_(self.value_net.bias, 0)\n",
    "\n",
    "#         return self.mlp_extractor\n",
    "\n",
    "# # Make sure that env and log_dir are defined\n",
    "\n",
    "\n",
    "# # Ensure that you have defined `env` and `log_dir` before this point.\n",
    "# # For example:\n",
    "# # env = ... # your environment\n",
    "# # log_dir = \"./logs/\"\n",
    "# # os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# ppo_model = PPO(\n",
    "#     policy=CustomPolicy,\n",
    "#     env=env,\n",
    "#     verbose=1, \n",
    "#     learning_rate=1e-5,\n",
    "#     n_steps=2048,\n",
    "#     batch_size=64,\n",
    "#     n_epochs=20,\n",
    "#     gamma=0.99,\n",
    "#     clip_range=0.2,\n",
    "#     ent_coef=0.05,\n",
    "#     gae_lambda=0.95,\n",
    "#     vf_coef=0.5,\n",
    "#     max_grad_norm=0.5,\n",
    "#     tensorboard_log=log_dir,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create evaluation callback to monitor agent performance every 100k steps\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs_fs/best_model/',\n",
    "                             log_path='./logs_fs/results/', eval_freq=10_000, deterministic=True, render=False)\n",
    "\n",
    "# Save models every 100k steps\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10_000, save_path='./logs_fs/checkpoints/', name_prefix='ppo_greenhouse')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "ppo_model.learn(total_timesteps=120_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the trained model\n",
    "ppo_model.save(\"ppo_greenhouse_final_model\")\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model, env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create evaluation callback to monitor agent performance every 100k steps\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs_fs/best_model/',\n",
    "                             log_path='./logs_fs/results/', eval_freq=10000, deterministic=True, render=False)\n",
    "\n",
    "# Save models every 100k steps\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./logs_fs/checkpoints/', name_prefix='ppo_greenhouse')\n",
    "\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Define the path to the checkpoint directory\n",
    "checkpoint_dir = './logs_fs/checkpoints/'\n",
    "# checkpoint_prefix = 'ppo_greenhouse'\n",
    "\n",
    "# # Find the latest checkpoint file\n",
    "# checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(checkpoint_prefix) and f.endswith('.zip')]\n",
    "\n",
    "# # Check for valid checkpoints and extract steps\n",
    "# if not checkpoints:\n",
    "#     raise FileNotFoundError(\"No checkpoint files found.\")\n",
    "\n",
    "# checkpoint_steps = []\n",
    "# for f in checkpoints:\n",
    "#     # Split the filename by underscores and find the step part\n",
    "#     parts = f.split('_')\n",
    "#     if len(parts) >= 3:  # Ensure there are enough parts to avoid IndexError\n",
    "#         step_part = parts[-2]  # Get the part before 'steps'\n",
    "#         if step_part.isdigit():  # Check if it's a digit\n",
    "#             checkpoint_steps.append(int(step_part))\n",
    "#         else:\n",
    "#             print(f\"Warning: '{step_part}' is not a valid step number. Skipping this file.\")\n",
    "#     else:\n",
    "#         print(f\"Warning: File '{f}' does not match expected format. Skipping.\")\n",
    "\n",
    "# # Ensure we have valid steps to process\n",
    "# if not checkpoint_steps:\n",
    "#     raise ValueError(\"No valid checkpoint steps found.\")\n",
    "\n",
    "# # Find the latest checkpoint based on the step number\n",
    "# latest_checkpoint_index = checkpoint_steps.index(max(checkpoint_steps))\n",
    "\n",
    "# latest_checkpoint = checkpoints[latest_checkpoint_index]\n",
    "latest_checkpoint_path = os.path.join(checkpoint_dir, 'ppo_greenhouse_80000_steps.zip')\n",
    "\n",
    "# Load the model from the latest checkpoint\n",
    "print(f\"Loading model from {latest_checkpoint_path}\")\n",
    "ppo_model = PPO.load(latest_checkpoint_path, env=env)\n",
    "\n",
    "# Continue training\n",
    "ppo_model.learn(total_timesteps=300_000, callback=[eval_callback, checkpoint_callback])  # Adjust timesteps as needed\n",
    "\n",
    "# Optionally save the model again after continuing training\n",
    "ppo_model.save(\"ppo_greenhouse_final_model_after_resume\")\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model, env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from stable_baselines3 import PPO , DDPG, SAC #,RecurrentPPO\n",
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "# import matplotlib.pyplot as plt\n",
    "# import joblib\n",
    "# from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "# import torch\n",
    "\n",
    "# # Check if a GPU is available\n",
    "# print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "# # Get the number of available GPUs\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "# print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "# # Print the name of each available GPU\n",
    "# for i in range(num_gpus):\n",
    "#     print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# # Load the pre-trained models\n",
    "# crop_parameters_estimator = load_model('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/wcp_LSTM_model_model.h5')\n",
    "# resource_consumption_estimator = load_model('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/rc_LSTM_model_model.h5')\n",
    "\n",
    "# rewards = []\n",
    "\n",
    "# class GreenhouseEnv(gym.Env):\n",
    "#     def __init__(self, crop_parameters_estimator, resource_consumption_estimator, weather_data):\n",
    "#         super(GreenhouseEnv, self).__init__()\n",
    "#         self.crop_parameters_estimator = crop_parameters_estimator\n",
    "#         self.resource_consumption_estimator = resource_consumption_estimator\n",
    "        \n",
    "#         # Action space: shape (2016, 34) - 2016 time steps for 34 control setpoints\n",
    "#         self.action_space = spaces.Box(low=0, high=1, shape=(2016, 34), dtype=np.float64)\n",
    "        \n",
    "#         # Observation space: weather (2016, 10), crop parameters (1, 3), resource consumption (1, 5)\n",
    "#         self.observation_space = spaces.Dict({\n",
    "#             'weather': spaces.Box(low=0, high=1, shape=(2016, 10), dtype=np.float64),\n",
    "#             'crop_params': spaces.Box(low=0, high=1, shape=(1, 3), dtype=np.float64),\n",
    "#             'resource_consumption': spaces.Box(low=0, high=7, shape=(1, 5), dtype=np.float64)\n",
    "#         })\n",
    "        \n",
    "#         # Initial state\n",
    "#         self.weather_data = weather_data\n",
    "#         self.current_step = 0\n",
    "#         self.steps = 0\n",
    "#         self.max_steps = 23\n",
    "\n",
    "#     def reset(self, seed=None):\n",
    "#         if seed is not None:\n",
    "#             np.random.seed(seed)\n",
    "#         self.steps = 0\n",
    "        \n",
    "#         # Random initial control setpoints\n",
    "#         control_setpoints = np.random.uniform(0, 1, size=(2016, 34)).reshape(2016, 34)\n",
    "#         day = np.array([(i // 288 + 1) / 166 * (self.steps + 1) for i in range(2016)]).reshape(2016, 1)\n",
    "#         initial_control_setpoints = np.concatenate([control_setpoints, day], axis=1).reshape(1, 2016, 35)\n",
    "        \n",
    "#         initial_crop_params = np.zeros(3).reshape(1, 3)+0.1\n",
    "#         initial_resource_consumption = np.zeros(5).reshape(1, 5)+0.1\n",
    "#         initial_daily_resource_consumption = np.zeros(shape=(7,1,5)).reshape(7,1,5)+0.1\n",
    "#         # Return the initial state\n",
    "#         self.state = {\n",
    "#             'weather': self.weather_data[self.steps * 2016:(self.steps + 1) * 2016],\n",
    "#             'crop_params': initial_crop_params,\n",
    "#             'resource_consumption': initial_resource_consumption\n",
    "#         }\n",
    "#         print(self.state['weather'].shape)\n",
    "#         self.daily_res_cons=initial_daily_resource_consumption\n",
    "\n",
    "#         return self.state, {}\n",
    "\n",
    "#     def step(self, action):\n",
    "#         self.steps += 1\n",
    "#         day = np.array([(i // 288 + 1) / 166 * (self.steps + 1) for i in range(2016)]).reshape(2016, 1)\n",
    "#         control_setpoints = np.concatenate([action.reshape(2016, 34), day], axis=1).reshape(1, 2016, 35)\n",
    "#         current_crop_par=self.state['crop_params'].reshape(1,1,3)\n",
    "#         print(current_crop_par.shape)\n",
    "#         current_res_cons=self.daily_res_cons\n",
    "#         # Predict crop parameters using the crop LSTM model\n",
    "#         weekly_crop_params = self.crop_parameters_estimator.predict(control_setpoints).reshape(1,3)\n",
    "        \n",
    "#         # Average daily control setpoints for resource consumption\n",
    "#         daily_actions = control_setpoints.reshape(7, 288, 35)\n",
    "#         #actions = np.array([a[i].mean(axis=0) for i in range(7)])\n",
    "#         #actions = pd.DataFrame(actions, columns=[\n",
    "#         #    'AssimLight', 'BlackScr', 'CO2air', 'Cum_irr', 'EC_drain_PC', 'EnScr', 'HumDef', \n",
    "#         #    'PipeGrow', 'PipeLow', 'Rhair', 'Tair', 'Tot_PAR', 'Tot_PAR_Lamps', 'VentLee', \n",
    "#         #    'Ventwind', 'assim_vip', 'co2_dos', 'co2_vip', 'dx_vip', 'int_blue_vip', 'int_farred_vip', \n",
    "#         #    'int_red_vip', 'int_white_vip', 'pH_drain_PC', 'scr_blck_vip', 'scr_enrg_vip', \n",
    "#         #    't_grow_min_vip', 't_heat_vip', 't_rail_min_vip', 't_ventlee_vip', 't_ventwind_vip', \n",
    "#         #    'water_sup', 'water_sup_intervals_vip_min', 'window_pos_lee_vip'\n",
    "#         #])\n",
    "#         daily_resource_consumption = self.resource_consumption_estimator.predict(daily_actions)\n",
    "#         print(current_res_cons.shape)\n",
    "#         print(daily_resource_consumption.shape)\n",
    "#         self.daily_res_cons=daily_resource_consumption\n",
    "#         weekly_resource_consumption = self.resource_consumption_estimator.predict(daily_actions).sum(axis=0).reshape(1, 5)\n",
    "#         print(weekly_resource_consumption.shape)\n",
    "#         #print(self.resource_consumption_estimator.predict(actions)[:])\n",
    "#         for i in self.resource_consumption_estimator.predict(daily_actions)[:]:\n",
    "#             #print(i)\n",
    "#             high_rc = np.any(i>=1)\n",
    "#             #print(high_rc)\n",
    "#             if high_rc:\n",
    "#                 #print('break')\n",
    "#                 break\n",
    "#         # Update state\n",
    "#         self.state = {\n",
    "#             'weather': self.weather_data[self.steps * 2016:(self.steps + 1) * 2016],\n",
    "#             'crop_params': weekly_crop_params,\n",
    "#             'resource_consumption': weekly_resource_consumption\n",
    "#         }\n",
    "#         #print(np.max(action),np.min(action),np.any(action>1),np.any(action<0))\n",
    "#         # Calculate reward\n",
    "#         print(weekly_crop_params[0].shape)\n",
    "#         print(weekly_resource_consumption[0].shape)\n",
    "#         reward = self.calculate_reward(weekly_crop_params[0], weekly_resource_consumption[0],high_rc,action)\n",
    "#         #print(np.any(weekly_crop_params[0]>=1 ) , np.any(daily_resource_consumption[0]>=7),high_rc)\n",
    "#         done = bool((self.steps >= self.max_steps) or np.any(weekly_crop_params[0]<0.2 ) or np.any(weekly_resource_consumption[0]>7) or np.all(weekly_crop_params[0]<0.5) ) #or high_rc)\n",
    "#         #print('done',done)\n",
    "#         #rewards.append(reward)\n",
    "        \n",
    "#         return self.state, reward, done, done, {}\n",
    "\n",
    "#     def calculate_reward(self, crop_params, resource_consumption,high_rc,current_actions):\n",
    "#         punishment=0\n",
    "#         big_reward=0\n",
    "#         alpha, beta, delta,gamma = 1, 0.1, 0.1,0.01 #1, 0.5, 0.1\n",
    "#         w1, w2, w3 = 0.40, 0.30, 0.30\n",
    "#         p1, p2, p3, p4 = 0.2, 0.3, 0.2, 0.3\n",
    "#         #w1, w2, w3 = 1, 1, 1\n",
    "#         #p1, p2, p3, p4 = 1, 1, 1, 1\n",
    "#         max_stem_elong, max_stem_thick, max_cum_trusses = 1.0, 1.0, 1.0\n",
    "#         max_heat, max_co2, max_electricity, max_irrigation = 7.0, 7.0, 7.0, 7.0\n",
    "\n",
    "#         crop_reward = (w1 * (crop_params[0] / max_stem_elong) +\n",
    "#                        w2 * (crop_params[1] / max_stem_thick) +\n",
    "#                        w3 * (crop_params[2] / max_cum_trusses))\n",
    "#         resource_penalty = (p1 * (resource_consumption[0] / max_heat) +\n",
    "#                             p2 * (resource_consumption[1] / max_co2) +\n",
    "#                             p3 * ((resource_consumption[2] + resource_consumption[3]) / max_electricity) +\n",
    "#                             p4 * (resource_consumption[4] / max_irrigation))\n",
    "#         #if high_rc:\n",
    "#         #    punishment=-.1\n",
    "#         #if np.any(current_actions>1 ) or np.any(current_actions<0):\n",
    "#         #    punishment-=.9\n",
    "#         if np.any(resource_consumption>7):\n",
    "#             print('resource_consumption>7',np.any(resource_consumption>7))\n",
    "#             punishment-=1\n",
    "#         if np.any(crop_params<.5):\n",
    "#             print('crop_params<.5',np.any(crop_params<.5))\n",
    "#             punishment-=0.8\n",
    "        \n",
    "#         if np.all(crop_params<.5):\n",
    "#             print('all crop_params<.5',np.all(crop_params<.5))\n",
    "#             punishment-=1\n",
    "#         if np.any(crop_params>=.7):\n",
    "#             print('crop_params>=.7',np.any(crop_params>=.7))\n",
    "#             big_reward+=.2\n",
    "#         if np.any(crop_params>=.8):\n",
    "#             print('crop_params>=.8',np.any(crop_params>=.8))\n",
    "#             big_reward+=.2\n",
    "#         if np.all(crop_params>=.5):\n",
    "#             print('all crop_params>=.5',np.all(crop_params>=.5))\n",
    "#             big_reward+=1\n",
    "#         if np.all(crop_params>=.7):\n",
    "#             print('all crop_params>=.7',np.all(crop_params>=.7))\n",
    "#             big_reward+=1\n",
    "#         if np.all(crop_params>=.8):\n",
    "#             print('all crop_params>=.8',np.all(crop_params>=.8))\n",
    "#             big_reward+=1\n",
    "#         #efficiency_factor = crop_reward / (1 + resource_penalty)\n",
    "#         # Stability penalty\n",
    "#         #s = 0.01\n",
    "#         #max_delta_action = 34.0  # Assuming actions are normalized between 0 and 1\n",
    "#         #stability=0\n",
    "#         ##print(len(current_actions)-1)\n",
    "#         #for idx in range(len(current_actions)-1):\n",
    "#         #    stability+=np.sum(np.abs(current_actions[idx]-current_actions[idx+1]))/ max_delta_action\n",
    "#         #    #print('Action Difference',stability)\n",
    "#         #stability_penalty = s * stability\n",
    "#         #print(stability_penalty)\n",
    "#         reward = alpha * crop_reward - beta * resource_penalty +punishment+big_reward #+ delta * efficiency_factor #- gamma * stability_penalty\n",
    "#         print(reward)\n",
    "#         #reward = alpha * crop_reward - beta * resource_penalty + delta * efficiency_factor+punishment+big_reward\n",
    "#         #print(reward)\n",
    "#         return reward\n",
    "\n",
    "\n",
    "# # Assuming weather_data is a preprocessed time series array for the environment\n",
    "# # weather_data = pd.read_csv('/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/weather_fill_missing_values.csv',index_col='%time')#pd.read_csv('D:\\\\Iman\\\\AGHC\\\\CherryTomato\\\\Data\\\\Weather\\\\Weather.csv',index_col='%time') #  # Example placeholder data #np.random.rand(48384, 10)  # Placeholder data\n",
    "# # #print(weather_data.head())\n",
    "# # print(weather_data.shape)\n",
    "# # #print(weather_data.isnull().sum())\n",
    "# # scaler=MinMaxScaler()\n",
    "# # weather_data=scaler.fit_transform(weather_data)\n",
    "# # #print(weather_data[:10])\n",
    "# # print(weather_data.shape)\n",
    "# # Assuming weather_data is randomly generated for the environment\n",
    "# # Generate random weather data with 2016 time steps (e.g., 24 hours * 7 days) and 10 features per step\n",
    "# weather_data = np.random.rand(48384, 10)  # Example shape (48384, 10) to simulate two weeks of data with 5-minute intervals\n",
    "\n",
    "# # Optionally scale the random data to be between 0 and 1, though it's already generated in this range\n",
    "# scaler = MinMaxScaler()\n",
    "# weather_data = scaler.fit_transform(weather_data)\n",
    "# print(weather_data.shape)\n",
    "# # Initialize the environment\n",
    "# env = GreenhouseEnv(crop_parameters_estimator, resource_consumption_estimator, weather_data)\n",
    "# check_env(env)\n",
    "\n",
    "# import os\n",
    "# # Define a folder for saving logs_fs and models\n",
    "# log_dir = \"./logs_fs/\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # Define the policy and agent\n",
    "# #model = PPO(\"MultiInputPolicy\", env, verbose=0, learning_rate=1e-5,tensorboard_log=log_dir) #MultiInputLstmPolicy #MultiInputPolicy\n",
    "# # Get the number of actions in the environment (for noise generation)\n",
    "\n",
    "# # Wrap the environment in a monitor to track performance\n",
    "# env = Monitor(env, log_dir)\n",
    "# # Reduced learning rate for more stable learning, increased exploration noise\n",
    "# # Define the policy and agent\n",
    "# #model = PPO(\"MultiInputPolicy\", env, verbose=0, learning_rate=1e-5,tensorboard_log=log_dir) #MultiInputLstmPolicy #MultiInputPolicy\n",
    "# # Assuming `model` is a PyTorch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Define the path to the best model directory\n",
    "best_model_dir = './logs_fs/best_model/'\n",
    "\n",
    "# Check if the best model file exists\n",
    "best_model_filename = 'best_model.zip'  # Change this to your actual best model filename if different\n",
    "best_model_path = os.path.join(best_model_dir, best_model_filename)\n",
    "\n",
    "# Load the best model\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Loading the best model from {best_model_path}\")\n",
    "    best_model = PPO.load(best_model_path, env=env)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No best model found at {best_model_path}\")\n",
    "\n",
    "# Optionally, evaluate the best model\n",
    "mean_reward, std_reward = evaluate_policy(best_model, env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward of the best model: {mean_reward} +/- {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "orgin_path = \"/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/\"\n",
    "filenames = ['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators'\n",
    "             ]\n",
    "\n",
    "T1_Action=pd.read_csv(orgin_path+f'{filenames[0]}Actions.csv',index_col='%time')\n",
    "T2_Action=pd.read_csv(orgin_path+f'{filenames[1]}Actions.csv',index_col='%time')\n",
    "T3_Action=pd.read_csv(orgin_path+f'{filenames[2]}Actions.csv',index_col='%time')\n",
    "T4_Action=pd.read_csv(orgin_path+f'{filenames[3]}Actions.csv',index_col='%time')\n",
    "T5_Action=pd.read_csv(orgin_path+f'{filenames[4]}Actions.csv',index_col='%time')\n",
    "T6_Action=pd.read_csv(orgin_path+f'{filenames[5]}Actions.csv',index_col='%time')\n",
    "\n",
    "\n",
    "T1_Results=pd.read_csv(orgin_path+f'{filenames[0]}Results.csv',index_col='%time')[:18]\n",
    "T2_Results=pd.read_csv(orgin_path+f'{filenames[1]}Results.csv',index_col='%time')[:18]\n",
    "T3_Results=pd.read_csv(orgin_path+f'{filenames[2]}Results.csv',index_col='%time')[:18]\n",
    "T4_Results=pd.read_csv(orgin_path+f'{filenames[3]}Results.csv',index_col='%time')[:18]\n",
    "T5_Results=pd.read_csv(orgin_path+f'{filenames[4]}Results.csv',index_col='%time')[:18]\n",
    "T6_Results=pd.read_csv(orgin_path+f'{filenames[5]}Results.csv',index_col='%time')[:18]\n",
    "\n",
    "T1_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[0]}resource_consumption.csv',index_col='%time')\n",
    "T2_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[1]}resource_consumption.csv',index_col='%time')\n",
    "T3_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[2]}resource_consumption.csv',index_col='%time')\n",
    "T4_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[3]}resource_consumption.csv',index_col='%time')\n",
    "T5_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[4]}resource_consumption.csv',index_col='%time')\n",
    "T6_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[5]}resource_consumption.csv',index_col='%time')\n",
    "\n",
    "actions_sp=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min',  'days'] #10 Actions \n",
    "resources=['Heat_cons','ElecHigh','ElecLow', 'CO2_cons','Irr']\n",
    "crop_param=['Stem_elong' ,'Stem_thick','Cum_trusses']\n",
    "\n",
    "episode_rewards = []\n",
    "#model_actions=np.empty(action.shape)\n",
    "model_results=[]\n",
    "model_r_consumption=[]\n",
    "env = GreenhouseEnv(crop_parameters_estimator, resource_consumption_estimator,gh_climate_estimator, weather_data)\n",
    "check_env(env)\n",
    "state = env.reset(seed=42)[0]\n",
    "done = False\n",
    "episode_reward = 0\n",
    "i=0\n",
    "model_action=[]\n",
    "while not done:\n",
    "        action = best_model.predict(state,deterministic=True)[0]\n",
    "        if i==0:\n",
    "          model_actions=action \n",
    "          print('model_actions',model_actions)  \n",
    "        else: \n",
    "          model_actions=np.concatenate([model_actions,action],axis=0)\n",
    "        print(model_actions)\n",
    "        print(model_actions.shape)\n",
    "        i+=1\n",
    "        #model_actions=np.concatenate([np.array(model_actions),action],axis=0)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        model_results.append(state['crop_params'])\n",
    "        model_r_consumption.append(state['resource_consumption'])\n",
    "        model_action.append(action)\n",
    "        episode_reward += reward\n",
    "episode_rewards.append(episode_reward.astype('float32'))\n",
    "print(f'Episode Reward: {episode_reward.astype(\"float32\")}')\n",
    "sp=actions_sp\n",
    "sp_noday=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min'] #10 Actions \n",
    "model_actions.shape\n",
    "model_results=pd.DataFrame(np.array(model_results).reshape(23,3),columns=crop_param)[:18]\n",
    "model_r_consumption=pd.DataFrame(np.array(model_r_consumption).reshape(23,5),columns=resources)\n",
    "model_actions=pd.DataFrame(np.array(model_action).reshape(23*2016,9),columns=sp_noday)\n",
    "model_results.set_index(T1_Results.index,inplace=True)\n",
    "model_r_consumption.set_index(T1_resource_Consumption[:-1].index,inplace=True)\n",
    "\n",
    "model_actions=model_actions[:len(T5_Action.index)].set_index(T5_Action.index[:len(model_actions.index)])\n",
    "\n",
    "model_actions.index\n",
    "teams=['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators','Ours']\n",
    "teams_Results={ teams[0]: T1_Results,\n",
    "                 teams[1]: T2_Results,\n",
    "                 teams[2]: T3_Results,\n",
    "                 teams[3]: T4_Results,\n",
    "                 teams[4]: T5_Results,\n",
    "                 teams[5]: T6_Results,\n",
    "                 teams[6]:  model_results,\n",
    "                    \n",
    "                    }\n",
    "teams_Actions={ teams[0]: T1_Action[actions_sp],\n",
    "                teams[1]: T2_Action[actions_sp],\n",
    "                teams[2]: T3_Action[actions_sp],\n",
    "                teams[3]: T4_Action[actions_sp],\n",
    "                teams[4]: T5_Action[actions_sp],\n",
    "                teams[5]: T6_Action[actions_sp],\n",
    "                teams[6]:  model_actions[sp_noday],\n",
    "                    }\n",
    "teams_rc={      teams[0]: T1_resource_Consumption,\n",
    "                teams[1]: T2_resource_Consumption,\n",
    "                teams[2]: T3_resource_Consumption,\n",
    "                teams[3]: T4_resource_Consumption,\n",
    "                teams[4]: T5_resource_Consumption,\n",
    "                teams[5]: T6_resource_Consumption,\n",
    "                teams[6]:  model_r_consumption,\n",
    "    \n",
    "}\n",
    "#plot Teams results\n",
    "print(teams_Results[teams[0]].columns)\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for result in teams_Results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        teams_Results[teams[i]][result].plot(ylabel=result,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "#plot Teams actions\n",
    "# import matplotlib.pyplot as plt\n",
    "# for action in teams_Actions[teams[5]].columns:\n",
    "#     for i in range(len(teams)):\n",
    "#         teams_Actions[teams[i]][\"2019-12-16 00:00:00\" : \"2019-12-17 00:00:00\"][action].plot(ylabel=action,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "#     plt.title(f'green_house_{action}_time_series_plot for all Teams ')\n",
    "#     plt.legend()\n",
    "#     #save_fig(f'green_house_{action}_time_series_plot ')\n",
    "\n",
    "#     plt.show()\n",
    "#plot Teams rc\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in teams_rc[teams[0]].columns[:-1]:\n",
    "    for i in range(len(teams)):\n",
    "        teams_rc[teams[i]][resource].plot(ylabel=resource,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "teams_episode_rewards = {}\n",
    "for team in teams:\n",
    "    print(f'Evaluate {team} Actions....')\n",
    "    \n",
    "    teams_model_actions=np.empty((2016, 10))\n",
    "    Our_model_actions=np.empty((2016, 9))\n",
    "    \n",
    "    model_results=[]\n",
    "    model_r_consumption=[]\n",
    "    state = env.reset(seed=42)[0]\n",
    "    done = False\n",
    "    episode_reward = []\n",
    "    i=0\n",
    "    sp=actions_sp\n",
    "    while not done:\n",
    "            if team=='Ours':\n",
    "                action = best_model.predict(state,deterministic=True)[0]\n",
    "                Our_model_actions=np.concatenate([np.array(Our_model_actions),np.array(action).reshape(2016,9)],axis=0)\n",
    "                state, reward, done, _, _ = env.step(np.array(action))\n",
    "            else:\n",
    "                action = teams_Actions[team].iloc[2016*i:2016*(i+1)][sp]\n",
    "                teams_model_actions=np.concatenate([np.array(teams_model_actions),np.array(action)],axis=0)\n",
    "                print(action.shape)\n",
    "                state, reward, done, _, _ = env.step(np.array(action.drop('days',axis=1)).reshape(2016*9,))\n",
    "            i+=1\n",
    "            \n",
    "            \n",
    "            model_results.append(state['crop_params'])\n",
    "            model_r_consumption.append(state['resource_consumption'])\n",
    "            episode_reward.append(reward.astype('float32'))\n",
    "    print(episode_reward)\n",
    "    teams_episode_rewards[team]=np.array(episode_reward)\n",
    "    print(f'{team} Episode Reward: {np.array(teams_episode_rewards[team]).sum().astype(\"float32\")}')\n",
    "\n",
    "\n",
    "#plt.bar(x=final_result.index,height=final_result[0],rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "orgin_path = \"/media/ai-ws2/8f2a1bf6-3409-412b-abae-522c4615e68f/ImanHindi/AGHC/AutonomousGreenHouseChallenge/Code/Training/\"\n",
    "filenames = ['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators'\n",
    "             ]\n",
    "\n",
    "T1_Action=pd.read_csv(orgin_path+f'{filenames[0]}Actions.csv',index_col='%time')\n",
    "T2_Action=pd.read_csv(orgin_path+f'{filenames[1]}Actions.csv',index_col='%time')\n",
    "T3_Action=pd.read_csv(orgin_path+f'{filenames[2]}Actions.csv',index_col='%time')\n",
    "T4_Action=pd.read_csv(orgin_path+f'{filenames[3]}Actions.csv',index_col='%time')\n",
    "T5_Action=pd.read_csv(orgin_path+f'{filenames[4]}Actions.csv',index_col='%time')\n",
    "T6_Action=pd.read_csv(orgin_path+f'{filenames[5]}Actions.csv',index_col='%time')\n",
    "\n",
    "\n",
    "T1_Results=pd.read_csv(orgin_path+f'{filenames[0]}Results.csv',index_col='%time')[:18]\n",
    "T2_Results=pd.read_csv(orgin_path+f'{filenames[1]}Results.csv',index_col='%time')[:18]\n",
    "T3_Results=pd.read_csv(orgin_path+f'{filenames[2]}Results.csv',index_col='%time')[:18]\n",
    "T4_Results=pd.read_csv(orgin_path+f'{filenames[3]}Results.csv',index_col='%time')[:18]\n",
    "T5_Results=pd.read_csv(orgin_path+f'{filenames[4]}Results.csv',index_col='%time')[:18]\n",
    "T6_Results=pd.read_csv(orgin_path+f'{filenames[5]}Results.csv',index_col='%time')[:18]\n",
    "\n",
    "T1_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[0]}resource_consumption.csv',index_col='%time')\n",
    "T2_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[1]}resource_consumption.csv',index_col='%time')\n",
    "T3_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[2]}resource_consumption.csv',index_col='%time')\n",
    "T4_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[3]}resource_consumption.csv',index_col='%time')\n",
    "T5_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[4]}resource_consumption.csv',index_col='%time')\n",
    "T6_resource_Consumption=pd.read_csv(orgin_path+f'{filenames[5]}resource_consumption.csv',index_col='%time')\n",
    "\n",
    "actions_sp=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min',  'days'] #10 Actions \n",
    "resources=['Heat_cons','ElecHigh','ElecLow', 'CO2_cons','Irr']\n",
    "crop_param=['Stem_elong' ,'Stem_thick','Cum_trusses']\n",
    "\n",
    "episode_rewards = []\n",
    "#model_actions=np.empty(action.shape)\n",
    "model_results=[]\n",
    "model_r_consumption=[]\n",
    "env = GreenhouseEnv(crop_parameters_estimator, resource_consumption_estimator,gh_climate_estimator, weather_data)\n",
    "check_env(env)\n",
    "state = env.reset(seed=42)[0]\n",
    "done = False\n",
    "episode_reward = 0\n",
    "i=0\n",
    "\n",
    "while not done:\n",
    "        action = best_model.predict(state,deterministic=True)[0]\n",
    "        print(action.shape)\n",
    "        if i==0:\n",
    "          model_actions=action.reshape(2016,9)\n",
    "        else: \n",
    "          model_actions=np.concatenate([model_actions,action.reshape(2016,9)],axis=0)\n",
    "        print(model_actions.shape)\n",
    "        i+=1\n",
    "        #model_actions=np.concatenate([np.array(model_actions),action],axis=0)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        model_results.append(state['crop_params'])\n",
    "        model_r_consumption.append(state['resource_consumption'])\n",
    "        episode_reward += reward\n",
    "episode_rewards.append(episode_reward.astype('float32'))\n",
    "print(f'Episode Reward: {episode_reward.astype(\"float32\")}')\n",
    "sp=actions_sp\n",
    "sp_noday=['co2_vip', 'int_white_vip', 'pH_drain_PC','scr_blck_vip', 'scr_enrg_vip',  't_heat_vip',\n",
    "            't_ventlee_vip', 'water_sup','water_sup_intervals_vip_min'] #10 Actions \n",
    "model_actions.shape\n",
    "model_results=pd.DataFrame(np.array(model_results).reshape(23,3),columns=crop_param)[:18]\n",
    "model_r_consumption=pd.DataFrame(np.array(model_r_consumption).reshape(23,5),columns=resources)\n",
    "model_actions=pd.DataFrame(np.array(model_actions),columns=sp_noday)\n",
    "model_results.set_index(T1_Results.index,inplace=True)\n",
    "model_r_consumption.set_index(T1_resource_Consumption[:-1].index,inplace=True)\n",
    "\n",
    "model_actions=model_actions[:len(T5_Action.index)].set_index(T5_Action.index[:len(model_actions.index)])\n",
    "\n",
    "model_actions.index\n",
    "teams=['AICU', 'Automatoes',\n",
    "             'Digilog', 'IUACAAS', \n",
    "             'Reference', 'TheAutomators','Ours']\n",
    "teams_Results={ teams[0]: T1_Results,\n",
    "                 teams[1]: T2_Results,\n",
    "                 teams[2]: T3_Results,\n",
    "                 teams[3]: T4_Results,\n",
    "                 teams[4]: T5_Results,\n",
    "                 teams[5]: T6_Results,\n",
    "                 teams[6]:  model_results,\n",
    "                    \n",
    "                    }\n",
    "teams_Actions={ teams[0]: T1_Action[actions_sp],\n",
    "                teams[1]: T2_Action[actions_sp],\n",
    "                teams[2]: T3_Action[actions_sp],\n",
    "                teams[3]: T4_Action[actions_sp],\n",
    "                teams[4]: T5_Action[actions_sp],\n",
    "                teams[5]: T6_Action[actions_sp],\n",
    "                teams[6]:  model_actions[sp_noday],\n",
    "                    }\n",
    "teams_rc={      teams[0]: T1_resource_Consumption,\n",
    "                teams[1]: T2_resource_Consumption,\n",
    "                teams[2]: T3_resource_Consumption,\n",
    "                teams[3]: T4_resource_Consumption,\n",
    "                teams[4]: T5_resource_Consumption,\n",
    "                teams[5]: T6_resource_Consumption,\n",
    "                teams[6]:  model_r_consumption,\n",
    "    \n",
    "}\n",
    "#plot Teams results\n",
    "print(teams_Results[teams[0]].columns)\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for result in teams_Results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        teams_Results[teams[i]][result].plot(ylabel=result,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "#plot Teams actions\n",
    "# import matplotlib.pyplot as plt\n",
    "# for action in teams_Actions[teams[5]].columns:\n",
    "#     for i in range(len(teams)):\n",
    "#         teams_Actions[teams[i]][\"2019-12-16 00:00:00\" : \"2019-12-17 00:00:00\"][action].plot(ylabel=action,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "#     plt.title(f'green_house_{action}_time_series_plot for all Teams ')\n",
    "#     plt.legend()\n",
    "#     #save_fig(f'green_house_{action}_time_series_plot ')\n",
    "\n",
    "#     plt.show()\n",
    "#plot Teams rc\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in teams_rc[teams[0]].columns[:-1]:\n",
    "    for i in range(len(teams)):\n",
    "        teams_rc[teams[i]][resource].plot(ylabel=resource,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "teams_episode_rewards = {}\n",
    "all_teams_results={}\n",
    "all_teams_r_consumption={}\n",
    "all_teams_episode_rewards={}\n",
    "for team in teams:\n",
    "    print(f'Evaluate {team} Actions....')\n",
    "    \n",
    "    teams_model_actions=np.empty((2016, 9))\n",
    "    Our_model_actions=np.empty((2016, 9))\n",
    "    \n",
    "    model_results=[]\n",
    "    \n",
    "    model_r_consumption=[]\n",
    "    state = env.reset(seed=42)[0]\n",
    "    done = False\n",
    "    episode_reward = []\n",
    "    i=0\n",
    "    sp=actions_sp\n",
    "    while not done:\n",
    "            if team=='Ours':\n",
    "                action = best_model.predict(state,deterministic=True)[0]\n",
    "                Our_model_actions=np.concatenate([np.array(Our_model_actions),np.array(action).reshape(2016,9)],axis=0)\n",
    "                state, reward, done, _, _ = env.step(np.array(action))\n",
    "                \n",
    "            else:\n",
    "                action = teams_Actions[team].iloc[2016*i:2016*(i+1)][sp_noday]\n",
    "                teams_model_actions=np.concatenate([np.array(teams_model_actions),np.array(action)],axis=0)\n",
    "                print(action.shape)\n",
    "                state, reward, done, _, _ = env.step(np.array(action))\n",
    "            i+=1\n",
    "            \n",
    "            \n",
    "            model_results.append(state['crop_params'])\n",
    "            model_r_consumption.append(state['resource_consumption'])\n",
    "            episode_reward.append(reward.astype('float32'))\n",
    "            \n",
    "    model_results=pd.DataFrame(np.array(model_results).reshape(23,3),columns=crop_param)[:18]\n",
    "    model_r_consumption=pd.DataFrame(np.array(model_r_consumption).reshape(23,5),columns=resources)\n",
    "    model_actions=pd.DataFrame(np.array(model_actions),columns=sp_noday)\n",
    "    model_results.set_index(T1_Results.index,inplace=True)\n",
    "    model_r_consumption.set_index(T1_resource_Consumption[:-1].index,inplace=True)\n",
    "    all_teams_results[team]=model_results\n",
    "    all_teams_r_consumption[team]=model_r_consumption\n",
    "    print(episode_reward)\n",
    "    teams_episode_rewards[team]=np.array(episode_reward)\n",
    "    print(f'{team} Episode Reward: {np.array(teams_episode_rewards[team]).sum().astype(\"float32\")}')\n",
    "    all_teams_episode_rewards[team]=np.array(teams_episode_rewards[team]).sum().astype(\"float32\")\n",
    "\n",
    "#plt.bar(x=final_result.index,height=final_result[0],rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_teams_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Teams results\n",
    "print(all_teams_results[teams[0]].columns)\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for result in all_teams_results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        all_teams_results[teams[i]][result].plot(ylabel=result,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "#plot Teams actions\n",
    "# import matplotlib.pyplot as plt\n",
    "# for action in teams_Actions[teams[5]].columns:\n",
    "#     for i in range(len(teams)):\n",
    "#         teams_Actions[teams[i]][\"2019-12-16 00:00:00\" : \"2019-12-17 00:00:00\"][action].plot(ylabel=action,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "#     plt.title(f'green_house_{action}_time_series_plot for all Teams ')\n",
    "#     plt.legend()\n",
    "#     #save_fig(f'green_house_{action}_time_series_plot ')\n",
    "\n",
    "#     plt.show()\n",
    "#plot Teams rc\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in all_teams_r_consumption[teams[0]].columns[:]:\n",
    "    for i in range(len(teams)):\n",
    "        all_teams_r_consumption[teams[i]][resource].plot(ylabel=resource,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teams_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teams_r_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for result in teams_Results[teams[0]].columns:\n",
    "    for i in range(len(teams)):\n",
    "        teams_Results[teams[i]][result].plot(ylabel=result,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{result}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{result}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n",
    "#plot Teams actions\n",
    "# import matplotlib.pyplot as plt\n",
    "# for action in teams_Actions[teams[5]].columns:\n",
    "#     for i in range(len(teams)):\n",
    "#         teams_Actions[teams[i]][\"2019-12-16 00:00:00\" : \"2019-12-17 00:00:00\"][action].plot(ylabel=action,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "#     plt.title(f'green_house_{action}_time_series_plot for all Teams ')\n",
    "#     plt.legend()\n",
    "#     #save_fig(f'green_house_{action}_time_series_plot ')\n",
    "\n",
    "#     plt.show()\n",
    "#plot Teams rc\n",
    "\n",
    "#plot Teams actions\n",
    "import matplotlib.pyplot as plt\n",
    "for resource in teams_rc[teams[0]].columns[:-1]:\n",
    "    for i in range(len(teams)):\n",
    "        teams_rc[teams[i]][resource].plot(ylabel=resource,grid=True, marker=\".\", figsize=(18, 6),legend=True,label=f'{teams[i]}')\n",
    "    plt.title(f'green_house_{resource}_time_series_plot for all Teams ')\n",
    "    plt.legend()\n",
    "    #save_fig(f'green_house_{resource}_time_series_plot ')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams_episode_rewards['Ours'])\n",
    "print()\n",
    "# teams\n",
    "# teams_episode_rewards_df=pd.DataFrame(teams_episode_rewards,columns=teams)\n",
    "# print(teams_episode_rewards_df)\n",
    "# final_result=pd.DataFrame(teams_episode_rewards_df.sum(axis=0).sort_values(ascending=False))\n",
    "# final_result\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.bar(final_result.index, final_result[0])\n",
    "# plt.xticks(rotation=90)  # Rotate the x-axis labels by 90 degrees\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards=[]\n",
    "for team_reward in teams_episode_rewards:\n",
    "    print(team_reward)\n",
    "    print(len(teams_episode_rewards[team_reward]))\n",
    "    print(teams_episode_rewards[team_reward].sum())\n",
    "    rewards.append(teams_episode_rewards[team_reward].sum())\n",
    "teams_episode_rewards_df=pd.DataFrame(np.array(rewards).reshape(1,7),columns=teams)\n",
    "print(teams_episode_rewards_df)\n",
    "final_result=pd.DataFrame(teams_episode_rewards_df.sum(axis=0).sort_values(ascending=False))\n",
    "final_result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(final_result.index, final_result[0])\n",
    "plt.xticks(rotation=90)  # Rotate the x-axis labels by 90 degrees\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(final_result.index, final_result[0])\n",
    "plt.xticks(rotation=90)  # Rotate the x-axis labels by 90 degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mean_rc={}\n",
    "\n",
    "for i in range(len(teams)):\n",
    "        mean_rc[teams[i]]=teams_rc[teams[i]].mean(axis=0)[['CO2_cons','ElecHigh','ElecLow','Heat_cons','Irr']].round(2)\n",
    "\n",
    "mean_rc_df=pd.DataFrame.from_dict(mean_rc)\n",
    "mean_rc_df.to_csv('mean_rc_df_data.csv', index=True)  # Set index=True to include the index column\n",
    "mean_rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mean_cp={}\n",
    "\n",
    "for i in range(len(teams)):\n",
    "        mean_cp[teams[i]]=teams_Results[teams[i]].mean(axis=0).round(2)\n",
    "\n",
    "mean_cp\n",
    "mean_cp_df=pd.DataFrame.from_dict(mean_cp)\n",
    "mean_cp_df.to_csv('mean_cp_df_data.csv', index=True)  # Set index=True to include the index column\n",
    "mean_cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(teams)):\n",
    "    #print(greenhouse_climate[teams[i]].index)\n",
    "    teams_Actions[teams[i]]['%time'] = pd.to_datetime(teams_Actions[teams[i]].index.str.split('_').str[0])\n",
    "    teams_Actions[teams[i]].set_index('%time', inplace=True)  # Set datetime as the index\n",
    "\n",
    "    \n",
    "    teams_Results[teams[i]]['%Time'] = pd.to_datetime(teams_Results[teams[i]].index.str.split('_').str[0])\n",
    "    teams_Results[teams[i]].set_index('%Time', inplace=True)  # Set datetime as the index\n",
    "\n",
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features=['assim_vip', 'co2_vip', 'dx_vip', 'int_blue_vip', 'int_farred_vip', 'int_red_vip',\n",
    "#        'int_white_vip', 'pH_drain_PC', 'scr_blck_vip', 'scr_enrg_vip',\n",
    "#        't_grow_min_vip', 't_heat_vip', 't_rail_min_vip', 't_ventlee_vip',\n",
    "#        't_ventwind_vip',  'water_sup_intervals_vip_min',\n",
    "#        'window_pos_lee_vip']\n",
    "features=['co2_vip', 'dx_vip', 'int_blue_vip', 'int_farred_vip', 'int_red_vip',\n",
    "       'int_white_vip', 'pH_drain_PC', 'scr_blck_vip', 'scr_enrg_vip',\n",
    "       't_grow_min_vip', 't_heat_vip', 't_rail_min_vip', 't_ventlee_vip',\n",
    "       't_ventwind_vip',  'water_sup_intervals_vip_min',\n",
    "       'window_pos_lee_vip']\n",
    "\n",
    "for feature in sp_noday:\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.gca()\n",
    "    for i in range(len(teams)):\n",
    "        if i!=5:\n",
    "            daily_avg = teams_Actions[teams[i]][feature].resample('h').mean() # compute the mean for each hour\n",
    "            weekly_avg=daily_avg.resample('d').mean()\n",
    "        else:\n",
    "            daily_avg = teams_Actions[teams[i]][feature].resample('h').mean()  # compute the mean for each hour\n",
    "            weekly_avg=daily_avg.resample('d').mean()\n",
    "\n",
    "        #weekly_avg = Weekly_Crop_Parameters[filenames[i]][output].resample('W').mean()\n",
    "        #print(dayly_avg)\n",
    "        period=slice(\"2019-12-16 00:00:00\", \"2020-05-20 00:00:00\")\n",
    "        rolling_average_daily = daily_avg[period].rolling(window=24*7).mean()\n",
    "        #rolling_average_weekly = weekly_avg[period].rolling(window=4).mean()\n",
    "\n",
    "        rolling_average_daily.plot(grid=True,legend=True,label=f'{teams[i]}')\n",
    "    ax.set_xlabel(xlabel='%time')\n",
    "    ax.set_ylabel(ylabel=f'{feature}')\n",
    "    ax.set_title(f'{feature} time series of Teams')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#add the legend for each line of plot which represent the file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
